% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  english,
  man,floatsintext]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Semantic content, not speech prosody, predicts momentary subjective affect experience},
  pdfauthor={Timo K. Koch1,2,3, Gabriella M. Harari3, Ramona Schoedel2,4, Zachariah Marrero5, Florian Bemmann6, Samuel D. Gosling5, Markus Bühner2, \& Clemens Stachl1},
  pdflang={en-EN},
  pdfkeywords={Affect, Speech, Prosody, Voice, Machine Learning},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

\makeatletter
\usepackage{etoolbox}
\patchcmd{\maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{Affect, Speech, Prosody, Voice, Machine Learning\newline\indent Word count: 6135}
\usepackage{csquotes}
\ifXeTeX
  % Load polyglossia as late as possible: uses bidi with RTL langages (e.g. Hebrew, Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\else
  \usepackage[main=english]{babel}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\fi
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\title{Semantic content, not speech prosody, predicts momentary subjective affect experience}
\author{Timo K. Koch\textsuperscript{1,2,3}, Gabriella M. Harari\textsuperscript{3}, Ramona Schoedel\textsuperscript{2,4}, Zachariah Marrero\textsuperscript{5}, Florian Bemmann\textsuperscript{6}, Samuel D. Gosling\textsuperscript{5}, Markus Bühner\textsuperscript{2}, \& Clemens Stachl\textsuperscript{1}}
\date{}


\shorttitle{Subjective Affect Experience in Speech}

\authornote{

Conflict of interest: none
Acknowledgements: We thank audEERING GmbH, Peter Ehrich, and Dominik Heinrich for their support with the technical implementation of the on-device voice feature extraction for study 1, ZPID for the support with data collection for study 1, and Sumer Vaid for the technical support with the set up of the computational environment for the analyses of study 2. This project was supported by the Swiss National Science Foundation (SNSF) under project number 215303 and a scholarship of the German Academic Scholarship Foundation awarded to the first author.
All code and data are available in the project's repository on the Open Science Framework (OSF): \url{https://osf.io/a5db8/}

The authors made the following contributions. Timo K. Koch: Conceptualization, Formal Analysis, Writing - Original Draft Preparation, Writing - Review \& Editing; Gabriella M. Harari: Writing - Review \& Editing, Supervision; Ramona Schoedel: Conceptualization, Writing - Review \& Editing; Zachariah Marrero: Data curation; Florian Bemmann: Software; Samuel D. Gosling: Resources; Markus Bühner: Resources; Clemens Stachl: Writing - Review \& Editing, Supervision.

Correspondence concerning this article should be addressed to Timo K. Koch, Torstrasse 25, 9000 St.~Gallen, Switzerland. E-mail: \href{mailto:timo.koch@unisg.ch}{\nolinkurl{timo.koch@unisg.ch}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Institute of Behavioral Science and Technology, University of St.~Gallen\\\textsuperscript{2} Department of Psychology, Ludwig-Maximilians-Universität München\\\textsuperscript{3} Department of Communication, Stanford University\\\textsuperscript{4} Charlotte Fresensius Hochschule, University of Psychology\\\textsuperscript{5} Department of Psychology, The University of Texas at Austin\\\textsuperscript{6} Media Informatics Group, Ludwig-Maximilians-Universität München}

\note{

\clearpage

}

\abstract{%
Advances in the area of artificial intelligence (AI) and the ubiquity of speech data from novel data sources such as voice assistants have sparked pioneering research into the algorithmic recognition of affective states, leading to the development of numerous commercial products that claim to automatically recognize emotions from prosodic features of the human voice. However, these algorithms are predomiantly trained on enacted or labelled speech samples from artificial lab settings representing affect \emph{expression} and are used to infer everyday subjective affect \emph{experience}. In this work, we investigate whether subjective momentary affect experience can be recognized from voice samples collected in the wild using machine learning. Across two studies, we extracted prosodic features as well as state-of-the-art word embeddings capturing semantic information from 24,766 speech samples, paired with experience-sampled self-reports on momentary affect experience, from 997 participants using off-the-shelf smartphones. We found that speech prosody conveyed only limited affective information, while semantic features were predictive of experienced affect. Additionally, we identified those prosodic features containing affective information. Moreover, our results suggest that the sentiment of the speech content does not affect predictions from prosodic features. Our findings challenge the transferability of prediction results from past research and commercial products on the recognition of affect \emph{expression} to the recognition of subjective affect \emph{experience} from prosody. We discuss the implications for the algorithmic monitoring of affect experience from speech in everyday life.
}



\begin{document}
\maketitle

Verbal communication is a fundamental modality to convey emotional information (Kraus, 2017; Scherer, 2003). Prosodic voice cues, such as tone, pitch, and rhythm, combined with the semantic content of speech, enable listeners to infer the speaker's affective state. Researchers have investigated variations in prosodic features with affect for decades and, more recently, trained algorithms to automatically recognize affective states and related affective disorders from voice, offering promising potential applications, including (mental) health care, human-machine interaction, education, and business (Hildebrand et al., 2020; Milling, Pokorny, Bartl-Pokorny, \& Schuller, 2022; Vlahos, 2019). The widespread availability of speech data from voice assistants (e.g., Amazon's Alexa and Apple's Siri) has also spurred commercial interest in deploying affect-detecting algorithms in everyday life. These tools aim to quantify at scale how patients, customers, and employees feel in a given moment, often to provide personalized feedback and services (e.g., (Matz \& Netzer, 2017)). However, existing affect-recognition models in research and commercial tools are predominantly trained on enacted or labelled speech samples from artificial lab settings, which represent affect \emph{expressions}, but are deployed to detect people's subjective affect \emph{experience} in real-world contexts. This discrepancy raises concerns about the models' generalizability and their ability to accurately detect subjective affect experience in everyday life. The present work addresses this gap by investigating the algorithmic recognition of subjective self-reported momentary affect experience from speech samples collected via smartphones in everyday life.

\hypertarget{predicting-affect-from-prosody}{%
\subsection{Predicting affect from prosody}\label{predicting-affect-from-prosody}}

Past work suggests that affective states (e.g., short-lived emotions) are predictable from a range of speech data, such as labelled TV clips (Grimm, Kroschel, \& Narayanan, 2008), recorded phone calls (Burkhardt, Paeschke, Rolfes, Sendlmeier, \& Weiss, 2005), and enacted speech samples from the lab (Bänziger, Mortillaro, \& Scherer, 2012; Schuller, 2018; Vogt, André, \& Wagner, 2008). They reported very high performance in the algorithmic recognition of affective arousal (r\textsubscript{max} = 81) and valence (r\textsubscript{max} = .68) (Weninger, Eyben, Schuller, Mortillaro, \& Scherer, 2013). In those previous studies, the targeted affective states were either enacted by professional actors or assigned labels by raters, serving as the ground truth. This approach comes with a set of downsides, such as actors potentially overacting and the ambiguity of ground truth due to the subjective nature of labeling (Batliner et al., 2011; Schuller, 2018; Wilting, Krahmer, \& Swerts, 2006). Moreover, by relying on actors or rater, past research focused on recognizing affect \emph{expression} rather than affect \emph{experience}. Investigating affect expressions is invaluable for understanding emotional communication, but it is phenomenologically distinct from affect experience. While there is often overlap between the emotions we express and those we experience, they are not identical. Affect expression encompasses the observable emotional behavior derived from our internal affect experience. Expressions can be intentionally moderated, socially conditioned (e.g., the Pan-Am smile), or culturally predisposed (e.g., variations in emotional expression across countries; (Safdar et al., 2009)). In contrast, affect experiences reflect an individual's genuine internal state. However, ``feeling is not always revealing''; one may not necessarily express what one experiences affectively and might even express something entirely different (Gross \& John, 1997; Gross, John, \& Richards, 2000). Additionally, the expression of affect can be perceived and interpreted differently due to interindividual, situational, and cultural differences. We argue that the experience of affect is the key criterion of interest for both researchers and commercial actors. It provides a deeper understanding of how individuals truly feel rather than just how they outwardly express themselves, which can be inaccurate, superficial, or misleading. Focusing more on affective experience over mere expressions could lead to more accurate and meaningful interpretations of affective states, which is crucial for developing empathetic technologies and fostering meaningful interactions.

Studies predicting subjective affect experience from speech collected in everyday life are rare and algorithmically recognizing real-life \emph{experienced} affect is considered more challenging than affect expressions (Vogt, André, \& Wagner, 2008). Some early, pioneering work has collected everyday speech samples using the Electronically Activated Recorder (EAR), a small electronic device usually attached to one's clothes that captures snippets of ambient sounds and conversations throughout the day (Mehl, 2017). Hereby, everyday speech samples can be unobtrusively collected over a period of time which allows researchers to not only investigate between-person differences in affect experience (i.e., is this person sad?), but also assess within-person fluctuations over time (i.e., is this person more sad than other days?) (Huang \& Epps, 2018; Sun, Schwartz, Son, Kern, \& Vazire, 2020; Weidman et al., 2020). Using the EAR can raise issues regarding privacy protection since potentially non-consenting persons may be recorded, too. Also, the continuous and unobtrusive collection of auditory data from microphones is not legally possible in many countries, raising ethical issues. Moreover, handling the EAR recorders and transmitting the collected data can be tedious for participants and researchers. To overcome these challenges, off-the-shelf smartphones represent a useful platform to collect in-situ samples of momentary affect experience over time and make corresponding speech records using the build-in microphone (Carlier et al., 2022; Sun, Schwartz, Son, Kern, \& Vazire, 2020; Weidman et al., 2020).

\hypertarget{prosody-semantics-interactions-in-affect-recognition-from-speech}{%
\subsection{Prosody-semantics interactions in affect recognition from speech}\label{prosody-semantics-interactions-in-affect-recognition-from-speech}}

Prosody and semantic content interact when transmitting affective information through speech (Ben-David, Multani, Shakuf, Rudzicz, \& van Lieshout, 2016). Moreover, some studies even suggest that there is a prosodic (i.e., from voice cues) dominance in the perception of affect based on data from lab experiments (Ben-David, Multani, Shakuf, Rudzicz, \& van Lieshout, 2016; Lin Yi, Ding Hongwei, \& Zhang Yang, 2020), but not (yet) using speech data from the wild (Schwartz \& Pell, 2012). Moreover, while this research field had focused on the interplay of prosody and semantics in the recognition of affect by human raters, there are, to our knowledge, no studies on prosody-content interactions in algorithmic affect detection. Therefore, the current state of research leaves the question unanswered of whether the content sentiment of user utterances (i.e., the inherent emotional charge) influences the prosodic features that underpin automated affect recognition. This ambiguity extends to practical applications: Can algorithms effectively discern affective states irrespective of the emotionality of the topic of conversation, whether it is neutral (e.g., talking about the weather) or inherently emotional (e.g., reuniting with a loved one)?

\hypertarget{the-sound-of-expressed-emotional-prosody}{%
\subsection{The sound of (expressed) emotional prosody}\label{the-sound-of-expressed-emotional-prosody}}

Prior research suggest that pitch, volume, speech rate, and voice quality, or timbre, serve as central cues in conveying affect. For instance, higher pitch and faster speech rates are generally associated with excitement or stress (Banse \& Scherer, 1996). However, while there is a degree of consistency in how emotions are expressed and perceived through prosody, significant variability exists in these affect-voice mappings (Banse \& Scherer, 1996; Larrouy-Maestri, Poeppel, \& Pell, 2024; Scherer, 2003). One source of variability is the cultural background of the person perceiving emotional speech (van Rijn \& Larrouy-Maestri, 2023). This variability underscores the challenges faced by algorithms in affect recognition, emphasizing that while certain prosodic features might be universally recognized, their interpretation can vary between individuals. However, again, those universal findings on affect-prosody links are grounded on data from enacted emotional speech or labelled samples, representing affect expressions. Hence, it remains unclear if the discovered markers of expressed emotional prosody generalize to subjective affect experience.

Across two large-scale field studies, the present work investigates the algorithmic recognition of momentary subjective affect experience from speech prosody collected via smartphones in everyday life. In the first study, we control for the semantic content by having participants read out scripted sentences and predicting self-reported affect experience from extracted prosodic features. In the second study, participants could speak freely about their current situation, thoughts, and feelings. Then, we predicted self-reported affect experience from prosody as well as semantic content in the form of state-of-the-art word embeddings. Moreover, we analyzed which prosodic cues carry the most affective information across data sets and investigated the effects of the sentiment of speech's content on algorithmic affect recognition from prosodic features in prediction models.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{prosody-contains-little-affective-information-but-semantic-content-does}{%
\subsection{Prosody contains little affective information, but semantic content does}\label{prosody-contains-little-affective-information-but-semantic-content-does}}

Overall, our results suggest that self-reported momentary subjective affect experience could be better predicted from semantic content than from prosodic features (see Figure \ref{fig:predictionoverview} for an overview). In the following, we report on the best performing algorithm for each prediction task respectively (either Random Forest or LASSO). Specifically, for scripted speech content (study 1), the prediction of affective arousal from prosody (\emph{r}\textsubscript{m} = 0.16) was low yet significantly better than chance (\emph{p} = .01). On the contrary, the valence predictions from prosodic features were on average even lower and did not yield significant prediction results (\emph{r}\textsubscript{m} = 0.03, \emph{p} = 1). Models trained on prosodic features from participants talking freely about their current situation, thoughts, and feelings (study 2) yielded a low prediction performance, too. Still, the prediction of arousal (\emph{r}\textsubscript{m} = 0.11, \emph{p} \textless{} .001 ) was significantly better than the baseline. Also, predictions of contentedness (\emph{r}\textsubscript{m} = 0.15, \emph{p} \textless{} .001) were significantly better than chance, but not for sadness (\emph{r}\textsubscript{m} = NA, \emph{p} = .65).
Further, as done in prior research on voice-affect predictions (Weidman et al., 2020), we also compared the prediction performance of machine learning models trained on the much larger Compare2016 (6,737 features) acoustic feature set (Schuller et al., 2016) in contrast to the economic eGeMAPS (88 features) feature set we had used. Just as in prior research, the larger voice feature set did not yield better affect predictions (Weidman et al., 2020). Detailed prediction results are provided in the online supplements. Moreover, to further relate our work to prior work in the field (Sun, Schwartz, Son, Kern, \& Vazire, 2020; Weidman et al., 2020), we also conducted supplementary analyses for the prediction of within-person fluctuations in affect experience. Overall, those yielded on average lower prediction performance than between-person differences as found in prior work (Sun, Schwartz, Son, Kern, \& Vazire, 2020; Weidman et al., 2020). All results can be found in the online supplements.

On the contrary, prediction models trained on semantic content in the form of word embeddings that had been extracted from participants talking freely yielded good performance results. Here, word embeddings were predictive of contentedness (\emph{r}\textsubscript{m} = 0.33), arousal (\emph{r}\textsubscript{m} = 0.31), and sadness (\emph{r}\textsubscript{m} = 0.22).
Finally, models trained on prosodic features and word embedding combined yielded on average a similar prediction performance compared to those models trained on word embeddings only. This pattern suggest that predictions were mostly driven by the information coming from semantic content as captured by word embeddings.
\newpage

\begin{figure}

{\centering \includegraphics[width=1\linewidth,height=1\textheight]{../figures/bmr_plot} 

}

\caption[Prediction performance]{Box and whisker plot of out-of-sample prediction performance measures from five times repeated 10-fold cross-validation for each affect measure. The top part above the horizontal line belongs to study 1 (scripted speech) and the part below the horizontal line to study 2 (free speech).}\label{fig:predictionoverview}
\end{figure}
\newpage

\hypertarget{content-sentiment-has-no-effect-on-affect-predictions-from-prosody}{%
\subsection{Content sentiment has no effect on affect predictions from prosody}\label{content-sentiment-has-no-effect-on-affect-predictions-from-prosody}}

For scripted speech (study 1), we analyzed if the experimentally altered sentiment (positive/ neutral/ negative) of the predefined sentences had an effect on affect predictions from prosody. There were no significant differences in prediction errors across the three sentence conditions for valence (\emph{F}\textsubscript{(2,9905)} = 0.09,
\emph{p} = .91) and arousal (\emph{F}\textsubscript{(2,9905)} = 0.39,
\emph{p} = .68) predictions suggesting that sentences' sentiment did not influence affect predictions from prosodic features.

Further, we investigated the effect of speech sentiment on affect predictions from prosody when participants could talk freely about their current situation, thoughts, and feelings. Here, we analyzed the correlation of content sentiment with the the absolute prediction errors in the prediction of self-reported momentary arousal, contentedness, and sadness from prosodic features using the LASSO algorithm. Results indicate that content sentiment did not have an effect on affect predictions from prosody: Correlations of content sentiment with absolute predictions errors were not significant for neither contentedness (\emph{r} = 0.01, \emph{p} = 0.13, sadness (\emph{r} = -0.01, \emph{p} = 0.18), nor arousal (\emph{r} = 0, \emph{p} = 0.43). When interpreting those results, one has to keep in mind that the overall predictive performance of the models trained on speech prosody was generally low. We provide additional figures illustrating the residuals in prediction models in the online supplements.

\hypertarget{affective-prosody}{%
\subsection{Affective prosody}\label{affective-prosody}}

For models with better-than-chance predictions performance (i.e., arousal predictions in study 1 and 2 as well as contentedness predictions in study 2), we investigated the most informative features in a LASSO regularized model that had been trained on the full data sets to capture the most information possible. As a consequence, any findings need to be interpreted in the context of this specific work. The LASSO algorithm selects those voice parameters from the initial set of 88 voice features that are most informative and assigns those respective betas coefficients while setting the beta weights of uninformative variables to zero. Figure \ref{fig:lassobetas} provides an overview of all selected prosodic features in the regularized regression across prediction tasks and their standardized beta regression weights. All regression coefficients and descriptive (in-sample) correlations of eGeMAPs voice features and self-reported affect experience measures are supplied in the online supplements.
For the prediction of momentary arousal, the non-zero beta coefficients suggest that more features related to prosody were selected in study 1 (scripted speech; p\textsubscript{retained} = 25) in comparison to study 2 (free speech; p\textsubscript{retained} = 8) with higher absolute average coefficients (B\textsubscript{meanS1} = .11; B\textsubscript{meanS2} = .03). Specifically, for the prediction of arousal from scripted speech, the variation of the third formant frequency (F3) from the speech signal was most informative. Formants are resonance frequencies in the vocal tract, and F3 is particularly important for speech clarity and vowel quality. Also the Hammarberg Index which measures the relative energy concentration in the higher frequencies compared to the lower frequencies within voiced speech segments was highly informative.
For arousal predictions from free speech, the variation of the logarithmic amplitude (instead of frequency) of the third formant relative to the fundamental frequency (F0) was most infomative followed by th average (mean and median) loudness.
For the recognition of contentedness, the LASSO algorithm selected 18 voice parameters with the variation of the third formant frequency (F3) being most informative - analogous to the arousal model trained on scripted speech. Also the variation of the first Mel-frequency cepstral coefficient (MFCC) for voiced speech segments was found to be informative of experience contentedness. The first MFCC typically captures the overall energy in the signal.
\newpage

\begin{figure}

{\centering \includegraphics[width=1\linewidth,height=1\textheight]{../figures/betas_plot} 

}

\caption[LASSO betas]{Standardized beta coefficients for eGeMAPs voice features from a LASSO (Least Absolute Shrinkage and Selection Operator) regularized regression model trained on the full data sets. All voice features that have been selected by any LASSO model for any target variable are shown. The target variables have been standardized to account for their varying scaling in the data sets.}\label{fig:lassobetas}
\end{figure}
\newpage

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

Affect-recognition algorithms in research and commercial tools are predominantly trained on prosody from enacted or labelled speech samples from artificial lab settings, which represent affect expressions, but are deployed to detect people's subjective affect experience in real-world contexts. In the present work, we predicted subjective momentary affect experience from prosodic and semantic features in naturalistic speech samples collected using machine learning. In contrast to prior work on the algorithmic recognition of affect expression, our findings suggest that prosody provides only limited predictive information on subjective affect experience while semantic content in the form of word embeddings is predictive of subjective affect experience. Moreover, experimental (Study 1) and explorative (Study 2) findings imply that content sentiment does not impact affect predictions from prosody.

The outcome that prediction models trained on prosodic features achieved lower prediction performance compared to prior work on automatic predictions of affect expression (e.g., Schuller, 2018), supports the notion that recognizing real-life affective states is inherently more challenging algorithmically than detecting enacted or elicited emotions (Vogt, André, \& Wagner, 2008). Nevertheless, our reported prediction performance aligns with previous studies predicting subjective affect experience from speech samples collected in natural settings (Carlier et al., 2022; Sun, Schwartz, Son, Kern, \& Vazire, 2020; Weidman et al., 2020). Another reason for the lower prediction performance in our and other work on naturally sampled affect experiences compared to prior work on recognizing affect expressions could be the fact that data sets contain relatively few instances of extreme affect experiences compared to those used in studies focusing on enacted or labeled affective states. Consequently, our predictions targeted mood states, which are inherently less intense than short-lived emotions that hab been investigated in prior work (Scherer, 2003), and thus present a greater challenge for algorithmic recognition (Vogt, André, \& Wagner, 2008).
Moreover, the prediction of arousal from prosody was successful across studies whereas the prediction of emotional valence only yielded significant predictions for contentedness, but not for overall valence and sadness. This finding is in line with prior work showing that emotional valence is more challenging to automatically infer than arousal due to its subjective nature (Sridhar \& Busso, 2022).

Content semantics in the form of state-of-the-art word embeddings showed a superior affect prediction performance compared to prosody suggesting that speech content could be more informative for algorithms to recognize affect than prosodic cues. This finding contrast prior research suggesting that speech prosody is more relevant for human affect inferences than semantics (Ben-David, Multani, Shakuf, Rudzicz, \& van Lieshout, 2016; Lin Yi, Ding Hongwei, \& Zhang Yang, 2020), while aligning with studies that found semantic content to be more predictive than prosodic cues in algorithmically predicting the momentary subjective experience of happiness (Sun, Schwartz, Son, Kern, \& Vazire, 2020; Weidman et al., 2020). We argue that algorithms are potentially better at detecting signals from the structured nature of text, in contrast to the complexity of prosody, such as subtle nuances of intonation, where humans excel since the ability to interpret affective prosodic cues is deeply rooted in our nature.

Generally, our findings challenge the generalizability of the impressive prediction results from past work on the recognition of affect \emph{expression} (e.g., in enacted speech) to the recognition of subjective affect \emph{experience} from speech prosody. Thereby, our findings also question the proclaimed performance of commercial affect recognition algorithms deployed in daily life that have been trained on enacted or labelled affect \emph{expression} and are used to infer affect \emph{experience}. Consequently, current expectations regarding the performance of affect-detecting algorithms, especially the ones that are focused on prosodic features, applied to everyday speech might be overly optimistic. Hence, more research is needed to determine how well algorithms can pick up on subjective affect experience from natural speech. Here, smartphones could play a prominent role in collecting and analyzing speech data and corresponding in-situ self-reports on subjective affect experience for affect inferences. Building on our work, smartphones could be utilized as mobile experimental labs to study various aspects of affect recognition from speech, such as experimentally varying the semantic content as done in Study 1 (Miller, 2012).

We experimentally varied the emotional valence of the spoken content in Study 1 and exploratively investigated the effect of content sentiment on voice predictions in Study 2. Our findings suggest that the sentiment of the content what participants talked about did not have an impact on the prediction of affect experience from prosody. This could imply that the sentiment of the content what people talk about does not affect model performance when algorithmically inferring affect experience from prosodic features. However, this finding has to be considered in the light of the overall limited predictive performance in this work. Hence, more research is needed to disentangle speech content and form in automatic affect experience recognition.
While the predefined sentences in Study 1 allowed us to control for the sentiment of the content of participants' voice records, they were unable to express themselves freely, which could have impaired predictions from prosodic features compared to Study 2 where participants could talk freely. As a result, researchers and practitioners should consider the context of speech production and keep in mind that findings and trained models might be specific to the given production context and do not necessarily generalize to other production contexts.

Across the detected prosody-affect links in this work, a number of spectral properties (MFCCs, spectral flux, formant amplitudes, and bandwidths) and voice quality indices (Hammarberg Index) showed as important for the prediction of affect experience. This mix underscores that emotional expression and perception through voice are multifaceted, with both the static qualities of sound and their temporal variations being crucial. The presence of features unique to either arousal (e.g., F3 amplitude) or contentedness (e.g., mean loudness) predictions further illustrates that while some prosodic features are broadly applicable to emotional expression, others more specifically align with the nuances of emotional intensity (arousal) or the affective valence conveyed (contentedness). These voice features have also been associated with affect expressions in prior work suggesting that those prosody-voice links generalize to affect experience (Banse \& Scherer, 1996; Larrouy-Maestri, Poeppel, \& Pell, 2024; van Rijn \& Larrouy-Maestri, 2023). Here, future research could expand on our work and investigate affect-prosody associations in more contexts.

\hypertarget{limitations-and-outlook}{%
\subsection{Limitations and outlook}\label{limitations-and-outlook}}

The findings of this work are limited in multiple ways. First, we used slightly different operationalizations of affect experience and sample compositions in the two studies that might affect their comparability: In Study 1, we assessed valence and arousal on a six-point Likert scale. In Study 2, we used two items to assess affective valence (contentedness and sadness) and arousal on five-/four-point Likert scales. As a consequence, findings might be transferable but not directly comparable across studies. We addressed this limitation by ensuring consistent methodologies and statistical approaches in both studies, allowing us to draw meaningful conclusions from our analyses. Further, while Study 1 drew on a representative German sample, Study 2 was based on a student cohort sample from the United States with the respective limitations, such as potential constraints in generalizability (Müller, Chen, Peters, Chaintreau, \& Matz, 2021). Also, the German data set that was analyzed in study 1 only contained Android users because of the technical requirements of the logging software. We do not expect this to impact our results since past research suggests that the selection bias regarding participant demographics and personality for the German population is negligible (Götz, Stieger, \& Reips, 2017; Keusch, Bähr, Haas, Kreuter, \& Trappmann, 2020; Schoedel et al., 2023). Moreover, both data sets have been collected in western countries, specifically Germany and the United States. Prior research suggests that cultural differences in emotion experience exist (Lim, 2016) and that mood inference models from, for example, smartphone sensing data do not necessarily generalize to other countries and cultures (Meegahapola et al., 2023). Therefore, future studies should draw on diverse international samples from different cultural contexts in non-western countries.

Moreover, in contrast to prior work using passive speech sensing (e.g., via the EAR; (Weidman et al., 2020)), the participants had to actively log their speech in the present work. This artificial setting might have had an effect on results. Hence, the findings of this study might be subject to the specific instructions related to the audio records. In this manner, future work should employ multiple different speech tasks for affect predictions and investigate how well predictions generalize from one task to another.
Also, similar to other work using the experience sampling method, our data might be impacted by peoples ability to complete experience samplings and make speech records in a given moment (e.g., phone nearby, driving). Moreover, study participants knew that their affect self-reports and corresponding speech samples would be recorded and later analyzed. As a consequence, with regard to assessed subjective affect experience, participants might have only completed the experience sampling questionnaire in selected affective situations, for example not when they were experiencing extreme affective states, or they had not reported on extreme affect at all (Schoedel et al., 2023). Moreover, participants might have not spoken as naturally as they would if they had not known that their data would be scientifically analyzed. Here, participants might have made the audio records only in selected suitable situations, for example when they were alone in a quiet place.

Additionally, specifically to Study 1, one limitation lies in our privacy-preserving on-device data pre-processing approach. By applying on-device feature extraction, we had limited possibilities to check in detail if participants truly complied with study instructions and had recorded their voice while reading out the predefined sentences accordingly (beyond the data-driven quality checks we had applied). Further, our approach did not allow to control for records' background noise (e.g., when participants were outside next to a road) or how they held their smartphone during the voice record. However, we excluded samples without human voice based on voice indicators and speaker gender predictions that served as a sanity check yielded very good prediction results (prediction accuracy = 91.65\%) indicating good data quality. Future research could investigate additional data-driven approaches to check speech data quality directly on the device. Finally, in future work, smartphones could be used to log and pre-process participants' everyday speech by using pre-trained language models to extract content features (e.g., specific topics or word embeddings) directly on the device, too. Thereby, no raw speech data would have to be transferred to a server and valuable information of language's content could be also used for privacy-respectful affect recognition.

Furthermore, the data in this work is comprised of in-situ self-reports of affect experience from participants' everyday life in a non-clinical population. As a result, the data represents the ``normal'' everyday moods of regular people with only few cases of extreme affect experience. As a result, the trained affect recognition models should be considered in this context. If the conducted analyses were to be replicated in a clinical sample that contains more cases of extreme affect experience, prediction performances might be even higher because the affective signals in speech prosody could be more distinct. This does not necessarily mean, however, that findings and prediction models from the present work generalize well to a clinical sample. Additionally, future studies could aim to collect language samples when participants are known to experience strong emotions, for example based on their physiological signals (Hoemann et al., 2020).

Finally, the ground truth, i.e., the information that is assumed to be fundamentally true, used for model training and evaluation are self-reports on participants' subjective affect experience that are prone to response biases (Gao, Saiedur Rahaman, Shao, \& Salim, 2021). The introduced measurement error can have an impact on predictive modeling (Jacobucci \& Grimm, 2020). Moreover, we employed single items to assess momentary affect experience on different dimensions to as we wanted to assess multiple other psychological constructs in the studies. This is an established approach to reduce participant burden, but it can introduce additional measurement error (Dejonckheere et al., 2022). Therefore, future studies that are particularly interested in subjective affect experience should use multiple items assessing a broad range of affect experience in an intensive longitudinal design.

Moreover, there is a debate on how much of an underlying psychological construct, i.e., of subjective affect experience, scientists can assess using questionnaires: In order to assess one's affect experience most accurately through a survey item, one needs to have introspection to recognize their current affective state and have an adequate understanding of how to translate it to a numerical value on a scale (Boyd, Pasca, \& Lanning, 2020; Montag, Dagum, Hall, \& Elhai, 2022). However, people can vary greatly in that regard (Critchley \& Garfinkel, 2017). Possibly, in the future, algorithms can replace self-report questionnaires altogether by analyzing natural language directly since transformers (that use word embeddings as employed in study 2) have been reported to converge with rating scales with an accuracy that approach the theoretical upper limits already (Kjell, Sikström, Kjell, \& Schwartz, 2022).

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

In this work, we investigated whether machine learning algorithms can recognize subjective affect experience from samples of speech prosody collected in the wild using smartphones. Extracted prosodic voice parameters provided only limited predictive information of affective arousal across both studies, while speech content as reflected in state-of-the-art word embeddings had been shown to be predictive of arousal as well emotional valence. Further, experimental and explorative findings suggest that emotional speech content did not affect predictions from prosody alone (i.e., what someone talked about did not affect how well emotions could be predicted from voice cues). Our findings challenge the generalizability of the optimistic prediction results from prior research work and commercial emotion-detection AI algorithms on the recognition of affect \emph{expression} (e.g., enacted and labelled speech) to the recognition of subjective affect \emph{experience} in everyday speech. The ability to to detect how people genuinely feel, as opposed to merely observing what they outwardly express, is crucial to gaining a deeper understanding of human emotions and guiding the development of technologies that better serve and respond to our emotional needs.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{data-collection}{%
\subsection{Data collection}\label{data-collection}}

The present work is comprised of two studies using machine learning that are based on separate data sets. The workflow of data collection, processing, and predictive modeling is described in detail in the following section and illustrated in Figure \ref{fig:methodsoverview}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{../figures/methods_figure} 

}

\caption[Method diagram]{Diagram illustrating methods in study 1 and study 2.}\label{fig:methodsoverview}
\end{figure}

\hypertarget{study-1}{%
\subsubsection{Study 1}\label{study-1}}

Data collection for study 1 was part of a large panel study (from May 15 until November 14, 2020) using the \emph{PhoneStudy} research app (Schoedel \& Oldemeier, 2020). Data collection was approved by the ethics committee of the psychology department at LMU Munich and all procedures adhered to the General Data Protection Regulation (GDPR). We recruited a quota sample of \emph{N} = 850 participants representative for the German population regarding age, gender, education, income, confession, and relationship status. Participants had to be between 18 and 65 years old, fluent in German, and in the possession of an Android smartphone (Android 5 or higher) for private usage as a sole user (for more details on the panel study see Schoedel et al. (2023) and große Deters and Schoedel (2024)). The study also comprised of two two-week experience sampling phases (July 27, 2020, to August 9,2020; September 21, 2020, to October 4, 2020) during which participants received two to four short questionnaires per day on their personal smartphone. Here, self-reported valence and arousal were assessed in two separate items on six-point Likert scales among other psychological properties (i.e., sleep quality, stress, situational characteristics that had been used in other research) as part of an experience sampling procedure.

The last experience sampling questionnaire of each day included an additional instruction, where participants were asked to read out a series of predefined emotional sentences while making an audio recording of their voice. The sentences presented to the participants are based on a set of 54 validated German neutral and affective sentences (Defren et al., 2018) and differ in their emotional content: positive (e.g., ``My team won yesterday.''), negative (e.g., ``Nobody is interested in my life.''), and neutral (e.g., ``The plate is on the round table.''). In each measurement instance, participants were instructed to read out three positive, neutral, and negative sentences. The order of the categories was randomized per experience sampling instance. For each emotional content category, three sentences were randomly drawn (with replacement) from the respective sets of sentences in the database. The experimental manipulation of these emotional semantic categories allowed us to control for the content spoken by our participants and at the same time enabled us to conduct a privacy-friendly study. The audio recording was started by the participants via a button on the screen. Participants could stop the recording manually after a minimum of four seconds. Alternatively, the recording was stopped automatically after twelve seconds. We chose these lower- and upper-time thresholds because this is the minimum and maximum time required to read out the three sentences per condition, determined by reading the sentences extremely fast and extremely slow and recording the times. Once the audio record had been completed, we used the widely adopted \emph{OpenSMILE} open-source algorithm (Eyben, Wöllmer, \& Schuller, 2010) to automatically extract acoustic features directly on the participant's device. Here, we used the extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPS) that is comprised of 88 acoustic features (Eyben et al., 2016). The eGeMAPS feature set has been used in a range of prior studies on affect recognition from speech (van Rijn \& Larrouy-Maestri, 2023; Weidman et al., 2020). After feature extraction, the voice records were automatically deleted and only the extracted voice features were transmitted to our servers. As a result, there were three sets of voice features per experience sampling instance (one per sentiment condition).

We collected 11205 audio logs with respective affect self-reports from 578 participants. Participants made on average 19.39 (\emph{SD} = 12.36) voice records. We excluded data from 158 participants with less than ten voice samples in total and the data from eight participants who had no variance in all their valence and arousal scores across all their experience samples. Finally, we excluded 214 voice logs from 5 participants because the respective voice indicators suggested that no human voice was recorded (mean voicing score \textless{} 0.5, voiced segments per second = 0, mean voiced segment length = 0).
This left us with a final data set of 9922 voice samples with corresponding acoustic features from 3377 experience sampling instances for valence and arousal from 409 participants (49.50\% female, Age\textsubscript{M} = 42.97 years). In the final sample, there were on average 24.1 (\emph{SD} = 10.6) voice logs with 8.22 (\emph{SD} = 3.53) corresponding affect self-reports per participant. Overall self-reported valence was positive (\emph{M} = 4.70, \emph{SD} = 1.04) and overall arousal was slightly geared towards activation (\emph{M} = 3.67, \emph{SD} = 1.34). The distribution of valence and arousal ratings is provided in the online supplements.

\hypertarget{study-2}{%
\subsubsection{Study 2}\label{study-2}}

Data collection for study 2 was part of the UT1000 Project at the University of Texas at Austin in the United States in fall 2018 (Wu et al., 2021). During a three-week self-tracking assignment using their smartphones, students from a Synchronous Massive Online Course (SMOC) in introductory psychology received four short experience sampling questionnaires per day where they could also make records of their speech at the end. Here, self-reported arousal (assessed on a five-point Likert scale), contentedness, and sadness were assessed in separate items on four-point Likert scales among other psychological properties as part of an experience sampling procedure (Wu et al., 2021). Thereby, in Study 2, we captured emotional valence on two items (contentedness and sadness) instead of one as done in Study 1. According to the affect grid, contentedness and sadness have a comparable low level of arousal and an opposing emotional valence (Posner, Russell, \& Peterson, 2005; Russell, 1980).\\
For the audio records, participants received the following instruction: ``Please record a short audio clip in which you describe the situation you are in, and your current thoughts and feelings. Collect about 10 seconds of environmental sound after the description.'' The responses to this prompt are analyzed in the present study. Any parts of the record that did not contain speech were cut out before further analysis since the focus of this work is affect in human speech. The collected speech samples had also been used in another research project that describes the data collection procedure in more detail (Marrero, Gosling, Pennebaker, \& Harari, 2022).

With this procedure, we collected 23482 audio logs with corresponding affect self-reports from 980 participants. Participants made on average 23.96 (\emph{SD} = 18.40) speech records with corresponding affect self-reports. We followed the same procedure to filter the data as in Study 1: First, we excluded 281 participants with less than ten audio records in total and another participant who had no variance in all their self-reports across all their experience samples. Next, we removed 337 voice records from 11 participants where the respective acoustic features indicated that no human voice was recorded and, to ensure comparability of the two studies with regard to the length of speech samples, we retained all speech transcripts that contained at least 15 words and were more than four seconds long which is equivalent to the length of the sentences that had been read out in Study 1.

This procedure left us with a final data set of 14913 speech samples with corresponding experience-sampled self-reports on momentary affect experience from 589 participants (63\% female, Age\textsubscript{M} = 18.57 years). Overall participants reported balanced experienced contentedness (\emph{M} = 1.68, \emph{SD} = 0.86) and low sadness (\emph{M} = 0.48, \emph{SD} = 0.75). Overall arousal was balanced out (\emph{M} = 1.97, \emph{SD} = 0.95). The distribution of self-reported arousal, contentedness, and sadness scores is provided in the online supplements.

Replicating the approach from study 1 we extracted the extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPS) from the collected audio files using the OpenSMILE algorithm (Eyben et al., 2016; Eyben, Wöllmer, \& Schuller, 2010). In contrast to the on-device feature-extraction approach in study 1, in study 2 those features were extracted from the raw recorded audio files after data collection.

We transcribed all raw audio records using the Google Speech-to-text API (version 1). Automatic speech-to-text technology has been shown to be well-suited for transcription tasks in psychological research (Pfeifer, Chilton, Grilli, \& Mehl, 2024). The Google API also assigns a sentiment score (\emph{M} = 0.03, \emph{SD} = 0.30) within the interval of {[}-1; 1{]} to each speech transcript. Then, we extracted state-of-the-art word embeddings from speech transcripts using the \emph{text} R package (Kjell, Giorgi, \& Schwartz, 2021). Word embeddings are vector representations of words in a high-dimensional space, which capture their contextualized meaning and relationships with other words. Specifically, we used the 1024 dimensions form the second to last layer (layer 23) from the language model ``RoBERTa large'' as features as recommended in prior work (Liu et al., 2019; Matero, Hung, \& Schwartz, 2022).

\hypertarget{predictive-modeling}{%
\subsection{Predictive modeling}\label{predictive-modeling}}

In both studies, we trained linear LASSO (Least Absolute Shrinkage and Selection Operator) regularized regression models (Tibshirani, 1996) and non-linear tree-based Random Forest models (Breiman, 2001; Wright \& Ziegler, 2017), and a baseline model on the extracted features for the prediction of self-reported affect experience. The baseline model predicted the respective mean values for valence and arousal of the respective training set for all cases in a test set. To conduct a preliminary evaluation of the informativeness of our data, we trained additional prediction models with gender as the dependent variable to evaluate whether the collected data at all contained information about individual differences since past work indicates that speaker gender can be reliably predicted from voice cues (Kwasny \& Hemmerling, 2021). We evaluated model performance using a ten-fold five times repeated cross-validation scheme (Bischl, Mersmann, Trautmann, \& Weihs, 2012) and used blocking of participants in the resampling procedure to ensure that for each train/test set pair the given participant is either in the training set or in the test set. In the results section, we report on the average model performance across those 50 models. Before predictive modeling, we replaced extreme outliers (mean +/- 4 times SD) with missing values that were imputed in the resampling procedure.

Our prediction models were evaluated based on how accurate new (unseen) samples can be predicted. Throughout this manuscript, we report on the average Spearman correlation (\(\rho\)) of true and predicted scores and the coefficient of determination (\emph{R}\textsuperscript{2}) as measures of model fit. In the online supplements, we provide the mean absolute error (MAE) and the root mean squared error (RMSE) as additional performance measures for prediction models. To determine whether prosodic features predicted affect experience beyond chance (\emph{alpha} = 0.05), we carried out variance-corrected (one-sided) t-tests comparing the \emph{R}\textsuperscript{2} measures of prediction models from speech prosody with those of the baseline models (Nadeau \& Bengio, 2003). We adjusted for multiple comparison (in study 1: \emph{n} = 4; in study 2: \emph{n} = 6) via Holm correction.

All data processing and statistical analyses in this work were performed with the statistical software R version 4.1.1 (R Core Team, 2021). For machine learning, we used the \emph{mlr3} framework (Lang et al., 2019). Specifically, we used the \emph{glmnet} (Friedman, Hastie, \& Tibshirani, 2010) and \emph{ranger} (Wright \& Ziegler, 2017) packages to fit prediction models. We preregistered study 1 as a transparent account of our work (Koch \& Schoedel, 2021) and extended the analytical approach to the data from study 2. All deviations from the preregistration are documented in the online supplements.

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-banseAcousticProfilesVocal1996}{}}%
Banse, R., \& Scherer, K. R. (1996). Acoustic profiles in vocal emotion expression. \emph{Journal of Personality and Social Psychology}, \emph{70}(3), 614--636. \url{https://doi.org/10.1037/0022-3514.70.3.614}

\leavevmode\vadjust pre{\hypertarget{ref-batlinerAutomaticRecognitionEmotions2011}{}}%
Batliner, A., Schuller, B., Seppi, D., Steidl, S., Devillers, L., Vidrascu, L., \ldots{} Amir, N. (2011). The {Automatic Recognition} of {Emotions} in {Speech}. In \emph{Cognitive {Technologies}} (pp. 71--99). \url{https://doi.org/10.1007/978-3-642-15184-2_6}

\leavevmode\vadjust pre{\hypertarget{ref-banzigerIntroducingGenevaMultimodal2012}{}}%
Bänziger, T., Mortillaro, M., \& Scherer, K. R. (2012). Introducing the {Geneva Multimodal} expression corpus for experimental research on emotion perception. \emph{Emotion (Washington, D.C.)}, \emph{12}(5), 1161--1179. \url{https://doi.org/10.1037/a0025827}

\leavevmode\vadjust pre{\hypertarget{ref-ben-davidProsodySemanticsAre2016}{}}%
Ben-David, B., Multani, N., Shakuf, V., Rudzicz, F., \& van Lieshout, P. H. H. M. (2016). Prosody and {Semantics Are Separate} but {Not Separable Channels} in the {Perception} of {Emotional Speech}: {Test} for {Rating} of {Emotions} in {Speech}. \emph{Journal of Speech, Language, and Hearing Research}, \emph{59}(1), 72--89. \url{https://doi.org/10.1044/2015_JSLHR-H-14-0323}

\leavevmode\vadjust pre{\hypertarget{ref-bischlResamplingMethodsMetaModel2012}{}}%
Bischl, B., Mersmann, O., Trautmann, H., \& Weihs, C. (2012). Resampling {Methods} for {Meta-Model Validation} with {Recommendations} for {Evolutionary Computation}. \emph{Evolutionary Computation}, \emph{20}(2), 249--275. \url{https://doi.org/10.1162/EVCO_a_00069}

\leavevmode\vadjust pre{\hypertarget{ref-boydPersonalityPanoramaConceptualizing2020}{}}%
Boyd, R. L., Pasca, P., \& Lanning, K. (2020). The {Personality Panorama}: {Conceptualizing Personality} through {Big Behavioural Data}. \emph{European Journal of Personality}, \emph{34}(5), 599--612. \url{https://doi.org/10.1002/per.2254}

\leavevmode\vadjust pre{\hypertarget{ref-breimanRandomForests2001}{}}%
Breiman, L. (2001). Random forests. \emph{Machine Learning}, \emph{45}(1), 5--32.

\leavevmode\vadjust pre{\hypertarget{ref-burkhardtDatabaseGermanEmotional2005a}{}}%
Burkhardt, F., Paeschke, A., Rolfes, M., Sendlmeier, W., \& Weiss, B. (2005). A database of {German} emotional speech. In \emph{9th European Conference on Speech Communication and Technology} (Vol. 5, p. 1520). \url{https://doi.org/10.21437/Interspeech.2005-446}

\leavevmode\vadjust pre{\hypertarget{ref-carlierSearchStateTrait2022}{}}%
Carlier, C., Niemeijer, K., Mestdagh, M., Bauwens, M., Vanbrabant, P., Geurts, L., \ldots{} Kuppens, P. (2022). In {Search} of {State} and {Trait Emotion Markers} in {Mobile-Sensed Language}: {Field Study}. \emph{JMIR Mental Health}, \emph{9}(2), e31724. \url{https://doi.org/10.2196/31724}

\leavevmode\vadjust pre{\hypertarget{ref-critchleyInteroceptionEmotion2017}{}}%
Critchley, H. D., \& Garfinkel, S. N. (2017). Interoception and emotion. \emph{Current Opinion in Psychology}, \emph{17}, 7--14. \url{https://doi.org/10.1016/j.copsyc.2017.04.020}

\leavevmode\vadjust pre{\hypertarget{ref-defrenEmotionalSpeechPerception2018}{}}%
Defren, S., de Brito Castilho Wesseling, P., Allen, S., Shakuf, V., Ben-David, B., \& Lachmann, T. (2018). Emotional {Speech Perception}: {A} set of semantically validated {German} neutral and emotionally affective sentences. \emph{9th {International Conference} on {Speech Prosody} 2018}, 714--718. ISCA. \url{https://doi.org/10.21437/SpeechProsody.2018-145}

\leavevmode\vadjust pre{\hypertarget{ref-dejonckheereAssessingReliabilitySingleitem2022}{}}%
Dejonckheere, E., Demeyer, F., Geusens, B., Piot, M., Tuerlinckx, F., Verdonck, S., \& Mestdagh, M. (2022). Assessing the reliability of single-item momentary affective measurements in experience sampling. \emph{Psychological Assessment}, \emph{34}(12), 1138--1154. \url{https://doi.org/10.1037/pas0001178}

\leavevmode\vadjust pre{\hypertarget{ref-eybenGenevaMinimalisticAcoustic2016}{}}%
Eyben, F., Scherer, K. R., Schuller, B. W., Sundberg, J., Andre, E., Busso, C., \ldots{} Truong, K. P. (2016). The {Geneva Minimalistic Acoustic Parameter Set} ({GeMAPS}) for {Voice Research} and {Affective Computing}. \emph{IEEE Transactions on Affective Computing}, \emph{7}(2), 190--202. \url{https://doi.org/10.1109/TAFFC.2015.2457417}

\leavevmode\vadjust pre{\hypertarget{ref-eybenOpensmileMunichVersatile2010}{}}%
Eyben, F., Wöllmer, M., \& Schuller, B. (2010). Opensmile: The munich versatile and fast open-source audio feature extractor. \emph{Proceedings of the International Conference on {Multimedia} - {MM} '10}, 1459. Firenze, Italy: ACM Press. \url{https://doi.org/10.1145/1873951.1874246}

\leavevmode\vadjust pre{\hypertarget{ref-friedmanRegularizationPathsGeneralized2010}{}}%
Friedman, J., Hastie, T., \& Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent. \emph{Journal of Statistical Software}, \emph{33}(1), 1.

\leavevmode\vadjust pre{\hypertarget{ref-gaoInvestigatingReliabilitySelfreport2021}{}}%
Gao, N., Saiedur Rahaman, M., Shao, W., \& Salim, F. D. (2021). Investigating the {Reliability} of {Self-report Data} in the {Wild}: {The Quest} for {Ground Truth}. \emph{Adjunct {Proceedings} of the 2021 {ACM International Joint Conference} on {Pervasive} and {Ubiquitous Computing} and {Proceedings} of the 2021 {ACM International Symposium} on {Wearable Computers}}, 237--242. New York, NY, USA: Association for Computing Machinery. \url{https://doi.org/10.1145/3460418.3479338}

\leavevmode\vadjust pre{\hypertarget{ref-gotzUsersMainSmartphone2017}{}}%
Götz, F. M., Stieger, S., \& Reips, U.-D. (2017). Users of the main smartphone operating systems ({iOS}, {Android}) differ only little in personality. \emph{PLOS ONE}, \emph{12}(5), e0176921. \url{https://doi.org/10.1371/journal.pone.0176921}

\leavevmode\vadjust pre{\hypertarget{ref-grimmVeraAmMittag2008}{}}%
Grimm, M., Kroschel, K., \& Narayanan, S. (2008). The {Vera} am {Mittag German} audio-visual emotional speech database. \emph{2008 {IEEE International Conference} on {Multimedia} and {Expo}}, 865--868. \url{https://doi.org/10.1109/ICME.2008.4607572}

\leavevmode\vadjust pre{\hypertarget{ref-grossRevealingFeelingsFacets1997}{}}%
Gross, J. J., \& John, O. L. I. E. (1997). Revealing feelings: {Facets} of emotional expressivity in self-reports, peer ratings, and expressive behavior. \emph{Journal of Personality and Social Psychology}, 434--447.

\leavevmode\vadjust pre{\hypertarget{ref-grossDissociationEmotionExpression2000}{}}%
Gross, J. J., John, O. P., \& Richards, J. M. (2000). The {Dissociation} of {Emotion Expression} from {Emotion Experience}: {A Personality Perspective}. \emph{Personality and Social Psychology Bulletin}, \emph{26}(6), 712--726. \url{https://doi.org/10.1177/0146167200268006}

\leavevmode\vadjust pre{\hypertarget{ref-grossedetersKeepScrollingUsing2024}{}}%
große Deters, F., \& Schoedel, R. (2024). Keep on scrolling? {Using} intensive longitudinal smartphone sensing data to assess how everyday smartphone usage behaviors are related to well-being. \emph{Computers in Human Behavior}, \emph{150}, 107977. \url{https://doi.org/10.1016/j.chb.2023.107977}

\leavevmode\vadjust pre{\hypertarget{ref-hildebrandVoiceAnalyticsBusiness2020}{}}%
Hildebrand, C., Efthymiou, F., Busquet, F., Hampton, W. H., Hoffman, D. L., \& Novak, T. P. (2020). Voice analytics in business research: {Conceptual} foundations, acoustic feature extraction, and applications. \emph{Journal of Business Research}, \emph{121}, 364--374. \url{https://doi.org/10.1016/j.jbusres.2020.09.020}

\leavevmode\vadjust pre{\hypertarget{ref-hoemannContextawareExperienceSampling2020a}{}}%
Hoemann, K., Khan, Z., Feldman, M. J., Nielson, C., Devlin, M., Dy, J., \ldots{} Quigley, K. S. (2020). Context-aware experience sampling reveals the scale of variation in affective experience. \emph{Scientific Reports}, \emph{10}, 12459. \url{https://doi.org/10.1038/s41598-020-69180-y}

\leavevmode\vadjust pre{\hypertarget{ref-huangPredictionEmotionChange2018}{}}%
Huang, Z., \& Epps, J. (2018). Prediction of {Emotion Change From Speech}. \emph{Frontiers in ICT}, \emph{5}.

\leavevmode\vadjust pre{\hypertarget{ref-jacobucciMachineLearningPsychological2020}{}}%
Jacobucci, R., \& Grimm, K. J. (2020). Machine {Learning} and {Psychological Research}: {The Unexplored Effect} of {Measurement}. \emph{Perspectives on Psychological Science}, \emph{15}(3), 809--816. \url{https://doi.org/10.1177/1745691620902467}

\leavevmode\vadjust pre{\hypertarget{ref-keuschCoverageErrorData2020}{}}%
Keusch, F., Bähr, S., Haas, G.-C., Kreuter, F., \& Trappmann, M. (2020). Coverage {Error} in {Data Collection Combining Mobile Surveys With Passive Measurement Using Apps}: {Data From} a {German National Survey}. \emph{Sociological Methods \& Research}, 0049124120914924. \url{https://doi.org/10.1177/0049124120914924}

\leavevmode\vadjust pre{\hypertarget{ref-kjellTextRpackageAnalyzing2021}{}}%
Kjell, O., Giorgi, S., \& Schwartz, H. A. (2021). \emph{Text: {An R-package} for {Analyzing} and {Visualizing Human Language Using Natural Language Processing} and {Deep Learning}}. PsyArXiv. \url{https://doi.org/10.31234/osf.io/293kt}

\leavevmode\vadjust pre{\hypertarget{ref-kjellNaturalLanguageAnalyzed2022}{}}%
Kjell, O., Sikström, S., Kjell, K., \& Schwartz, H. A. (2022). Natural language analyzed with {AI-based} transformers predict traditional subjective well-being measures approaching the theoretical upper limits in accuracy. \emph{Scientific Reports}, \emph{12}(1), 3918. \url{https://doi.org/10.1038/s41598-022-07520-w}

\leavevmode\vadjust pre{\hypertarget{ref-kochPredictingAffectiveStates2021}{}}%
Koch, T., \& Schoedel, R. (2021). \emph{Predicting {Affective States} from {Acoustic Voice Cues Collected} with {Smartphones}}. \url{https://doi.org/10.23668/psycharchives.4454}

\leavevmode\vadjust pre{\hypertarget{ref-krausVoiceonlyCommunicationEnhances2017}{}}%
Kraus, M. W. (2017). Voice-only communication enhances empathic accuracy. \emph{The American Psychologist}, \emph{72}(7), 644--654. \url{https://doi.org/10.1037/amp0000147}

\leavevmode\vadjust pre{\hypertarget{ref-kwasnyGenderAgeEstimation2021}{}}%
Kwasny, D., \& Hemmerling, D. (2021). Gender and {Age Estimation Methods Based} on {Speech Using Deep Neural Networks}. \emph{Sensors (Basel, Switzerland)}, \emph{21}(14), 4785. \url{https://doi.org/10.3390/s21144785}

\leavevmode\vadjust pre{\hypertarget{ref-langMlr3ModernObjectoriented2019}{}}%
Lang, M., Binder, M., Richter, J., Schratz, P., Pfisterer, F., Coors, S., \ldots{} Bischl, B. (2019). Mlr3: {A} modern object-oriented machine learning framework in {R}. \emph{Journal of Open Source Software}, \emph{4}(44), 1903. \url{https://doi.org/10.21105/joss.01903}

\leavevmode\vadjust pre{\hypertarget{ref-larrouy-maestriSoundEmotionalProsody2024}{}}%
Larrouy-Maestri, P., Poeppel, D., \& Pell, M. D. (2024). The {Sound} of {Emotional Prosody}: {Nearly} 3 {Decades} of {Research} and {Future Directions}. \emph{Perspectives on Psychological Science}, 17456916231217722. \url{https://doi.org/10.1177/17456916231217722}

\leavevmode\vadjust pre{\hypertarget{ref-limCulturalDifferencesEmotion2016}{}}%
Lim, N. (2016). Cultural differences in emotion: Differences in emotional arousal level between the {East} and the {West}. \emph{Integrative Medicine Research}, \emph{5}(2), 105--109. \url{https://doi.org/10.1016/j.imr.2016.03.004}

\leavevmode\vadjust pre{\hypertarget{ref-linyiProsodyDominatesSemantics2020}{}}%
Lin Yi, Ding Hongwei, \& Zhang Yang. (2020). Prosody {Dominates Over Semantics} in {Emotion Word Processing}: {Evidence From Cross-Channel} and {Cross-Modal Stroop Effects}. \emph{Journal of Speech, Language, and Hearing Research}, \emph{63}(3), 896--912. \url{https://doi.org/10.1044/2020_JSLHR-19-00258}

\leavevmode\vadjust pre{\hypertarget{ref-liuRoBERTaRobustlyOptimized2019}{}}%
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., \ldots{} Stoyanov, V. (2019). \emph{{RoBERTa}: {A Robustly Optimized BERT Pretraining Approach}}. arXiv. \url{https://doi.org/10.48550/arXiv.1907.11692}

\leavevmode\vadjust pre{\hypertarget{ref-marreroEvaluatingVoiceSamples2022}{}}%
Marrero, Z. N. K., Gosling, S. D., Pennebaker, J. W., \& Harari, G. M. (2022). Evaluating voice samples as a potential source of information about personality. \emph{Acta Psychologica}, \emph{230}, 103740. \url{https://doi.org/10.1016/j.actpsy.2022.103740}

\leavevmode\vadjust pre{\hypertarget{ref-materoEvaluatingContextualEmbeddings2022}{}}%
Matero, M., Hung, A., \& Schwartz, H. A. (2022). Evaluating {Contextual Embeddings} and their {Extraction Layers} for {Depression Assessment}. \emph{Proceedings of the 12th {Workshop} on {Computational Approaches} to {Subjectivity}, {Sentiment} \& {Social Media Analysis}}, 89--94. Dublin, Ireland: Association for Computational Linguistics. \url{https://doi.org/10.18653/v1/2022.wassa-1.9}

\leavevmode\vadjust pre{\hypertarget{ref-matzUsingBigData2017}{}}%
Matz, S. C., \& Netzer, O. (2017). Using {Big Data} as a window into consumers' psychology. \emph{Current Opinion in Behavioral Sciences}, \emph{18}, 7--12. \url{https://doi.org/10.1016/j.cobeha.2017.05.009}

\leavevmode\vadjust pre{\hypertarget{ref-meegahapolaGeneralizationPersonalizationMobile2023}{}}%
Meegahapola, L., Droz, W., Kun, P., de Götzen, A., Nutakki, C., Diwakar, S., \ldots{} Gatica-Perez, D. (2023). Generalization and {Personalization} of {Mobile Sensing-Based Mood Inference Models}: {An Analysis} of {College Students} in {Eight Countries}. \emph{Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies}, \emph{6}(4), 176:1--176:32. \url{https://doi.org/10.1145/3569483}

\leavevmode\vadjust pre{\hypertarget{ref-mehlElectronicallyActivatedRecorder2017}{}}%
Mehl, M. R. (2017). The {Electronically Activated Recorder} ({EAR}): {A Method} for the {Naturalistic Observation} of {Daily Social Behavior}. \emph{Current Directions in Psychological Science}, \emph{26}(2), 184--190. \url{https://doi.org/10.1177/0963721416680611}

\leavevmode\vadjust pre{\hypertarget{ref-millerSmartphonePsychologyManifesto2012}{}}%
Miller, G. (2012). The {Smartphone Psychology Manifesto}. \emph{Perspectives on Psychological Science: A Journal of the Association for Psychological Science}, \emph{7}(3), 221--237. \url{https://doi.org/10.1177/1745691612441215}

\leavevmode\vadjust pre{\hypertarget{ref-millingSpeechNewBlood2022}{}}%
Milling, M., Pokorny, F., Bartl-Pokorny, K., \& Schuller, B. (2022). Is {Speech} the {New Blood}? {Recent Progress} in {AI-Based Disease Detection From Audio} in a {Nutshell}. \emph{Frontiers in Digital Health}, \emph{4}, 886615. \url{https://doi.org/10.3389/fdgth.2022.886615}

\leavevmode\vadjust pre{\hypertarget{ref-montagWeStillNeed2022}{}}%
Montag, C., Dagum, P., Hall, B. J., \& Elhai, J. D. (2022). Do we still need psychological self-report questionnaires in the age of the {Internet} of {Things}? \emph{Discover Psychology}, \emph{2}(1), 1. \url{https://doi.org/10.1007/s44202-021-00012-4}

\leavevmode\vadjust pre{\hypertarget{ref-mullerDepressionPredictionsGPSbased2021}{}}%
Müller, S. R., Chen, X. L., Peters, H., Chaintreau, A., \& Matz, S. C. (2021). Depression predictions from {GPS-based} mobility do not generalize well to large demographically heterogeneous samples. \emph{Scientific Reports}, \emph{11}(1), 14007. \url{https://doi.org/10.1038/s41598-021-93087-x}

\leavevmode\vadjust pre{\hypertarget{ref-nadeauInferenceGeneralizationError2003}{}}%
Nadeau, C., \& Bengio, Y. (2003). Inference for the {Generalization Error}. \emph{Machine Learning}, \emph{52}(3), 239--281. \url{https://doi.org/10.1023/A:1024068626366}

\leavevmode\vadjust pre{\hypertarget{ref-pfeiferHowReadySpeechtotext2024}{}}%
Pfeifer, V. A., Chilton, T. D., Grilli, M. D., \& Mehl, M. R. (2024). How ready is speech-to-text for psychological language research? {Evaluating} the validity of {AI-generated English} transcripts for analyzing free-spoken responses in younger and older adults. \emph{Behavior Research Methods}. \url{https://doi.org/10.3758/s13428-024-02440-1}

\leavevmode\vadjust pre{\hypertarget{ref-posnerCircumplexModelAffect2005}{}}%
Posner, J., Russell, J. A., \& Peterson, B. S. (2005). The circumplex model of affect: {An} integrative approach to affective neuroscience, cognitive development, and psychopathology. \emph{Development and Psychopathology}, \emph{17}(3), 715--734. \url{https://doi.org/10.1017/S0954579405050340}

\leavevmode\vadjust pre{\hypertarget{ref-rcoreteamLanguageEnvironmentStatistical2021}{}}%
R Core Team. (2021). \emph{R: {A Language} and {Environment} for {Statistical Computing}}. Vienna, Austria: R Foundation for Statistical Computing.

\leavevmode\vadjust pre{\hypertarget{ref-russellCircumplexModelAffect1980a}{}}%
Russell, J. A. (1980). A {Circumplex Model} of {Affect}. \emph{Journal of Personality and Social Psychology}, \emph{39}, 1161--1178. \url{https://doi.org/10.1037/h0077714}

\leavevmode\vadjust pre{\hypertarget{ref-safdarVariationsEmotionalDisplay2009}{}}%
Safdar, S., Friedlmeier, W., Matsumoto, D., Yoo, S. H., Kwantes, C. T., Kakai, H., \& Shigemasu, E. (2009). Variations of emotional display rules within and across cultures: {A} comparison between {Canada}, {USA}, and {Japan}. \emph{Canadian Journal of Behavioural Science / Revue Canadienne Des Sciences Du Comportement}, \emph{41}(1), 1--10. \url{https://doi.org/10.1037/a0014387}

\leavevmode\vadjust pre{\hypertarget{ref-schererVocalCommunicationEmotion2003}{}}%
Scherer, K. (2003). Vocal communication of emotion: {A} review of research paradigms. \emph{Speech Communication}, \emph{40}(1-2), 227--256. \url{https://doi.org/10.1016/S0167-6393(02)00084-5}

\leavevmode\vadjust pre{\hypertarget{ref-schoedelSnapshotsDailyLife2023}{}}%
Schoedel, R., Kunz, F., Bergmann, M., Bemmann, F., Bühner, M., \& Sust, L. (2023). Snapshots of daily life: {Situations} investigated through the lens of smartphone sensing. \emph{Journal of Personality and Social Psychology}.

\leavevmode\vadjust pre{\hypertarget{ref-schoedelBasicProtocolSmartphone2020}{}}%
Schoedel, R., \& Oldemeier, M. (2020). \emph{Basic {Protocol}: {Smartphone Sensing Panel Study}}. \url{https://doi.org/10.23668/psycharchives.2901}

\leavevmode\vadjust pre{\hypertarget{ref-schullerSpeechEmotionRecognition2018}{}}%
Schuller, B. (2018). Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends. \emph{Communications of the ACM}, \emph{61}(5), 90--99. \url{https://doi.org/10.1145/3129340}

\leavevmode\vadjust pre{\hypertarget{ref-schullerINTERSPEECH2016Computational2016}{}}%
Schuller, B., Steidl, S., Batliner, A., Hirschberg, J., Burgoon, J., Baird, A., \ldots{} Evanini, K. (2016). \emph{The {INTERSPEECH} 2016 {Computational Paralinguistics Challenge}: {Deception}, {Sincerity} and {Native Language}} (p. 2005). \url{https://doi.org/10.21437/Interspeech.2016-129}

\leavevmode\vadjust pre{\hypertarget{ref-schwartzEmotionalSpeechProcessing2012}{}}%
Schwartz, R., \& Pell, M. D. (2012). Emotional {Speech Processing} at the {Intersection} of {Prosody} and {Semantics}. \emph{PLoS ONE}, \emph{7}(10), e47279. \url{https://doi.org/10.1371/journal.pone.0047279}

\leavevmode\vadjust pre{\hypertarget{ref-sridharUnsupervisedPersonalizationEmotion2022}{}}%
Sridhar, K., \& Busso, C. (2022). Unsupervised {Personalization} of an {Emotion Recognition System}: {The Unique Properties} of the {Externalization} of {Valence} in {Speech}. \emph{IEEE Transactions on Affective Computing}, \emph{13}(4), 1959--1972. \url{https://doi.org/10.1109/TAFFC.2022.3187336}

\leavevmode\vadjust pre{\hypertarget{ref-sunLanguageWellbeingTracking2020}{}}%
Sun, J., Schwartz, H. A., Son, Y., Kern, M. L., \& Vazire, S. (2020). The language of well-being: {Tracking} fluctuations in emotion experience through everyday speech. \emph{Journal of Personality and Social Psychology}, \emph{118}(2), 364--387. \url{https://doi.org/10.1037/pspp0000244}

\leavevmode\vadjust pre{\hypertarget{ref-tibshiraniRegressionShrinkageSelection1996}{}}%
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. \emph{Journal of the Royal Statistical Society. Series B (Methodological)}, 267--288.

\leavevmode\vadjust pre{\hypertarget{ref-vanrijnModellingIndividualCrosscultural2023}{}}%
van Rijn, P., \& Larrouy-Maestri, P. (2023). Modelling individual and cross-cultural variation in the mapping of emotions to speech prosody. \emph{Nature Human Behaviour}, \emph{7}(3), 386--396. \url{https://doi.org/10.1038/s41562-022-01505-5}

\leavevmode\vadjust pre{\hypertarget{ref-vlahosTalkMeHow2019}{}}%
Vlahos, J. (2019). \emph{Talk to {Me}: {How Voice Computing Will Transform} the {Way We Live}, {Work}, and {Think}}. Eamon Dolan Books.

\leavevmode\vadjust pre{\hypertarget{ref-vogtAutomaticRecognitionEmotions2008}{}}%
Vogt, T., André, E., \& Wagner, J. (2008). Automatic {Recognition} of {Emotions} from {Speech}: {A Review} of the {Literature} and {Recommendations} for {Practical Realisation}. In C. Peter \& R. Beale (Eds.), \emph{Affect and {Emotion} in {Human-Computer Interaction}} (Vol. 4868, pp. 75--91). Berlin, Heidelberg: Springer Berlin Heidelberg. \url{https://doi.org/10.1007/978-3-540-85099-1_7}

\leavevmode\vadjust pre{\hypertarget{ref-weidmanNotHearingHappiness2020}{}}%
Weidman, A. C., Sun, J., Vazire, S., Quoidbach, J., Ungar, L. H., \& Dunn, E. W. (2020). ({Not}) hearing happiness: {Predicting} fluctuations in happy mood from acoustic cues using machine learning. \emph{Emotion (Washington, D.C.)}, \emph{20}(4), 642--658. \url{https://doi.org/10.1037/emo0000571}

\leavevmode\vadjust pre{\hypertarget{ref-weningerAcousticsEmotionAudio2013}{}}%
Weninger, F., Eyben, F., Schuller, B. W., Mortillaro, M., \& Scherer, K. R. (2013). On the {Acoustics} of {Emotion} in {Audio}: {What Speech}, {Music}, and {Sound} have in {Common}. \emph{Frontiers in Psychology}, \emph{4}. \url{https://doi.org/10.3389/fpsyg.2013.00292}

\leavevmode\vadjust pre{\hypertarget{ref-wiltingRealVsActed2006}{}}%
Wilting, J., Krahmer, E. J., \& Swerts, M. G. J. (2006). Real vs. Acted emotional speech. \emph{Proceedings of the International Conference on Spoken Language Processing (Interspeech 2006)}.

\leavevmode\vadjust pre{\hypertarget{ref-wrightRangerFastImplementation2017}{}}%
Wright, M. N., \& Ziegler, A. (2017). Ranger: {A Fast Implementation} of {Random Forests} for {High Dimensional Data} in {C}++ and {R}. \emph{Journal of Statistical Software}, \emph{77}(1), 1--17. \url{https://doi.org/10.18637/jss.v077.i01}

\leavevmode\vadjust pre{\hypertarget{ref-wuMultimodalDataCollection2021}{}}%
Wu, C., Fritz, H., Bastami, S., Maestre, J. P., Thomaz, E., Julien, C., \ldots{} Nagy, Z. (2021). Multi-modal data collection for measuring health, behavior, and living environment of large-scale participant cohorts. \emph{GigaScience}, \emph{10}(6). \url{https://doi.org/10.1093/gigascience/giab044}

\end{CSLReferences}


\end{document}
