---
title             : "Semantic content outperforms speech prosody in predicting subjective affect experience"
shorttitle        : "Subjective Affect Experience in Speech"

author: 
  - name          : "Timo K. Koch"
    affiliation   : "1,2,3"
    corresponding : yes    # Define only one corresponding author
    address       : "Torstrasse 25, 9000 St. Gallen, Switzerland"
    email         : "timo.koch@unisg.ch"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Formal Analysis"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Gabriella M. Harari"
    affiliation   : "3"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
  - name          : "Ramona Schoedel"
    affiliation   : "2,4"
    role:
      - "Conceptualization"
      - "Writing - Review & Editing"
  - name          : "Zachariah Marrero"
    affiliation   : "5"
    role:
      - "Data curation"
  - name          : "Florian Bemmann"
    affiliation   : "6"
    role:
      - "Software"
  - name          : "Samuel D. Gosling"
    affiliation   : "5"
    role:
      - "Resources"
  - name          : "Markus Bühner"
    affiliation   : "2"
    role:
      - "Resources"
  - name          : "Clemens Stachl"
    affiliation   : "1"
    role:
      - "Writing - Review & Editing"
      - "Supervision"  

affiliation:
  - id            : "1"
    institution   : "Institute of Behavioral Science and Technology, University of St. Gallen"
  - id            : "2"
    institution   : "Department of Psychology, Ludwig-Maximilians-Universität München" 
  - id            : "3"
    institution   : "Department of Communication, Stanford University"
  - id            : "4"
    institution   : "Charlotte Fresenius Hochschule, University of Psychology"
  - id            : "5"
    institution   : "Department of Psychology, The University of Texas at Austin"
  - id            : "6"
    institution   : "Media Informatics Group, Ludwig-Maximilians-Universität München"
note: "\\clearpage" #added this to move entire author note to second page
authornote: |
  Conflict of interest: none
  
  Acknowledgements: We thank audEERING GmbH, Peter Ehrich, and Dominik Heinrich for their support with the technical implementation of the on-device voice feature extraction for Study 1, ZPID for the support with data collection for Study 1, and Sumer Vaid for the technical support with the set up of the computational environment for the analyses of Study 2. This project was supported by the Swiss National Science Foundation (SNSF) under project number 215303 and a scholarship of the German Academic Scholarship Foundation awarded to the first author.
  All code and data are available in the project’s repository on the Open Science Framework (OSF): https://osf.io/a5db8/

abstract: |
  Advances in the area of artificial intelligence (AI) and the ubiquity of speech data from novel data sources such as voice assistants have sparked pioneering research into the algorithmic recognition of affective states, leading to the development of numerous commercial products that claim to automatically recognize emotions from prosodic features of the human voice. However, these algorithms are predominantly trained on enacted or labeled speech samples from artificial lab settings representing affect *expression* and are used to infer everyday subjective affect *experience*. In this work, we investigate whether subjective momentary affect experience can be recognized from voice samples collected in the wild using machine learning. Across two studies, we extracted prosodic features as well as state-of-the-art word embeddings capturing semantic information from 24,766 speech samples, paired with experience-sampled self-reports on momentary affect experience, from 997 participants using off-the-shelf smartphones. We found that speech prosody conveyed only limited affective information (up to *r*~md~ = .16), while semantic features were predictive of experienced affect (up to *r*~md~ = .33). Additionally, we identified those prosodic features containing affective information. Moreover, our results suggest that the sentiment of the speech content does not affect predictions from prosodic features. Our findings challenge the generalizability of prediction results from past research and commercial products on the recognition of affect *expression* to the recognition of subjective affect *experience* from speech prosody. We discuss the implications for the algorithmic monitoring of affect experience from speech in everyday life.
  
keywords          : "Affect, Speech, Prosody, Voice, Machine Learning"
wordcount         : "6446"

bibliography      : ["../speech_paper_library.bib"]

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

documentclass     : "apa6"
classoption       : "man"
output: 
  papaja::apa6_word:
    figsintext: yes

---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

<!-- One catchy intro paragraph on voice AI boom and opportunities / promises of affect recognition research, commercial tools and applications - relevance and link to study - especially focus on excitement around voice! -->
Verbal communication is a fundamental modality to convey emotional information [@schererVocalCommunicationEmotion2003; @krausVoiceonlyCommunicationEnhances2017]. Prosodic voice cues, such as tone, pitch, and rhythm, combined with the semantic content of speech, enable listeners to infer the speaker's affective state. Researchers have investigated variations in prosodic features with affect for decades and, more recently, trained algorithms to automatically recognize affective states and related affective disorders from voice, offering promising potential applications, including (mental) health care, human-machine interaction, education, and business [@millingSpeechNewBlood2022; @hildebrandVoiceAnalyticsBusiness2020; @vlahosTalkMeHow2019]. The widespread availability of speech data from voice assistants (e.g., Amazon's Alexa and Apple's Siri) has also spurred commercial interest in deploying affect-detection algorithms in everyday life. These tools aim to quantify at scale how patients, customers, and employees feel in a given moment, often to provide personalized feedback and services (e.g., @matzUsingBigData2017). However, existing affect-recognition models in research and commercial tools are predominantly trained on enacted or labeled speech samples from artificial lab settings, which represent affect *expressions*, but are deployed to detect people’s subjective affect *experience* in real-world contexts. This discrepancy raises concerns about the models' generalizability and their ability to accurately detect subjective affect experience in everyday life. The present work addresses this gap by investigating the algorithmic recognition of subjective self-reported momentary affect experience from speech samples collected via smartphones in everyday life.

## Predicting affect from prosody

<!-- voice cues AND speech content; Studies on affect expression, very good performance reported!, difference versus affect experience  -->
Past work suggests that affective states (e.g., short-lived emotions) are predictable from a range of speech data, such as labeled TV clips [@grimmVeraAmMittag2008], recorded phone calls [@burkhardtDatabaseGermanEmotional2005a], and enacted speech samples from the lab [@banzigerIntroducingGenevaMultimodal2012; @schullerSpeechEmotionRecognition2018; @vogtAutomaticRecognitionEmotions2008]. They reported very high performance in the algorithmic recognition of affective arousal (*r*~max~ = .81) and valence (*r*~max~ = .68) [@weningerAcousticsEmotionAudio2013]. In those previous studies, the targeted affective states were either enacted by professional actors or assigned labels by raters, serving as the ground truth. This approach comes with a set of downsides, such as actors potentially overacting and the ambiguity of ground truth due to the subjective nature of labeling [@batlinerAutomaticRecognitionEmotions2011; @schullerSpeechEmotionRecognition2018; @wiltingRealVsActed2006]. Moreover, by relying on actors or raters, past research focused on recognizing affect *expression* rather than affect *experience*. Investigating affect expressions is invaluable for understanding emotional communication, but it is phenomenologically distinct from affect experience. While there is often overlap between the emotions we express and those we experience, they are not identical. Affect expression encompasses the observable emotional behavior derived from our internal affect experience. Expressions can be intentionally moderated, socially conditioned (e.g., the Pan-Am smile), or culturally predisposed (e.g., variations in emotional expression across countries; [@safdarVariationsEmotionalDisplay2009]). In contrast, affect experiences reflect an individual's genuine internal state. However, "feeling is not always revealing"; one may not necessarily express what one experiences affectively and might even express something entirely different [@grossRevealingFeelingsFacets1997; @grossDissociationEmotionExpression2000]. Additionally, the expression of affect can be perceived and interpreted differently due to interindividual, situational, and cultural differences. We argue that the experience of affect is the key criterion of interest for both researchers and commercial actors. It provides a deeper understanding of how individuals truly feel rather than just how they outwardly express themselves, which can be inaccurate, superficial, or misleading. Focusing more on affective experience over mere expressions could lead to more accurate and meaningful interpretations of affective states, which is crucial for developing empathetic technologies and fostering meaningful interactions.

<!-- studies on everyday speech + EAR and its downsides + how smartphones can help -->
Studies predicting subjective affect experience from speech collected in everyday life are rare and algorithmically recognizing real-life *experienced* affect is considered more challenging than affect expressions [@vogtAutomaticRecognitionEmotions2008]. Some early, pioneering work has collected everyday speech samples using the Electronically Activated Recorder (EAR), a small electronic device usually attached to one's clothes that captures snippets of ambient sounds and conversations throughout the day [@mehlElectronicallyActivatedRecorder2017]. Hereby, everyday speech samples can be unobtrusively collected over a period of time which allows researchers to not only investigate between-person differences in affect experience (i.e., is this person sad?), but also assess within-person fluctuations over time (i.e., is this person more sad than other days?) [@huangPredictionEmotionChange2018; @weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020]. Using the EAR can raise issues regarding privacy protection since potentially non-consenting persons may be recorded, too. Also, the continuous and unobtrusive collection of auditory data from microphones is not legally possible in many countries, raising ethical issues. Moreover, handling the EAR recorders and transmitting the collected data can be tedious for participants and researchers. To overcome these challenges, off-the-shelf smartphones represent a useful platform to collect in-situ samples of momentary affect experience over time and make corresponding speech records using the built-in microphone [@weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020; @carlierSearchStateTrait2022].

## Prosody-semantics interactions in affect recognition from speech
<!-- considering the interplay of speech from and content: WHAT people talk about and how it impacts acoustics, comparing phonetic and semantic predictions, how semantic content affects acoustics, how this could be investigated using smartphones -->
Prosody and semantic content interact when transmitting affective information through speech [@ben-davidProsodySemanticsAre2016]. Moreover, some studies even suggest that there is a prosodic (i.e., from voice cues) dominance in the perception of affect based on data from lab experiments [@ben-davidProsodySemanticsAre2016; @linyiProsodyDominatesSemantics2020], but not (yet) using speech data from the wild [@schwartzEmotionalSpeechProcessing2012]. Moreover, while this research field has focused on the interplay of prosody and semantics in the recognition of affect by human raters, there are, to our knowledge, no studies on prosody-content interactions in algorithmic affect detection. Therefore, the current state of research leaves the question unanswered of whether the content sentiment of user utterances (i.e., the inherent emotional charge) influences the prosodic features that underpin automated affect recognition. This ambiguity extends to practical applications: Can algorithms effectively discern affective states irrespective of the emotionality of the topic of conversation, whether it is neutral (e.g., talking about the weather) or inherently emotional (e.g., reuniting with a loved one)?

## The sound of (expressed) emotional prosody
<!-- summarize key findings of affect-prosody links, BUT again only regarding affect expressions! -->
Prior research suggests that pitch, volume, speech rate, and voice quality, or timbre, serve as central cues in conveying affect. For instance, higher pitch and faster speech rates are generally associated with excitement or stress [@banseAcousticProfilesVocal1996]. However, while there is a degree of consistency in how emotions are expressed and perceived through prosody, significant variability exists in these affect-voice mappings [@larrouy-maestriSoundEmotionalProsody2024; @banseAcousticProfilesVocal1996; @schererVocalCommunicationEmotion2003]. One source of variability is the cultural background of the person perceiving emotional speech [@vanrijnModellingIndividualCrosscultural2023]. This variability underscores the challenges faced by algorithms in affect recognition, emphasizing that while certain prosodic features might be universally recognized, their interpretation can vary between individuals. However, again, those universal findings on affect-prosody links are grounded on data from enacted emotional speech or labeled samples, representing affect expressions. Hence, it remains unclear if the discovered markers of expressed emotional prosody generalize to subjective affect experience.

<!-- conclusion of intro & theory, wrap up, lead to this work and what we do in the two studies, present tense! -->
Across two large-scale field studies, the present work investigates the algorithmic recognition of momentary subjective affect experience from speech prosody collected via smartphones in everyday life. In the first study, we control for the semantic content by having participants read out scripted sentences and predicting self-reported affect experience from extracted prosodic features. In the second study, participants could speak spontaneously about their current situation, thoughts, and feelings. Then, we predicted self-reported affect experience from prosody as well as semantic content in the form of state-of-the-art word embeddings. Moreover, we analyzed which prosodic cues carry the most affective information across data sets and investigated the effects of the sentiment of speech's content on algorithmic affect recognition from prosodic features in prediction models. 

# Results

## Semantic content outperforms prosody in predicting subjective affect experience

```{r prediction results, echo = FALSE, warning = FALSE}

library(mlr3)

#load benchmark results (study 1)
bmr_study1 <- readRDS("../results/study1/bmr_study1.rds") 

bmr_egemaps_gender_study1 <- readRDS("../results/study1/bmr_egemaps_gender_study1.rds") 

#load benchmark results (study 2)
bmr_study2 <- readRDS("../results/study2/bmr_study2.rds")

bmr_egemaps_gender_study2 <- readRDS("../results/study2/bmr_egemaps_gender_study2.rds")

## modify rho to handle NAs and aggregate to the median 
md_rho = msr("regr.srho")
md_rho$aggregator = function(x) median(x, na.rm = TRUE)

md_rsq = msr("regr.rsq")
md_rsq$aggregator = function(x) median(x, na.rm = TRUE)

mes = c(md_rho, md_rsq) # set performance measures

# get aggregated performance measures (study 1)
mes_study1 <- bmr_study1$aggregate(mes)

mes_egemaps_gender_study1 <- bmr_egemaps_gender_study1$aggregate(msrs(c("classif.acc", "classif.auc")))

# get aggregated performance measures (study 2)
mes_study2 <- bmr_study2$aggregate(mes)

mes_egemaps_gender_study2 <- bmr_egemaps_gender_study2$aggregate(msrs(c("classif.acc", "classif.auc")))

```

<!-- overall into +  prediction from voice cues (study 1 and 2)-->
Overall, our results suggest that self-reported momentary subjective affect experience could be better predicted from semantic content than from prosodic features (see Figure \@ref(fig:predictionoverview) for an overview). In the following, we report on the best performing algorithm for each prediction task respectively (either Random Forest or LASSO). Specifically, for scripted speech content (Study 1), the prediction of affective arousal from prosody (*r*~md~ = `r round(mes_study1[which(mes_study1$learner_id == "imputehist.regr.cv_glmnet" & mes_study1$task_id  == "egemaps_arousal")]$regr.srho, 2)`) was low yet significantly better than chance (*p* = .01). On the contrary, valence predictions from prosodic features were on average even lower and did not yield significant prediction results (*r*~md~ = `r round(mes_study1[which(mes_study1$learner_id == "imputeoor.regr.ranger" & mes_study1$task_id  == "egemaps_valence")]$regr.srho, 2)`, *p* = 1). Models trained on prosodic features from participants talking spontaneously about their current situation, thoughts, and feelings (Study 2) yielded a low prediction performance, too. Still, the prediction of arousal (*r*~md~ = `r round(mes_study2[which(mes_study2$learner_id == "imputehist.regr.cv_glmnet" & mes_study2$task_id  == "egemaps_arousal")]$regr.srho, 2)`, *p* < .001 ) was significantly better than the baseline. Also, predictions of contentedness (*r*~md~ = `r round(mes_study2[which(mes_study2$learner_id == "imputehist.regr.cv_glmnet" & mes_study2$task_id  == "egemaps_content")]$regr.srho, 2)`, *p* < .001) were significantly better than chance, but not for sadness (*r*~md~ = `r round(mes_study2[which(mes_study2$learner_id == "imputehist.regr.cv_glmnet" & mes_study2$task_id  == "egemaps_sad")]$regr.srho, 2)`, *p* = .33). 
<!-- suppl analyses: using larger acoustic feature set did not improve acoustic predictions, ie no issue with voice features + within pers fluct-->
Further, as done in prior research on voice-affect predictions [@weidmanNotHearingHappiness2020], we also compared the prediction performance of machine learning models trained on the much larger Compare2016 (6,737 features) acoustic feature set [@schullerINTERSPEECH2016Computational2016] in contrast to the economic eGeMAPS (88 features) feature set we had used.  Just as in prior research, the larger voice feature set did not yield better affect predictions [@weidmanNotHearingHappiness2020]. Detailed prediction results are provided in the online supplements. Moreover, to further relate our work to prior work in the field [@weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020], we also conducted supplementary analyses for the prediction of within-person fluctuations in affect experience. Overall, those yielded on average lower prediction performance than between-person differences as found in prior work [@weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020]. All results can be found in the online supplements. 

<!-- study 2: word embeddings and combined voice + embeddings -->
On the contrary, prediction models trained on semantic content in the form of word embeddings that had been extracted from participants talking spontaneously yielded good performance results. Here, word embeddings were predictive of contentedness (*r*~md~ = `r round(mes_study2[which(mes_study2$learner_id == "imputehist.regr.cv_glmnet" & mes_study2$task_id  == "wordembeddings_content")]$regr.srho, 2)`), arousal (*r*~md~ = `r round(mes_study2[which(mes_study2$learner_id == "imputehist.regr.cv_glmnet" & mes_study2$task_id  == "wordembeddings_arousal")]$regr.srho, 2)`), and sadness (*r*~md~ = `r round(mes_study2[which(mes_study2$learner_id == "imputehist.regr.cv_glmnet" & mes_study2$task_id  == "wordembeddings_sad")]$regr.srho, 2)`).
Finally, models trained on prosodic features and word embedding combined yielded on average a similar prediction performance compared to those models trained on word embeddings only. This pattern suggests that predictions were mostly driven by the information coming from semantic content as captured by word embeddings. 

```{r predictionoverview, echo = FALSE, warning=FALSE, fig.cap="Box and whisker plot of out-of-sample prediction performance measures from five times repeated ten-fold cross-validation for each affect measure. Feature sets (either prosodic or semantic features) for model training are shown in parentheses. Names of significant models are displayed in boldface.", fig.scap= "Prediction performance", out.width="100%", fig.align="center",  fig.pos. = "H", results="asis"}

knitr::include_graphics(path = "../figures/bmr_plot_edited.png")
```

<!-- give some details on predictive word embeddings in study 2 ?? No! very student specific! -->
## Content sentiment has no effect on affect predictions from prosody

```{r sentenceanova, echo = FALSE, warning = FALSE, results = FALSE, message = FALSE}

library(car)

predictions_condition <- readRDS(file="../results/study1/predictions_condition.RData")

# run anova comparing means for valence 
aov_valence <- car::Anova(aov(error_valence ~ condition, data = predictions_condition))

# run anova comparing means for arousal
aov_arousal <- car::Anova(aov(error_arousal ~ condition, data = predictions_condition))

```

<!-- study1: is absolute error different across conditions -->
For scripted speech (Study 1), we analyzed if the experimentally altered sentiment (positive/ neutral/ negative) of the predefined sentences had an effect on affect predictions from prosody. There were no significant differences in prediction errors across the three sentence conditions for valence (*F*~(2,9905)~ = `r round(aov_valence[3][[1]][1],2)`,
*p* = .91) and arousal (*F*~(2,9905)~ = `r round(aov_arousal[3][[1]][1],2)`,
*p* = .68) predictions suggesting that sentences' sentiment did not influence affect predictions from prosodic features.

```{r sentimentcorrs, echo = FALSE, warning = FALSE, results = FALSE, message = FALSE}

predictions_sentiment <- readRDS(file="../results/study2/predictions_sentiment.rds")

cor_error_content <- cor.test(predictions_sentiment$sentiment, predictions_sentiment$error_content)
cor_error_arousal <- cor.test(predictions_sentiment$sentiment, predictions_sentiment$error_arousal)
cor_error_sad <- cor.test(predictions_sentiment$sentiment, predictions_sentiment$error_sad)

affect_voice_wordembeddings_study2 <- readRDS("../data/study2/affect_voice_wordembeddings_study2_ml.rds")

```

<!-- study 2: (no) Effects of sentiment on voice predictions, maybe (!) a little bit for contentedness - but keep in mind that predictions were not significant overall! -->
Further, we investigated the effect of speech sentiment on affect predictions from prosody when participants could talk spontaneously about their current situation, thoughts, and feelings (Study 2). Here, we analyzed the correlation of content sentiment with the absolute prediction errors in the prediction of self-reported momentary arousal, contentedness, and sadness from prosodic features. Results indicate that content sentiment did not have an effect on affect predictions from prosody: Correlations of content sentiment with absolute predictions errors were not significant for neither contentedness (*r* = `r round(cor_error_content$estimate[[1]], 2)`, *p* = `r round(cor_error_content$p.value, 2)`, sadness (*r* = `r round(cor_error_sad$estimate[[1]], 2)`, *p* = `r round(cor_error_sad$p.value, 2)`), nor arousal (*r* = `r round(cor_error_arousal$estimate[[1]], 2)`, *p* = `r round(cor_error_arousal$p.value, 2)`). When interpreting those results, one has to keep in mind that the overall predictive performance of the models trained on speech prosody was generally low. We provide additional figures illustrating the residuals in prediction models in the online supplements. 
<!-- combined error figure goes to supplements or put here? -->

## Prosodic predictors of affect experience 
<!-- general description and info-->
For models with better-than-chance predictions performance (i.e., arousal predictions in Study 1 and 2 as well as contentedness predictions in Study 2), we investigated the most informative features in a LASSO regularized model that had been trained on the full data sets to capture the most information possible. As a consequence, any findings need to be interpreted in the context of this specific work. The LASSO algorithm selects those voice parameters from the initial set of 88 prosodic features that are most informative and assigns those respective beta coefficients while setting the beta weights of uninformative variables to zero. Figure \@ref(fig:lassobetas) provides an overview of the top five selected prosodic features for each affect target in the regularized regression and their standardized beta regression weights. All regression coefficients and descriptive (in-sample) correlations of eGeMAPs voice features and self-reported affect experience measures are supplied in the online supplements. 
<!-- arousal predictions across studies -->
For the prediction of momentary subjective arousal, the non-zero beta coefficients suggest that more features related to prosody were selected in Study 1 (scripted speech; p~retained~ = 15) in comparison to Study 2 (spontaneous speech; p~retained~ = 9) with higher absolute average coefficients (B~meanS1~ = .14; B~meanS2~ = .04). 

Arousal in scripted speech was positively associated with variability in the first and third formant (F1 and F3) frequencies, indicating dynamic vowel articulations. Formants are resonance frequencies in the vocal tract, and F3 is particularly important for speech clarity and vowel quality. The first formant frequency is related to vowel height, and variability in F1 suggests more dynamic and varied vowel articulations, correlating with heightened arousal. Arousal was also linked to higher spectral flux in unvoiced regions, reflecting energetic speech as rapid changes in the power spectrum of unvoiced sounds occur. Conversely, arousal was negatively associated with variation in the Hammarberg Index which measures the relative energy concentration in the higher frequencies compared to the lower frequencies within voiced speech segments as well as the average spectral slope. Lower energy in higher frequencies and a steeper energy drop-off (indicated by the negative association with the spectral slope) characterize less aroused speech, suggesting that calmer speech tends to have more energy concentrated in lower frequencies and a darker sound quality.

Arousal in spontaneous speech showed positive associations with average loudness (median and 80th percentile), indicating that a louder voice was associated with higher experienced arousal. Self-reported arousal also had a weak positive association with F1 frequency variability. This suggests that greater variability in the first formant frequency, which is related to vowel height, reflects more dynamic and varied voice, often associated with heightened arousal. Additionally, there was a negative association with the variability in the amplitude of the third formant relative to the fundamental frequency (F0). This indicates that more consistent amplitudes of the third formant (F3) relative to F0 are linked to lower subjective arousal. In other words, when the intensity of higher resonances is stable, it suggests a calmer, less aroused state, whereas greater variability in these higher resonance amplitudes may indicate higher arousal levels.

<!-- contentedness -->
For the recognition of contentedness from spontaneous speech, the LASSO algorithm selected a total of eleven voice parameters with overall smaller beta coefficients (B~meanS2~ = .01) compared to arousal predictions. Experienced contentedness in spontaneous speech was positively associated with loudness (mean and 80th percentile), equivalent sound level, and average spectral flux in voiced regions, indicating that a louder and dynamically changing voice was indicative of higher subjective contentedness. Moreover, there was a weak negative association with the variability in the first Mel-Frequency Cepstral Coefficient (MFCC1) in voiced regions, suggesting that when the spectral shape of the voiced sounds remains more stable and consistent (i.e., there is less variation in MFCC1), was linked to higher levels of contentedness. In other words, less fluctuation in the spectral properties of the voice may reflect a more content affect state.

```{r lassobetas, echo = FALSE, warning=FALSE, fig.cap="Standardized beta coefficients for eGeMAPs voice features from LASSO (Least Absolute Shrinkage and Selection Operator) regularized regression models trained on the full data sets. The top five prosodic features per affect target that have been selected by LASSO models and their standardized beta coefficients are displayed. The target variables have been standardized to account for their varying scaling in the data sets.", fig.scap= "LASSO betas", out.width="100%", fig.align="center",  fig.pos. = "H", results="asis"}

knitr::include_graphics(path = "../figures/betas_plot.png")
```

# Discussion

<!-- mini summary of findings, use past tense here -->
Affect-recognition algorithms in research and commercial tools are predominantly trained on prosody from enacted or labeled speech samples from artificial lab settings, which represent affect expressions, but are deployed to detect people’s subjective affect experience in real-world contexts. In the present work, we predicted subjective momentary affect experience from prosodic and semantic features in naturalistic speech samples collected using machine learning. In contrast to prior work on the algorithmic recognition of affect expression, our findings suggest that prosody provides only limited predictive information on subjective affect experience while semantic content in the form of word embeddings is predictive of subjective affect experience. Moreover, experimental (Study 1) and exploratory (Study 2) findings imply that content sentiment does not impact affect predictions from prosody. 

<!-- prosody: comparing our prosody results to prior find: comparable performance to affect experience but lower performance than affect expression predictions; why perf is lower here -->
The outcome that prediction models trained on prosodic features achieved lower prediction performance compared to prior work on the automatic prediction of affect expression [e.g., @schullerSpeechEmotionRecognition2018], supports the notion that recognizing real-life affective states algorithmically is inherently more challenging than detecting enacted emotions [@vogtAutomaticRecognitionEmotions2008]. Nevertheless, our reported prediction performance aligns with previous studies predicting subjective affect experience from speech samples collected in natural settings [@weidmanNotHearingHappiness2020; @carlierSearchStateTrait2022; @sunLanguageWellbeingTracking2020]. Another reason for the lower prediction performance in our and other work on naturally sampled affect experiences compared to prior work on recognizing affect expressions could be the fact that data sets contain relatively few instances of extreme affect experiences compared to those used in studies focusing on enacted or labeled affective states. Consequently, our predictions targeted mood states, which are inherently less intense than short-lived emotions that had been investigated in prior work [@schererVocalCommunicationEmotion2003], and thus present a greater challenge for algorithmic recognition [@vogtAutomaticRecognitionEmotions2008].
<!-- prosody: arousal predictions from prosody worked across two studies, indicating that arousal could be algorithmically detected more easily -->
Moreover, the prediction of arousal from prosody was successful across studies whereas the prediction of emotional valence only yielded significant predictions for contentedness, but not for overall valence and sadness. This finding is in line with prior work showing that emotional valence is more challenging to automatically infer than arousal due to its subjective nature [@sridharUnsupervisedPersonalizationEmotion2022]. 

<!--  semantics (word embeddings): way superior to voice cues (study 2) + what are implications? -->
Content semantics in the form of state-of-the-art word embeddings showed a superior affect prediction performance compared to prosody suggesting that speech content could be more informative for algorithms to recognize experienced affect than prosodic cues. This finding contrasts prior research suggesting that speech prosody is more relevant for human affect inferences than semantics [@ben-davidProsodySemanticsAre2016; @linyiProsodyDominatesSemantics2020], while aligning with studies that found semantic content to be more predictive than prosodic cues in algorithmically predicting the momentary subjective experience of happiness [@weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020]. As an explanation, we argue that algorithms are potentially better at detecting signals from the structured nature of text, in contrast to the complexity of prosody, such as subtle nuances of intonation, where humans excel since the ability to interpret affective prosodic cues is deeply rooted in our nature.

<!-- wrap up pred: we question the GENERALIZABILITY of prior findings to subjective affect experience and NOT findings themselves; implications, role of smartphones -->
Generally, our findings challenge the generalizability of the impressive prediction results from past work on the recognition of affect *expression* (e.g., in enacted speech) to the recognition of subjective affect *experience* from speech prosody. Thereby, our findings also question the proclaimed performance of commercial affect recognition algorithms deployed in daily life that have been trained on enacted or labeled affect *expression* and are used to infer affect *experience*. Consequently, current expectations regarding the performance of affect-detecting algorithms, especially the ones that are focused on prosodic features, applied to everyday speech might be overly optimistic. Hence, more research is needed to determine how well algorithms can pick up on subjective affect experience from natural speech. Here, smartphones could play a prominent role in collecting and analyzing speech data and corresponding in-situ self-reports on subjective affect experience for affect inferences. Building on our work, smartphones could be utilized as mobile experimental labs to study various aspects of affect recognition from speech, such as experimentally varying the semantic content as done in Study 1 [@millerSmartphonePsychologyManifesto2012].

<!-- the role of content on algo affect recognition from prosody: Sentence sentiment condition (study 1) and sentiment score (study 2) -->
We experimentally varied the emotional valence of the spoken content in Study 1 and exploratively investigated the effect of content sentiment on voice predictions in Study 2. Our findings suggest that the sentiment of the content that participants talked about did not have an impact on the prediction of affect experience from prosody. This could imply that the sentiment of the content that people talk about does not affect model performance when algorithmically inferring affect experience from prosodic features. However, this finding has to be considered in the light of the overall limited predictive performance in this work. Hence, more research is needed to disentangle speech content and prosody in automatic affect experience recognition.
<!-- some words on scripted sentences vs prompted speech , valence was better in study 2, but might be due to specific target -->
Moreover, while the predefined sentences in Study 1 allowed us to control for the sentiment of the content of participants' voice records, they were unable to express themselves spontaneously, which could have impaired prediction performance from prosodic features compared to Study 2 where participants could talk spontaneously. As a result, researchers and practitioners should consider the context of speech production and keep in mind that findings and trained models might be specific to the given production context and do not necessarily generalize to other production contexts. 

<!-- affect-prosody links: summarize findings and embed our findings into existing lit  -->
Across the detected prosody-affect links in this work, a number of spectral properties (Spectral flux, formant amplitudes, and bandwidths) and voice quality indices (Hammarberg Index) showed as important for the prediction of affect experience. This mix underscores that affective traces in voice are multifaceted, with both the static qualities of sound and their temporal variations being crucial. The presence of features unique to either arousal (e.g., variation in F1 frequency) or contentedness (e.g., MFCC1) predictions further illustrates that while some prosodic features are broadly applicable to emotional expression, others more specifically align with the nuances of emotional intensity (arousal) or the affective valence conveyed (contentedness). These voice features have also been associated with affect expressions in prior work suggesting that those prosody-voice links generalize to affect experience [@larrouy-maestriSoundEmotionalProsody2024; @banseAcousticProfilesVocal1996; @vanrijnModellingIndividualCrosscultural2023]. Here, future research could expand on our work and investigate affect-prosody associations in more contexts.

## Limitations and outlook

<!-- limitation 1: operationalizations / sample composition / slightly different target variable and participants in the two samples  -->
The findings of this work are limited in multiple ways. First, we used slightly different operationalizations of affect experience and sample compositions in the two studies that might affect their comparability: In Study 1, we assessed valence and arousal on a six-point Likert scale. In Study 2, we used two items to assess affective valence (contentedness and sadness) and arousal on five-/four-point Likert scales. As a consequence, findings might be transferable but not directly comparable across studies. We addressed this limitation by ensuring consistent methodologies and statistical approaches in both studies, allowing us to draw meaningful conclusions from our analyses. Further, while Study 1 drew on a representative German sample, Study 2 was based on a student cohort sample from the United States with the respective limitations, such as potential constraints in generalizability [@mullerDepressionPredictionsGPSbased2021]. Also, the German data set that was analyzed in Study 1 only contained Android users because of the technical requirements of the logging software. We do not expect this to impact our results since past research suggests that the selection bias regarding participant demographics and personality for the German population is negligible [@schoedelSnapshotsDailyLife2023; @gotzUsersMainSmartphone2017; @keuschCoverageErrorData2020]. Moreover, both data sets have been collected in western countries, specifically Germany and the United States. Prior research suggests that cultural differences in emotion experience exist [@limCulturalDifferencesEmotion2016] and that mood inference models from, for example, smartphone sensing data do not necessarily generalize to other countries and cultures [@meegahapolaGeneralizationPersonalizationMobile2023]. Therefore, future studies should draw on diverse international samples from different cultural contexts in non-western countries.

<!-- limitation 2: how we collected the speech data (still artificial records) and ESM data + we do not know for sure that proper records have been made in study1 -->
Moreover, in contrast to prior work using passive speech sensing (e.g., via the EAR; [@weidmanNotHearingHappiness2020]), the participants had to actively log their speech in the present work. This artificial setting might have had an effect on results. Hence, the findings of this study might be subject to the specific instructions related to the audio records. In this manner, future work should employ multiple different speech tasks for affect predictions and investigate how well predictions generalize from one task to another.
Also, similar to other work using the experience sampling method, our data might be impacted by people's ability to complete experience samplings and make speech records in a given moment (e.g., phone nearby, driving). Moreover, study participants knew that their affect self-reports and corresponding speech samples would be recorded and later analyzed. As a consequence, with regard to assessed subjective affect experience, participants might have only completed the experience sampling questionnaire in selected affective situations, for example not when they were experiencing extreme affective states, or they had not reported on extreme affect at all [@schoedelSnapshotsDailyLife2023]. Moreover, participants might have not spoken as naturally as they would if they had not known that their data would be scientifically analyzed. Here, participants might have made the audio records only in selected suitable situations, for example when they were alone in a quiet place.

Additionally, specifically to Study 1, one limitation lies in our privacy-preserving on-device data pre-processing approach. By applying on-device feature extraction, we had limited possibilities to check in detail if participants truly complied with study instructions and had recorded their voice while reading out the predefined sentences accordingly (beyond the data-driven quality checks we had applied). Further, our approach did not allow us to control for records' background noise (e.g., when participants were outside next to a road) or how they held their smartphone during the voice recording. However, we excluded samples without human voice based on voice indicators and speaker gender predictions that served as a sanity check yielded very good prediction results (prediction accuracy = `r round(mes_egemaps_gender_study1[which(mes_egemaps_gender_study1$learner_id == "imputeoor.classif.ranger" & mes_egemaps_gender_study1$task_id  == "egemaps_gender_study1")]$classif.acc[1]*100, 2)`%) indicating good data quality. Future research could investigate additional data-driven approaches to check speech data quality directly on the device. Finally, in future work, smartphones could be used to log and pre-process participants' everyday speech by using pre-trained language models to extract content features (e.g., specific topics or word embeddings) directly on the device, too. Thereby, no raw speech data would have to be transferred to a server and valuable information of language's content could be also used for privacy-respectful affect recognition.

<!-- limitation 3: everyday affect in this sample vs. extreme cases in prior research, future studies could use this approach for clinical studies -->
Furthermore, the data in this work consists of in-situ self-reports of affect experience from participants' everyday life in a non-clinical population. As a result, the data represents the "normal" everyday moods of regular people with only few cases of extreme affect experience. As a result, the trained affect recognition models should be considered in this context. If the conducted analyses were to be replicated in a clinical sample that contains more cases of extreme affect experience, prediction performances might be even higher because the affective signals in speech prosody could be more distinct. This does not necessarily mean, however, that findings and prediction models from the present work generalize well to a clinical sample. Additionally, future studies could aim to collect language samples when participants are known to experience strong emotions, for example based on their physiological signals [@hoemannContextawareExperienceSampling2020a]. 

<!-- measurement error in self reports, particularly for single-item measures! + future outlook -->
Finally, the ground truth, i.e., the information that is assumed to be fundamentally true, used for model training and evaluation are self-reports on participants' subjective affect experience that are prone to response biases [@gaoInvestigatingReliabilitySelfreport2021]. The introduced measurement error can have an impact on predictive modeling [@jacobucciMachineLearningPsychological2020]. Moreover, we employed single items to assess momentary affect experience on different dimensions as we wanted to assess multiple other psychological constructs in the studies. This is an established approach to reduce participant burden, but it can introduce additional measurement error [@dejonckheereAssessingReliabilitySingleitem2022]. Therefore, future studies that are particularly interested in subjective affect experience should use multiple items assessing a broad range of affect experience in an intensive longitudinal design. 

Moreover, there is a debate on how much of an underlying psychological construct, i.e., of subjective affect experience, scientists can capture using questionnaires: In order to assess one's affect experience most accurately through a survey item, one needs to have introspection to recognize their current affective state and have an adequate understanding of how to translate it to a numerical value on a scale [@montagWeStillNeed2022; @boydPersonalityPanoramaConceptualizing2020]. However, people can vary greatly in that regard [@critchleyInteroceptionEmotion2017]. Possibly, in the future, algorithms can replace self-report questionnaires altogether by analyzing natural language directly since transformers (that produce word embeddings as employed in Study 2) have been reported to converge with rating scales with an accuracy that approach the theoretical upper limits already [@kjellNaturalLanguageAnalyzed2022].

# Conclusion

<!-- summary of discussion: predictions for experience are worse than for expression, semantics superior to prosody, semi-free prompt better than fixed content, smartphones are awesome to collect data! -->
In this work, we investigated whether machine learning algorithms can recognize subjective affect experience from samples of speech prosody collected in everyday life. Extracted prosodic voice parameters provided only limited predictive information of affective arousal across both studies, while semantic content as reflected in state-of-the-art word embeddings had been shown to be predictive of arousal as well emotional valence. Further, our findings suggest that emotional speech content did not affect predictions from prosody alone (i.e., what someone talked about did not affect how well emotions could be predicted from voice cues). Our findings challenge the generalizability of the optimistic prediction results from prior research work and commercial emotion-detection AI algorithms on the recognition of affect *expression* (e.g., enacted and labeled speech) to the recognition of subjective affect *experience* in everyday speech. The ability to detect how people genuinely feel, as opposed to merely observing what they outwardly express, is crucial to gaining a deeper understanding of human emotions and guiding the development of technologies that better serve and respond to our emotional needs. 

\newpage
# Methods

## Data collection

The present work consists of two studies using machine learning that are based on separate data sets. The workflow of data collection, processing, and predictive modeling is described in detail in the following section and illustrated in Figure \@ref(fig:methodsoverview).

```{r methodsoverview, echo = FALSE, warning=FALSE, fig.cap="Flow diagram illustrating the data collection, processing, and predictive modeling in Study 1 and Study 2.", fig.scap= "Method diagram", out.width="100%", fig.align="center", fig.pos. = "H", results="asis"}

knitr::include_graphics(path = "../figures/methods_figure.png")
```

### Study 1

<!-- Study data collection description + ESM -->
Data collection for Study 1 was part of a large panel study (from May 15 until November 14, 2020) using the *PhoneStudy* research app [@schoedelBasicProtocolSmartphone2020]. Data collection was approved by the ethics committee of the psychology department at LMU Munich and all procedures adhered to the General Data Protection Regulation (GDPR). We recruited a quota sample of *N* = 850 participants representative of the German population regarding age, gender, education, income, confession, and relationship status. Participants had to be between 18 and 65 years old, fluent in German, and in the possession of an Android smartphone (Android 5 or higher) for private usage as a sole user (for more details on the panel study see @schoedelSnapshotsDailyLife2023 and  @grossedetersKeepScrollingUsing2024). The study also comprised two two-week experience sampling phases (July 27, 2020, to August 9,2020; September 21, 2020, to October 4, 2020) during which participants received two to four short questionnaires per day on their personal smartphone. Here, self-reported valence and arousal were assessed in two separate items on six-point Likert scales among other psychological properties (i.e., sleep quality, stress, situational characteristics that had been used in other research) as part of an experience sampling procedure.

<!-- voice: on device acoustic feature extraction -->
The last experience sampling questionnaire of each day included an additional instruction, where participants were asked to read out a series of predefined emotional sentences while making an audio recording of their voice. The sentences presented to the participants are based on a set of 54 validated German neutral and affective sentences [@defrenEmotionalSpeechPerception2018] and differ in their emotional content: positive (e.g., "My team won yesterday."), negative (e.g., "Nobody is interested in my life."), and neutral (e.g., "The plate is on the round table."). In each measurement instance, participants were instructed to read out three positive, neutral, and negative sentences. The order of the categories was randomized per experience sampling instance. For each emotional content category, three sentences were randomly drawn (with replacement) from the respective sets of sentences in the database. The experimental manipulation of these emotional semantic categories allowed us to control for the content spoken by our participants and at the same time enabled us to conduct a privacy-friendly study. The audio recording was started by the participants via a button on the screen. Participants could stop the recording manually after a minimum of four seconds. Alternatively, the recording was stopped automatically after twelve seconds. We chose these lower- and upper-time thresholds because this is the minimum and maximum time required to read out the three sentences per condition, determined by reading the sentences extremely fast and extremely slow and recording the times. Once the audio record had been completed, we used the widely adopted *OpenSMILE* open-source algorithm [@eybenOpensmileMunichVersatile2010] to automatically extract acoustic features directly on the participant’s device. Here, we used the extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPS) that comprises 88 acoustic features [@eybenGenevaMinimalisticAcoustic2016]. The eGeMAPS feature set has been used in a range of prior studies on affect recognition from speech [@vanrijnModellingIndividualCrosscultural2023; @weidmanNotHearingHappiness2020]. After feature extraction, the voice records were automatically deleted and only the extracted voice features were transmitted to our servers. As a result, there were three sets of voice features per experience sampling instance (one per sentiment condition).
```{r study 1 sample data, echo = FALSE, warning = FALSE, results = FALSE, message = FALSE}

library("dplyr")

## load data

# merged voice and affect data before quality checks
affect_voice_study1  <- readRDS("../data/study1/affect_voice_changed.rds")

# final merged data set of voice data with esm data after quality checks
affect_voice_study1_cleaned  <- readRDS("../data/study1/affect_voice_study1_cleaned.rds")

# count how many es instances with valence and arousal ratings are available per participant in raw data and final data

count_es_user_init <- affect_voice_study1 %>%
  dplyr::group_by(user_id) %>%
  dplyr::summarise(n = n(), .groups = 'drop')

es_init_sample <- count_es_user_init %>%
  summarise(mean_n = mean(n), sd_n = sd(n))

count_es_user_final <- affect_voice_study1_cleaned %>%
  dplyr::group_by(user_id) %>%
  dplyr::summarise(n = n(), .groups = 'drop')

es_final_sample <- count_es_user_final %>%
  summarise(mean_n = mean(n), sd_n = sd(n))

# find participants with less than 10 audio samples
length(which(count_es_user_init$n < 10))

```
<!-- describe the sample, raw sample and final sample  -->
We collected `r nrow(affect_voice_study1)` audio logs with respective affect self-reports from `r length(unique(affect_voice_study1$user_id))` participants. Participants made on average `r round(es_init_sample$mean_n, 2)` (*SD* = `r round(es_init_sample$sd_n, 2)`) voice records. We excluded data from `r length(which(count_es_user_init$n < 10))` participants with less than ten voice samples in total and the data from eight participants who had no variance in all their valence and arousal scores across all their experience samples. Finally, we excluded 214 voice logs from 5 participants because the respective voice indicators suggested that no human voice was recorded (mean voicing score < 0.5, voiced segments per second = 0, mean voiced segment length = 0).
This left us with a final data set of `r nrow(affect_voice_study1_cleaned)` voice samples with corresponding acoustic features from `r length(unique(affect_voice_study1_cleaned$e_s_questionnaire_id))` experience sampling instances for valence and arousal from `r length(unique(affect_voice_study1_cleaned$user_id))` participants (`r round((table(affect_voice_study1_cleaned$Demo_GE)[2] / sum(table(affect_voice_study1_cleaned$Demo_GE)))*100,1)`% female, Age~M~ = `r round(mean(affect_voice_study1_cleaned$Demo_A1, na.rm = T),2)` years). In the final sample, there were on average 24.1 (*SD* = 10.6) voice logs with 8.22 (*SD* = 3.53) corresponding affect self-reports per participant. Overall self-reported valence was positive (*M* = `r round(mean(affect_voice_study1_cleaned$valence),2)`, *SD* = `r round(sd(affect_voice_study1_cleaned$valence),2)`) and overall arousal was slightly geared towards activation (*M* = `r round(mean(affect_voice_study1_cleaned$arousal),2)`, *SD* = `r round(sd(affect_voice_study1_cleaned$arousal),2)`). The distribution of valence and arousal ratings is provided in the online supplements. 

### Study 2

<!-- US Study data collection description + ESM -->
Data collection for Study 2 was part of the UT1000 Project at the University of Texas at Austin in the United States in fall 2018 [@wuMultimodalDataCollection2021]. During a three-week self-tracking assignment using their smartphones, students from a Synchronous Massive Online Course (SMOC) in introductory psychology received four short experience sampling questionnaires per day where they could also make records of their speech at the end. Here, self-reported arousal (assessed on a five-point Likert scale), contentedness, and sadness were assessed in separate items on four-point Likert scales among other psychological properties as part of an experience sampling procedure [@wuMultimodalDataCollection2021]. Thereby, in Study 2, we captured emotional valence on two items (contentedness and sadness) instead of one as done in Study 1. According to the affect grid, contentedness and sadness have a comparable low level of arousal and an opposing emotional valence [@russellCircumplexModelAffect1980a; @posnerCircumplexModelAffect2005].  
For the audio records, participants received the following instruction: "Please record a short audio clip in which you describe the situation you are in, and your current thoughts and feelings. Collect about 10 seconds of environmental sound after the description.” The responses to this prompt are analyzed in the present study. Any parts of the record that did not contain speech were cut out before further analysis since the focus of this work is affect in human speech. The collected speech samples had also been used in another research project that describes the data collection procedure in more detail [@marreroEvaluatingVoiceSamples2022].

```{r study2 sample data, include = FALSE}

library("dplyr")

## load data

# ema and voice data before data cleaning 
affect_voice_study2  <- readRDS("../data/study2/affect_voice_study2.rds")

# count how many es instances with valence and arousal ratings are available per participant in raw data

count_es_user <- affect_voice_study2 %>%
  dplyr::group_by(user_id) %>%
  dplyr::summarise(n = n(), .groups = 'drop')

es_init_sample <- count_es_user %>%
  summarise(mean_n = mean(n), sd_n = sd(n))


# final merged data set of voice data with esm data after cleaning 
affect_egemaps_wordembeddings_study2  <- readRDS("../data/study2/affect_voice_wordembeddings.rds")

```

<!--  initial sample, preprocessing -->
With this procedure, we collected `r nrow(affect_voice_study2)` audio logs with corresponding affect self-reports from `r length(unique(affect_voice_study2$user_id))` participants. Participants made on average `r round(es_init_sample$mean_n,2)` (*SD* = `r round(es_init_sample$sd_n,2)`) speech records with corresponding affect self-reports. We followed the same procedure to filter the data as in Study 1: First, we excluded 281 participants with less than ten audio records in total and another participant who had no variance in all their self-reports across all their experience samples. Next, we removed 337 voice records from 11 participants where the respective acoustic features indicated that no human voice was recorded and, to ensure comparability of the two studies with regard to the length of speech samples, we retained all speech transcripts that contained at least 15 words and were more than four seconds long which is equivalent to the length of the sentences that had been read out in Study 1. 

<!-- final sample description - study 2 -->
This procedure left us with a final data set of `r nrow(affect_egemaps_wordembeddings_study2)` speech samples with corresponding experience-sampled self-reports on momentary affect experience from `r length(unique(affect_egemaps_wordembeddings_study2$user_id))` participants (`r round((table(affect_egemaps_wordembeddings_study2$Gender)[1] / sum(table(affect_egemaps_wordembeddings_study2$Gender)))*100,1)`% female, Age~M~ = `r round(mean(affect_egemaps_wordembeddings_study2$Age, na.rm = T),2)` years). Overall participants reported balanced experienced contentedness (*M* = `r round(mean(affect_egemaps_wordembeddings_study2$content),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings_study2$content),2)`) and low sadness (*M* = `r round(mean(affect_egemaps_wordembeddings_study2$sad),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings_study2$sad),2)`). Overall arousal was balanced out (*M* = `r round(mean(affect_egemaps_wordembeddings_study2$arousal),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings_study2$arousal),2)`). The distribution of self-reported arousal, contentedness, and sadness scores is provided in the online supplements. 

<!-- acoustic feature extraction -->
Replicating the approach from Study 1 we extracted the extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPS) from the collected audio files using the OpenSMILE algorithm [@eybenOpensmileMunichVersatile2010; @eybenGenevaMinimalisticAcoustic2016]. In contrast to the on-device feature-extraction approach in Study 1, in Study 2 those features were extracted from the raw recorded audio files after data collection.

<!-- word embeddings -->
We transcribed all raw audio records using the Google Speech-to-text API (version 1). Automatic speech-to-text technology has been shown to be well-suited for transcription tasks in psychological research [@pfeiferHowReadySpeechtotext2024]. The Google API also assigns a sentiment score (*M* = `r round(mean(affect_egemaps_wordembeddings_study2$Sentiment.score, na.rm =T),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings_study2$Sentiment.score, na.rm =T),2)`) within the interval of [-1; 1] to each speech transcript. Then, we extracted state-of-the-art word embeddings from speech transcripts using the *text* R package [@kjellTextRpackageAnalyzing2021]. Word embeddings are vector representations of words in a high-dimensional space, which capture their contextualized meaning and relationships with other words. Specifically, we used the 1024 dimensions from the second to last layer (layer 23) from the language model "RoBERTa large" as features as recommended in prior work [@liuRoBERTaRobustlyOptimized2019; @materoEvaluatingContextualEmbeddings2022].

## Predictive modeling

<!-- general ml -->
In both studies, we trained linear LASSO (Least Absolute Shrinkage and Selection Operator) regularized regression models [@tibshiraniRegressionShrinkageSelection1996] and non-linear tree-based Random Forest models [@breimanRandomForests2001; @wrightRangerFastImplementation2017], and a baseline model on the extracted features for the prediction of self-reported affect experience. The baseline model predicted the respective mean values for valence and arousal of the respective training set for all cases in a test set. To conduct a preliminary evaluation of the informativeness of our data, we trained additional prediction models with gender as the dependent variable to evaluate whether the collected data at all contained information about individual differences since past work indicates that speaker gender can be reliably predicted from voice cues [@kwasnyGenderAgeEstimation2021]. We evaluated model performance using a ten-fold five times repeated cross-validation scheme [@bischlResamplingMethodsMetaModel2012] and used blocking of participants in the resampling procedure to ensure that for each train/test set pair the given participant is either in the training set or in the test set. In the results section, we report on the average model performance across those 50 models. Before predictive modeling, we replaced extreme outliers (mean +/- 4 times SD) with missing values that were imputed in the resampling procedure.

<!-- Model evaluation -->
Our prediction models were evaluated based on how accurate new (unseen) samples can be predicted. Throughout this manuscript, we report on the average Spearman correlation ($\rho$) of true and predicted scores and the coefficient of determination (*R*^2^) as measures of model fit. In the online supplements, we provide the mean absolute error (MAE) and the root mean squared error (RMSE) as additional performance measures for prediction models. To determine whether prosodic features predicted affect experience beyond chance (*alpha* = 0.05), we carried out variance-corrected (one-sided) t-tests comparing the *R*^2^ measures of prediction models with those of the baseline models [@nadeauInferenceGeneralizationError2003]. We adjusted for multiple comparisons  (*n* = 16) via Holm correction. 

<!-- Model interpretation ?? -->
<!-- Software -->
All data processing and statistical analyses in this work were performed with the statistical software R version 4.1.1 [@rcoreteamLanguageEnvironmentStatistical2021]. For machine learning, we used the *mlr3* framework [@langMlr3ModernObjectoriented2019]. Specifically, we used the *glmnet* [@friedmanRegularizationPathsGeneralized2010] and *ranger* [@wrightRangerFastImplementation2017] packages to fit prediction models. We preregistered Study 1 as a transparent account of our work [@kochPredictingAffectiveStates2021] and extended the analytical approach to the data from Study 2. All deviations from the preregistration are documented in the online supplements. 

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
