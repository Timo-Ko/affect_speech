---
title             : "Semantic content outperforms speech prosody in predicting affective experience in naturalistic settings"
shorttitle        : "Affective Experience in Speech"

author: 
  - name          : "Timo K. Koch"
    affiliation   : "1,2,3"
    corresponding : yes    # Define only one corresponding author
    address       : "Torstrasse 25, 9000 St. Gallen, Switzerland"
    email         : "timo.koch@unisg.ch"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Methodology"
      - "Formal Analysis"
      - "Visualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Gabriella M. Harari"
    affiliation   : "3"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
  - name          : "Ramona Schoedel"
    affiliation   : "2,4"
    role:
      - "Conceptualization"
      - "Writing - Review & Editing"
  - name          : "Samuel D. Gosling"
    affiliation   : "5"
    role:
      - "Resources"
      - "Writing - Review & Editing"
  - name          : "Zachariah Marrero"
    affiliation   : "5"
    role:
      - "Data curation"
  - name          : "Florian Bemmann"
    affiliation   : "6"
    role:
      - "Software"
  - name          : "Markus Bühner"
    affiliation   : "2"
    role:
      - "Resources"
  - name          : "Clemens Stachl"
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Writing - Review & Editing"
      - "Supervision"  

affiliation:
  - id            : "1"
    institution   : "Institute of Behavioral Science and Technology, University of St. Gallen"
  - id            : "2"
    institution   : "Department of Psychology, Ludwig-Maximilians-Universität München" 
  - id            : "3"
    institution   : "Department of Communication, Stanford University"
  - id            : "4"
    institution   : "Charlotte Fresenius Hochschule, University of Psychology"
  - id            : "5"
    institution   : "Department of Psychology, The University of Texas at Austin"
  - id            : "6"
    institution   : "Media Informatics Group, Ludwig-Maximilians-Universität München"
note: "\\clearpage" #added this to move entire author note to second page
authornote: |
  Conflict of interest: none
  
  Acknowledgements: We thank audEERING GmbH, Peter Ehrich, and Dominik Heinrich for their support with the technical implementation of the on-device voice feature extraction for Study 1, ZPID for the support with data collection for Study 1, and Sumer Vaid for the technical support with the setup of the computational environment for the analyses of Study 2. This project was supported by the Swiss National Science Foundation (SNSF) under project number 215303, the National Science Foundation (NSF) under project number 1758835, a Stanford HAI Google Cloud Credit Award, and a scholarship of the German Academic Scholarship Foundation awarded to the first author. All code and data are available in the project’s repository on the Open Science Framework (OSF): https://osf.io/a5db8/

abstract: |
  Many commercial products use algorithms to recognize affect based on speech prosody (i.e., voice acoustics). These algorithms are typically trained on enacted or labeled speech samples collected in lab settings. However, they are used to infer affective experiences occurring in everyday life. Here, we investigate whether the experience of affective states can be predicted from speech samples collected using smartphones in naturalistic settings. In two field studies (experimental Study 1: *N* = 409; observational Study 2: *N* = 687), we collected 25,403 speech samples from participants along with their self-reported affective experiences. Machine learning analyses show that prosody reveals limited affective information (*r*~md~ = .17) and is outperformed by semantic content (*r*~md~ = .33). Our findings demonstrate the importance of semantic content and challenge whether previously reported prediction performances for affective expression from prosody in controlled settings generalize to the recognition of subjective affective experience in naturalistic settings.
  
keywords          : "Affect, Voice, Speech, Prosody, Machine Learning"
wordcount         : "5144"

bibliography      : ["../speech_paper_library.bib"]

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

documentclass     : "apa6"
classoption       : "man"
output: 
  papaja::apa6_pdf:
    figsintext: yes

---

```{r setup, include = FALSE}
library("papaja")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

The prosody of a voice, such as acoustic tone, pitch, and rhythm, serves as a primary means for conveying emotional information [@schererVocalCommunicationEmotion2003; @krausVoiceonlyCommunicationEnhances2017]. Decades of research investigated how speech prosody varies with affect [@larrouy-maestriSoundEmotionalProsody2024], finding, for example, that higher pitch and faster speech rates are associated with excitement or stress [@banseAcousticProfilesVocal1996]. More recently, algorithms trained to automatically recognize affect and affective disorders from speech prosody offer promising potential applications in the domains of health care, human-machine interaction, education, and business [@millingSpeechNewBlood2022; @hildebrandVoiceAnalyticsBusiness2020; @vlahosTalkMeHow2019]. The widespread adoption of voice assistants like Amazon’s Alexa and Apple’s Siri has also fueled commercial interest in developing algorithms that can detect affect in everyday life. These algorithms aim to quantify at scale how patients, customers, and employees feel in a given moment, often to provide personalized recommendations, feedback, and services [e.g., @matzUsingBigData2017; @seiferthHowEmentalHealth2023]. 

## Algorithmic recognition of affective expression in the lab versus experience in the wild

Existing affect recognition algorithms are predominantly trained on enacted emotional speech [from professional actors in lab settings, @banzigerIntroducingGenevaMultimodal2012; @schullerSpeechEmotionRecognition2018; @vogtAutomaticRecognitionEmotions2008] or on labeled speech samples [such as TV clips, @grimmVeraAmMittag2008]. Using such training data generally results in high performance for the algorithmic recognition of, for example, affective arousal (*r*~max~ = .81) and valence (*r*~max~ = .68) in artificial settings [@weningerAcousticsEmotionAudio2013]. However, these data, which are used to train the algorithms, differ in two important respects from the data that are actually processed by the algorithms when they are applied in the real world. 

First, the data used to train such algorithms consists of actors' portrayals of affective states or raters' labels of existing voice samples that both rely on folk theories of how affect is expressed; for instance, how does a sad person sound when they talk? [@batlinerAutomaticRecognitionEmotions2011; @schullerSpeechEmotionRecognition2018; @wiltingRealVsActed2006]. However, those prototypical affective expressions are not necessarily reflective of people’s subjective affective *experience*; that is, their genuine subjective affective state. For example, someone might feel nervous about giving a presentation, but choose to express themselves by talking calmly and confidently. Investigating affective expression is invaluable for understanding emotional *communication* [@ekmanRepertoireNonverbalBehavior1969], but the main purpose of such algorithms is to detect people’s subjective affective *experience*. This is important for both researchers and commercial entities who want to gain a deeper understanding of how individuals, such as patients or customers, genuinely feel, as opposed to merely observing how they outwardly express themselves.

The second way the data used to train current algorithms differ from instances in which they will be used is whether speech data has been produced and collected in laboratory or real-world settings. In controlled lab settings, actors enact predefined affective states or desired emotions are elicited in participants and their speech is recorded, allowing for the investigation of affective speech in standardized conditions using controlled stimuli [e.g., @cowenPrimacyCategoriesRecognition2019]. However, speech data collected in lab settings limit ecological validity. Algorithms trained on such data may perform poorly in real-world situations, where conditions differ significantly from controlled environments. For example, lab recordings are made in isolated rooms without background noise, unlike the complex auditory environments of everyday life (e.g., noisy cafés or restaurants). As a result, these algorithms might fail to accurately recognize affect in naturalistic settings, leading to unreliable results and misinterpretations in practical applications.

The focus on affective expression (versus affective experience) and on lab-based (versus real world) speech samples that characterized past research raises concerns about the generalizability of the performance of existing algorithms. Specifically, the speech data on which these algorithms are being trained do not match the speech data encountered in the settings in which they are deployed. This raises the question: Are algorithms capable of accurately detecting subjective affective experience in everyday life, and with comparable degrees of accuracy as they have shown for affective expression in the lab?

## Prosody versus semantics for affect recognition from speech

In addition to prosody, the semantic content (i.e., meaning of the words) of speech plays a central role in conveying affective information. Prosody and semantics interact dynamically, working together to transmit affective cues through speech [@ben-davidProsodySemanticsAre2016]. Prior research suggests that prosody often takes precedence over semantics in how humans perceive affective states [@ben-davidProsodySemanticsAre2016; @linyiProsodyDominatesSemantics2020]. In the same fashion, prior work indicates that algorithms might also prioritize prosodic cues over semantics for the recognition of affect from speech [@elayadiSurveySpeechEmotion2011; @polzehlAngerRecognitionSpeech2011; @schullerSpeechEmotionRecognition2004]. However, as noted above, these findings are based on algorithms trained to recognize affect expression (not experience) in controlled lab settings. Consequently, what remains unknown is whether algorithms rely more on prosodic features or semantic content when recognizing affective experience from spontaneous speech in real-world settings.

In the present work, we address the existing gaps in algorithmic affect recognition from speech, namely (1) the generalization of algorithms’ prediction performance from affective expression in the lab to affective experience in the real world, and (2) the predictive power of prosody versus semantics for affect inferences from speech. To investigate the algorithmic recognition of affective experience in everyday life, we conducted two large-scale field studies in two different countries: a field experiment with German-speaking participants in Germany (Study 1) and an observational field study with English-speaking participants in the United States (Study 2). In both studies, we used smartphone applications to collect experience sampling reports and speech samples via the built-in microphone as people went about their daily lives. Using this approach, we collected a total of 25,403 speech samples, paired with self-reported affective experience from 1,096 participants.

The first study was a field experiment. We manipulated what people said in speech samples collected in naturalistic settings to conservatively test the predictive power of voice prosody for algorithmic recognition of affective experience, while controlling for semantic content. Participants were instructed to read out pre-tested scripted sentences with varying emotional sentiment (i.e., positive, negative, and neutral) while recording their voice. From the extracted prosodic features we then predicted self-reported affective states on the dimensions of arousal and valence using machine learning. In the second study, we collected spontaneous speech samples in naturalistic settings to investigate the predictive power of voice prosody and semantic content for algorithmic recognition of affective experience. Participants were prompted to talk spontaneously about their current situation, thoughts, and feelings while recording their voice. Just as in Study 1, we subsequently predicted self-reported affective states on the dimensions of arousal as well as contentment and sadness (for emotional valence) from voice prosody to investigate the predictive power of prosody in spontaneous speech. Additionally, we used the semantic content captured by word embeddings from those speech samples to predict momentary affective states and compare the models' performance to those trained solely on prosody.

\newpage
# Results

In this section, we report on the performance of machine learning models predicting self-reported affective states from prosodic and semantic features derived from scripted and spontaneous speech samples collected via smartphones. While Figure \@ref(fig:predictionoverview) provides an overview of models' performance across all predictions and algorithms, we report on the average performance of the best performing algorithm in the text.

## Prosody reveals limited affective information in scripted and spontaneous speech

```{r prediction results, echo = FALSE, warning = FALSE}

# load prediction tables
pred_table_study1 <- read.csv2("../results/pred_table_study1.csv")
pred_table_study2 <- read.csv2("../results/pred_table_study2.csv")

```

```{r sentenceanova, echo = FALSE, warning = FALSE, results = FALSE, message = FALSE}

library(car)

predictions_condition <- readRDS(file="../results/study1/predictions_condition.rds")

# run anova comparing means for valence 
aov_valence <- car::Anova(aov(error_valence ~ condition, data = predictions_condition))

# run anova comparing means for arousal
aov_arousal <- car::Anova(aov(error_arousal ~ condition, data = predictions_condition))

```

When participants read out scripted content (Study 1), the prediction of momentary affective arousal from voice prosody was low on average (*r*~md~ = `r round(pred_table_study1[pred_table_study1$learner_id == "imputehist.regr.cv_glmnet" & pred_table_study1$task_id == "egemaps_arousal", ]$Md_srho, 2)`, *r*~sd~ = `r round(pred_table_study1[pred_table_study1$learner_id == "imputehist.regr.cv_glmnet" & pred_table_study1$task_id == "egemaps_arousal", ]$SD_srho, 2)`), yet significantly better than chance levels. The prediction of momentary affective valence from voice prosody was on average even lower than for arousal and was not significantly better than chance (*r*~md~ = `r round(pred_table_study1[pred_table_study1$learner_id == "imputehist.regr.cv_glmnet" & pred_table_study1$task_id == "egemaps_valence", ]$Md_srho, 2)`, *r*~sd~ = `r round(pred_table_study1[pred_table_study1$learner_id == "imputehist.regr.cv_glmnet" & pred_table_study1$task_id == "egemaps_valence", ]$SD_srho, 2)`). We analyzed whether the experimentally altered sentiment (positive/ neutral/ negative) of the scripted sentences had an effect on affect predictions from prosody. We found no significant differences in prediction errors across the three sentence conditions for arousal (*F*(2,49702) = `r round(aov_valence[3][[1]][1],2)`, *p* = .500) and valence (F(2,49702) = `r round(aov_arousal[3][[1]][1],2)`, *p* = .993) predictions suggesting that sentences’ sentiment did not influence affect predictions from prosodic features.

Models that had been trained on prosodic features from participants talking spontaneously about their current situation, thoughts, and feelings (Study 2) also yielded a low average prediction performance overall. Still, the prediction of momentary arousal (*r*~md~ = `r round(pred_table_study2[pred_table_study2$learner_id == "imputehist.regr.cv_glmnet" &  pred_table_study2$task_id == "egemaps_arousal", ]$Md_srho, 2)`, *r*~sd~ = `r round(pred_table_study2[pred_table_study2$learner_id == "imputehist.regr.cv_glmnet" &  pred_table_study2$task_id == "egemaps_arousal", ]$SD_srho, 2)`) and contentment (*r*~md~ = `r round(pred_table_study2[pred_table_study2$learner_id == "imputeoor.regr.ranger" &  pred_table_study2$task_id == "egemaps_content", ]$Md_srho, 2)`, *r*~sd~ = `r round(pred_table_study2[pred_table_study2$learner_id == "imputeoor.regr.ranger" &  pred_table_study2$task_id == "egemaps_content", ]$SD_srho, 2)`) were significantly better than chance. Predictions of sadness were not better than the baseline (*r*~md~ = `r round(pred_table_study2[pred_table_study2$learner_id == "imputeoor.regr.ranger" &  pred_table_study2$task_id == "egemaps_sad", ]$Md_srho, 2)`, *r*~sd~ = `r round(pred_table_study2[pred_table_study2$learner_id == "imputeoor.regr.ranger" &  pred_table_study2$task_id == "egemaps_sad", ]$SD_srho, 2)`). To provide insights into which prosodic features could be associated with affective states, we included an overview of the importance of prosodic features in an Elastic Net model and report on the correlations of voice features with momentary affective experience in the repository. 

## Semantic content outperforms prosody in predicting subjective affective experience

Machine learning models trained on semantic content in the form of word embeddings extracted from participants’ spontaneous speech (Study 2) yielded good performance results overall that were significantly better than chance, outperforming models trained on voice prosody. Specifically, we found semantic content to be, on average, more predictive of the momentary experience of arousal (*r*~md~ = `r round(pred_table_study2[pred_table_study2$learner_id == "imputehist.regr.cv_glmnet" &  pred_table_study2$task_id == "wordembeddings_arousal", ]$Md_srho, 2)`, *r*~sd~ = `r round(pred_table_study2[pred_table_study2$learner_id == "imputehist.regr.cv_glmnet" &  pred_table_study2$task_id == "wordembeddings_arousal", ]$SD_srho, 2)`), contentment (*r*~md~ = `r round(pred_table_study2[pred_table_study2$learner_id == "imputehist.regr.cv_glmnet" &  pred_table_study2$task_id == "wordembeddings_content", ]$Md_srho, 2)`, *r*~sd~ = `r round(pred_table_study2[pred_table_study2$learner_id == "imputehist.regr.cv_glmnet" &  pred_table_study2$task_id == "wordembeddings_content", ]$SD_srho, 2)`), and sadness (*r*~md~ = `r round(pred_table_study2[pred_table_study2$learner_id == "imputehist.regr.cv_glmnet" &  pred_table_study2$task_id == "wordembeddings_sad", ]$Md_srho, 2)`, *r*~sd~ = `r round(pred_table_study2[pred_table_study2$learner_id == "imputehist.regr.cv_glmnet" &  pred_table_study2$task_id == "wordembeddings_sad", ]$SD_srho, 2)`) than voice prosody. Models trained on both prosodic features and word embeddings showed a similar prediction performance to those trained on word embeddings alone, suggesting that the predictions were primarily driven by semantic information.

```{r predictionoverview, echo = FALSE, warning=FALSE, fig.cap="Box and whisker plot of out-of-sample prediction performance measures from five times repeated ten-fold cross-validation for each affect measure. Feature sets (either prosodic or semantic features) for model training are shown in parentheses. Names of models that are significantly better than chance are displayed in boldface. Pearson correlation is not available for baseline models because it predicts a constant value, for which correlation measures are not defined.", fig.scap= "Prediction performance", out.width="100%", fig.align="center",  fig.pos. = "H", results="asis"}

knitr::include_graphics(path = "../figures/bmr_plot_annotated.png")
```

\newpage
# Discussion

Affect-recognition algorithms developed in research and commercial tools are mainly trained on prosody extracted from enacted or labeled speech samples in artificial lab settings. Yet, these algorithms are deployed to detect people's subjective affective experience in naturalistic settings. In the present work, we investigated whether subjective momentary affective states are predictable from prosodic and semantic features in real-world speech samples collected with smartphones. In contrast to prior work, which was based on affective expressions from enacted speech data from the lab, our findings suggest that prosody reveals only limited information about affective experience and is outperformed by semantic content in the real world.

The first major finding to emerge from this work is that prosody provided limited information about affective experience in naturalistic settings. Machine learning models trained on prosodic features achieved a lower prediction performance for affective experience (up to *r*~md~ = .17) compared to prior work on the algorithmic prediction of affective expression in controlled lab settings [up to *r*~max~ = .81, e.g., @weningerAcousticsEmotionAudio2013], which might have also been susceptible to overfitting to the training data [@aishwaryaComputationallyEfficientSpeech2024]. This result supports the notion that recognizing real-life affective experience from voice cues algorithmically is inherently more challenging than detecting affective expression in the lab [@vogtAutomaticRecognitionEmotions2008]. Nevertheless, past research predicting affective experience from speech in the wild reported similar prediction performance [@weidmanNotHearingHappiness2020; @carlierSearchStateTrait2022; @sunLanguageWellbeingTracking2020]. Moreover, the prediction of arousal from prosody was successful across prediction models whereas the prediction of valence yielded significant predictions only for contentment, but not for overall valence and sadness. This finding is in line with prior work showing that valence is more challenging to infer algorithmically than is arousal due to its subjective nature [@sridharUnsupervisedPersonalizationEmotion2022]. Thus, our findings raise questions about the generalizability of the high prediction performance from past research [e.g., @weningerAcousticsEmotionAudio2013], which focus on recognizing affective expression from speech data collected in lab settings. Specifically, it seems that the performance obtained from lab-based studies may not extend to the recognition of affective experience from speech prosody in everyday life.

Another reason why prediction performance is lower in real-world data sets could be that they contain fewer instances of extreme affective experience than those used in lab studies focusing on enacted or labeled speech. Consequently, our predictions targeted “normal” everyday affective states with only a few cases of extreme affect experience, which are inherently less intense than the short-lived emotions that have been investigated in prior work [@schererVocalCommunicationEmotion2003], and thus present a greater challenge for algorithmic recognition [@vogtAutomaticRecognitionEmotions2008].

To test whether a broader set of prosodic features would improve prediction performance, we reran predictions using the 2016 Interspeech Computational Paralinguistic Challenge set of 6,737 prosodic features (versus 88 features in the eGeMAPS set used in the main analyses) [@eybenGenevaMinimalisticAcoustic2016; @schullerINTERSPEECH2016Computational2016]. In line with prior research, the larger voice feature set did not yield better predictions of momentary affective experience [@weidmanNotHearingHappiness2020], suggesting that the limited predictive power of prosody was not due to the parsimonious size of the prosodic feature set (see repository for details). 

The second major finding to emerge from this work is that semantic content captured by word embeddings outperformed prosody in predicting affective experience in spontaneous speech. This insight suggests that speech content may be more informative than prosodic cues for algorithms recognizing experienced affect in everyday life. It contrasts prior research suggesting that speech prosody is more relevant than semantics for affect inferences by humans [@ben-davidProsodySemanticsAre2016; @linyiProsodyDominatesSemantics2020] and algorithms trained on lab data [@elayadiSurveySpeechEmotion2011; @polzehlAngerRecognitionSpeech2011; @schullerSpeechEmotionRecognition2004]. However, our results align with primary research that found semantic content to be more predictive than prosodic cues in algorithmically predicting the momentary subjective affective experience [@weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020; @carlierSearchStateTrait2022]. As an explanation, we argue that algorithms are potentially better at detecting signals from the structured nature of text, in contrast to the complexity of prosody, such as subtle nuances of intonation (e.g., by stressing a single word). Nonetheless, participants in our study were prompted to speak spontaneously about their current situations, thoughts, and feelings, which can be expected to reveal some affective information. Further research is needed to examine how the themes people talk about (e.g., explicitly describing how one feels, which is likely to carry a rich affective signal, versus describing the physical environment, which may contain minimal affective content) impact prediction performance.

Finally, we replicated analyses from past work that investigated predictions of fluctuations in affective states from speech prosody and semantic content. The results were consistent with prior work, indicating that the prediction performance for within-person fluctuations in affective states is lower than the performance of models predicting between-person differences [@weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020]. 

The findings of this work are limited in three ways. First, there are some differences between the two studies that might affect the comparability of results: In particular, the studies differed in terms of the scales used to assess affective experience (see methods section for details), the sampling strategy (representative quota sample vs. undergraduate cohort), language and culture (Germany vs. USA), and smartphone platforms (Android only vs. Android and iOS). We sought to minimize the effects of these differences by, where possible, using consistent methodologies and statistical approaches across studies. Nonetheless, the findings might be considered transferable but not directly comparable across studies.

Second, both data sets were collected in WEIRD (Western, Educated, Industrialized, Rich, and Democratic) countries [@henrichWeirdestPeopleWorld2010]. Plenty of prior research has pointed to cultural differences in the experience and expression of affect [@limCulturalDifferencesEmotion2016], including cultural variations in prosodic affect markers in the voice [@vanrijnModellingIndividualCrosscultural2023; @laukkaCrossCulturalEmotionRecognition2021; @brooksDeepLearningReveals2023]. Therefore, the generalizability of the current work should be considered as limited to this cultural area. Future studies should seek to broaden participant inclusion to diverse cultural contexts, especially non-Western countries [@phanMobileSensingGlobe2023].

Third, in contrast to prior work using passive speech sensing, for example via the Electronically Activated Recorder (EAR) [@mehlElectronicallyActivatedRecorder2017; @weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020], participants had to actively log their speech via their smartphone in the present work. As a consequence, participants might not have spoken as naturally as they would have in a naturalistic conversation. Further, they might have made the audio records only in selected situations that were suitable for making an audio record, for example when they were alone in a quiet place.

\newpage
# Conclusion

In this work, we investigated whether machine learning algorithms can recognize subjective affective experience from naturalistic speech samples collected in everyday life via smartphones. Extracted prosodic voice parameters provided only limited affective information and were outperformed by semantic content as captured in word embeddings. Our findings challenge whether the optimistic prediction performance results from prior research on the recognition of affective expression (e.g., enacted and labeled speech) from lab settings generalize to the recognition of subjective affective experience in everyday speech. The ability to detect how people genuinely feel, as opposed to merely observing what they outwardly express, is crucial to gaining a deeper understanding of human emotions and guiding the development of technologies that are better placed to serve and respond to our emotional needs.

\newpage
# Methods

## Data collection

The present work consists of two studies using machine learning that are based on separate datasets. The workflow of data collection, processing, and predictive modeling is described in detail in the following section and illustrated in Figure \@ref(fig:methodsoverview).

```{r methodsoverview, echo = FALSE, warning=FALSE, fig.cap="Flow diagram illustrating data collection, processing, and predictive modeling in Study 1 and Study 2.", fig.scap= "Method diagram", out.width="100%", fig.align="center", fig.pos. = "H", results="asis"}

knitr::include_graphics(path = "../figures/methods_figure.png")
```

### Study 1

Data collection for Study 1 was part of a large panel study (from May 15 until November 14, 2020) using the *PhoneStudy* research app [@schoedelBasicProtocolSmartphone2020]. Data collection was approved by the ethics committee of the psychology department at LMU Munich and all procedures adhered to the General Data Protection Regulation (GDPR). We recruited a quota sample of *N* = 850 participants representative of the German population regarding age, gender, education, income, confession, and relationship status. Participants had to be between 18 and 65 years old, fluent in German, and in the possession of an Android smartphone (Android 5 or higher) for private usage as a sole user. The study also comprised two two-week experience sampling phases (July 27, 2020, to August 9, 2020; September 21, 2020, to October 4, 2020) during which participants received two to four short questionnaires per day on their personal smartphone as part of an experience sampling procedure. Here, self-reported momentary affective valence and arousal were assessed in two separate items on six-point Likert scales: Valence ranging from “very unpleasant” (1) to “very pleasant” (6) and arousal ranging from “very inactive” (1) to “very activated” (6). The experience sampling procedure also covered other psychological properties (i.e., sleep quality, stress, situational characteristics) that had been used in other research [@schoedelSnapshotsDailyLife2023; @grossedetersKeepScrollingUsing2024].

The last experience sampling questionnaire of each day included an additional instruction, where participants were asked to read out a series of predefined emotional sentences while making an audio recording of their voice. The sentences presented to the participants are based on a set of 54 validated German neutral and affective sentences [@defrenEmotionalSpeechPerception2018] and differ in their emotional content: positive (e.g., “My team won yesterday.”), negative (e.g., “Nobody is interested in my life.”), and neutral (e.g., “The plate is on the round table.”). In each measurement instance, participants were instructed to read out three positive, neutral, and negative sentences. The order of the categories was randomized per experience sampling instance. For each emotional content category, three sentences were randomly drawn (with replacement) from the respective sets of sentences in the database. The experimental manipulation of these emotional semantic categories allowed us to control for the content spoken by our participants and at the same time enabled us to conduct a privacy-friendly study. The audio recording was started by the participants via a button on the screen. Participants could stop the recording manually after a minimum of four seconds. Alternatively, the recording was stopped automatically after twelve seconds. We chose these lower- and upper-time thresholds because this is the minimum and maximum time required to read out the three sentences per condition, determined by reading the sentences extremely fast and extremely slow and recording the times. Once the audio record had been completed, we used the widely adopted OpenSMILE open-source algorithm [@eybenOpensmileMunichVersatile2010] to automatically extract acoustic features directly on the participant’s device. Here, we used the extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPS) that comprises 88 acoustic features [@eybenOpensmileMunichVersatile2010] and the 2016 Interspeech Computational Paralinguistic Challenge set of 6,737 acoustic features [@schullerINTERSPEECH2016Computational2016]. Those feature sets have been used in a range of prior studies on affect recognition from speech [@vanrijnModellingIndividualCrosscultural2023; @weidmanNotHearingHappiness2020]. We used the parsimonious eGeMAPS feature set for our main analyses. After feature extraction, the voice records were automatically deleted and only the extracted voice features were transmitted to our servers. As a result, there were three sets of voice features per experience sampling instance (one per sentiment condition).

```{r study 1 sample data, echo = FALSE, warning = FALSE, results = FALSE, message = FALSE}

library("dplyr")

## load data

# merged voice and affect data before quality checks
affect_voice_study1  <- readRDS("../data/study1/affect_voice_changed.rds")

# final merged data set of voice data with esm data after quality checks
affect_voice_study1_cleaned  <- readRDS("../data/study1/affect_voice_study1_cleaned.rds")

# count how many es instances with valence and arousal ratings are available per participant in raw data and final data

count_es_user_init <- affect_voice_study1 %>%
  dplyr::group_by(user_id) %>%
  dplyr::summarise(n = n(), .groups = 'drop')

es_init_sample <- count_es_user_init %>%
  summarise(mean_n = mean(n), sd_n = sd(n))

count_es_user_final <- affect_voice_study1_cleaned %>%
  dplyr::group_by(user_id) %>%
  dplyr::summarise(n = n(), .groups = 'drop')

es_final_sample <- count_es_user_final %>%
  summarise(mean_n = mean(n), sd_n = sd(n))

# find participants with less than 10 audio samples
length(which(count_es_user_init$n < 10))

```

We collected `r nrow(affect_voice_study1)` audio logs with respective affect self-reports from `r length(unique(affect_voice_study1$user_id))` participants. Participants made on average `r round(es_init_sample$mean_n, 2)` (*SD* = `r round(es_init_sample$sd_n, 2)`) voice records. We excluded data from `r length(which(count_es_user_init$n < 10))` participants with less than ten voice samples in total and the data from eight participants who had no variance in all their valence and arousal scores across all their observations. Finally, we excluded 163 voice logs from 22 participants because the respective voice indicators suggested that no human voice was recorded (mean voicing score < 0.5, voiced segments per second = 0, mean voiced segment length = 0). This left us with a final data set of `r nrow(affect_voice_study1_cleaned)` voice samples with corresponding acoustic features from `r length(unique(affect_voice_study1_cleaned$e_s_questionnaire_id))` experience sampling instances for valence and arousal from `r length(unique(affect_voice_study1_cleaned$user_id))` participants (`r round((table(affect_voice_study1_cleaned$Demo_GE)[2] / sum(table(affect_voice_study1_cleaned$Demo_GE)))*100,2)`% female, Age~M~ = `r round(mean(affect_voice_study1_cleaned$Demo_A1, na.rm = T),2)` years). In the final sample, there were on average 24.31 (*SD* = 10.54) voice logs with 8.26 (*SD* = 3.53) corresponding affect self-reports per participant. Overall self-reported valence was positive (*M* = `r round(mean(affect_voice_study1_cleaned$valence),2)`, *SD* = `r round(sd(affect_voice_study1_cleaned$valence),2)`) and overall arousal was slightly geared towards activation (*M* = `r round(mean(affect_voice_study1_cleaned$arousal),2)`, *SD* = `r round(sd(affect_voice_study1_cleaned$arousal),2)`). The distribution of valence and arousal ratings is provided in the online supplements. 

### Study 2

Data collection for Study 2 was part of the UT1000 Project at the University of Texas at Austin in the United States in fall 2018 [@wuMultimodalDataCollection2021]. During a three-week self-tracking assignment using their smartphones, students from a Synchronous Massive Online Course (SMOC) in introductory psychology received four short experience sampling questionnaires per day where they could also make speech records at the end. Here, self-reported arousal (assessed on a five-point Likert scale ranging from “low energy” (1) to “high energy” (5)), contentment, and sadness were assessed in separate items on four-point Likert scales (ranging from “not at all” (1) to “very much” (4)) among other psychological properties as part of an experience sampling procedure [@wuMultimodalDataCollection2021]. In Study 2, we captured emotional valence on two items (contentment and sadness) instead of one as done in Study 1. According to the affect grid, contentment and sadness have a comparable low level of arousal and an opposing emotional valence [@russellCircumplexModelAffect1980a; @posnerCircumplexModelAffect2005].

For the audio records, participants received the following instruction: “Please record a short audio clip in which you describe the situation you are in, and your current thoughts and feelings. Collect about 10 seconds of environmental sound after the description.” The responses to this prompt are analyzed in the present study. Any parts of the record that did not contain speech were cut out before further analysis since the focus of this work is on affect in human speech. The collected speech samples had also been used in another research project that describes the data collection procedure in more detail [@marreroEvaluatingVoiceSamples2022].

```{r study2 sample data, include = FALSE}

library("dplyr")

## load data

# ema and voice data before data cleaning 
affect_voice_study2  <- readRDS("../data/study2/affect_voice_study2.rds")

# count how many speech samples are available per participant in raw data

count_es_user <- affect_voice_study2 %>%
  dplyr::group_by(user_id) %>%
  dplyr::summarise(n = n(), .groups = 'drop')

es_init_sample <- count_es_user %>%
  summarise(mean_n = mean(n), sd_n = sd(n))


# final merged data set of voice data with esm data after cleaning 
affect_egemaps_wordembeddings_study2  <- readRDS("../data/study2/affect_voice_wordembeddings.rds")

```

In total, we collected `r nrow(affect_voice_study2)` audio logs with corresponding affect self-reports from `r length(unique(affect_voice_study2$user_id))` participants. Participants made on average `r round(es_init_sample$mean_n,2)` (*SD* = `r round(es_init_sample$sd_n,2)`) speech records with corresponding affect self-reports. We followed the identical procedure to filter the data as in Study 1: First, we excluded the data from 281 participants with less than ten audio records in total and from another participant who had no variance in all their self-reports across all their experience samples. To ensure comparability of the two studies with regard to the length of speech samples, we removed 6,871 speech transcripts that contained less than 15 words and were less than four seconds long which is equivalent to the length of the sentences that had been read out in Study 1. Acoustic features indicated that human voice had been recorded in all of the remaining speech samples. 

This procedure left us with a final data set of `r nrow(affect_egemaps_wordembeddings_study2)` speech samples with an average length of 52.37 words (*SD* = 25.49) and duration of 34.33 seconds (*SD* = 13.13) with corresponding experience-sampled self-reports on momentary affective experience from `r length(unique(affect_egemaps_wordembeddings_study2$user_id))` participants (`r round((table(affect_egemaps_wordembeddings_study2$Gender)[1] / sum(table(affect_egemaps_wordembeddings_study2$Gender)))*100,2)`% female, Age~M~ = `r round(mean(affect_egemaps_wordembeddings_study2$Age, na.rm = T),2)` years). In the final sample, there were on average 22.51 (*SD* = 14.10) speech samples with corresponding affect self-reports per participant. Overall participants reported balanced experienced contentment (*M* = `r round(mean(affect_egemaps_wordembeddings_study2$content),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings_study2$content),2)`) and low sadness (*M* = `r round(mean(affect_egemaps_wordembeddings_study2$sad),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings_study2$sad),2)`). Overall arousal was balanced out (*M* = `r round(mean(affect_egemaps_wordembeddings_study2$arousal),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings_study2$arousal),2)`). The distribution of self-reported arousal, contentment, and sadness scores is provided in the repository.

Replicating the approach from Study 1, we extracted the extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPS) and the 2016 Interspeech Computational Paralinguistic Challenge set from the collected audio files using the OpenSMILE algorithm [@eybenOpensmileMunichVersatile2010; @eybenGenevaMinimalisticAcoustic2016; @schullerINTERSPEECH2016Computational2016]. In contrast to the on-device feature-extraction approach in Study 1, those features were extracted from the raw recorded audio files after data collection in Study 2.

We transcribed all raw audio records using the Google Speech-to-text API (version 1). Automatic speech-to-text technology has been shown to be well-suited for transcription tasks in psychological research [@pfeiferHowReadySpeechtotext2024]. The Google API also assigns a sentiment score (*M* = `r round(mean(affect_egemaps_wordembeddings_study2$Sentiment.score, na.rm =T),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings_study2$Sentiment.score, na.rm =T),2)`) within the interval of [-1; 1] to each speech transcript. Then, we extracted state-of-the-art word embeddings from speech transcripts using the *text* R package [@kjellTextRpackageAnalyzing2021]. Word embeddings are vector representations of words in a high-dimensional space, which capture their contextualized meaning and relationships with other words. Specifically, we used the 1,024 dimensions from the second to last layer (layer 23) from the language model “RoBERTa large” as features as recommended in prior work [@liuRoBERTaRobustlyOptimized2019; @materoEvaluatingContextualEmbeddings2022].

## Machine learning

In both studies, we trained linear Elastic Net regularized regression models [@zouRegularizationVariableSelection2005] and non-linear tree-based Random Forest models [@breimanRandomForests2001; @wrightRangerFastImplementation2017], and a baseline model on the extracted features for the prediction of self-reported affective experience. The baseline model predicted the respective mean values for affective experience of the respective training set for all cases in a test set. We evaluated model performance using a five times repeated ten-fold cross-validation scheme [@bischlResamplingMethodsMetaModel2012] and used blocking of participants in the resampling procedure to ensure that for each train/test set pair the given participant is either in the training set or in the test set. In the results section, we report on the average (median) model performance across those 50 models. Before predictive modeling, we replaced extreme outliers (mean +/- four times *SD*) with missing values that were imputed as part of the resampling procedure. 

In our main analyses, we predicted self-reported affective states (Study 1: arousal and valence; Study 2: arousal, contentment, sadness) from prosodic features (eGeMAPS feature set). For Study 2, we also used semantic content, captured by word embeddings, as predictor. We also conducted a number of supplementary analyses that can be found in the repository: As done in prior research [@weidmanNotHearingHappiness2020], we predicted affective states from the extensive 2016 Interspeech Computational Paralinguistic Challenge set [@schullerINTERSPEECH2016Computational2016] to investigate whether a larger prosodic feature set would affect prediction performance. Moreover, in line with prior work [@weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020], we predicted momentary fluctuations in affective experience from each individual's baseline (defined as their median response) to explore if those would be also predictable from speech cues. Specifically for Study 1, we predicted self-reported affective states from prosodic features separately for each sentence condition (positive, neutral, negative) to assess whether the sentiment of the read-out sentences would affect prediction performance. Here, we also conducted *F*-tests to analyze whether models' prediction errors were significantly different across sentence conditions. For Study 2, we predicted self-reported affective states from a combination of prosodic and semantic features to investigate whether combining those features improved overall performance.

Our prediction models were evaluated based on how accurate new (unseen) samples can be predicted. Throughout this manuscript, we report on the Spearman correlation ($\rho$) of true and predicted scores and the coefficient of determination (*R*^2^) as measures of model fit. In the repository, we provide the mean absolute error (*MAE*) and the root mean squared error (*RMSE*) as additional performance measures for prediction models. In our main analyses, we carried out variance-corrected (one-sided) *t*-tests comparing the *R*^2^ measures of prediction models with those of the baseline models [@nadeauInferenceGeneralizationError2003] to determine whether prosodic and semantic features predicted affective experience beyond chance (*alpha* = 0.05). We adjusted for multiple comparisons (*n* = 16) via Holm correction.

```{r gender prediction results, echo = FALSE, warning = FALSE}

library(mlr3)

# load gender pred results 
bmr_egemaps_gender_study1 <- readRDS("../results/study1/bmr_egemaps_gender_study1.rds")
bmr_egemaps_gender_study2 <- readRDS("../results/study2/bmr_egemaps_gender_study2.rds")

mes_egemaps_gender_study1 <- bmr_egemaps_gender_study1$aggregate(msr("classif.acc"))
mes_egemaps_gender_study2 <- bmr_egemaps_gender_study2$aggregate(msr("classif.acc"))

```

Our quality checks were data driven, based on voice features (i.e., humans marked as absent when mean voicing score < 0.5, voiced segments per second = 0, mean voiced segment length = 0). We evaluated the quality of the resulting speech data by training machine learning models on a task they should be able to accomplish if the data are of high quality. Specifically, we trained the models with gender as the dependent variable because past work indicates that speaker gender can be reliably predicted from voice cues [@kwasnyGenderAgeEstimation2021]. Those models predicted speaker gender from prosody with very high accuracy (Study1: Accuracy ~md~ = `r round(mes_egemaps_gender_study1[which(mes_egemaps_gender_study1$learner_id == "imputeoor.classif.ranger" & mes_egemaps_gender_study2$task_id  == "egemaps_gender")]$classif.acc, 4)*100`%; Study2: Accuracy~md~ = `r round(mes_egemaps_gender_study2[which(mes_egemaps_gender_study2$learner_id == "imputeoor.classif.ranger" & mes_egemaps_gender_study2$task_id  == "egemaps_gender")]$classif.acc, 4)*100`%), indicating good data quality.

All data processing and statistical analyses in this work were performed with the statistical software R version 4.0.4 [@rcoreteamLanguageEnvironmentStatistical2021]. For machine learning, we used the *mlr3* framework [@langMlr3ModernObjectoriented2019]. Specifically, we used the *glmnet* [@friedmanRegularizationPathsGeneralized2010] and *ranger* [@wrightRangerFastImplementation2017] packages to fit machine learning models. We provide the R code, figures, and results in the project's repository on the Open Science Framework (https://osf.io/a5db8/). Raw data of the voice samples may contain personally identifiable information and, therefore, cannot be shared publicly. We preregistered Study 1 as a transparent account of our work [@kochPredictingAffectiveStates2021] and extended the analytical approach to the data from Study 2.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
