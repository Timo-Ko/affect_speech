---
title             : "AI-based recognition of momentary subjective affect experience from natural speech"
shorttitle        : "Subjective Affect Experience in Natural Speech"

author: 
  - name          : "Timo K. Koch"
    affiliation   : "1,2,3"
    corresponding : yes    # Define only one corresponding author
    address       : "Torstrasse 25, 9000 St. Gallen, Switzerland"
    email         : "timo.koch@unisg.ch"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Formal Analysis"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Gabriella M. Harari"
    affiliation   : "3"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
  - name          : "Ramona Schoedel"
    affiliation   : "2,4"
    role:
      - "Conceptualization"
      - "Writing - Review & Editing"
  - name          : "Zachariah Marrero"
    affiliation   : "5"
    role:
      - "Data curation"
  - name          : "Florian Bemmann"
    affiliation   : "6"
    role:
      - "Software"
  - name          : "Samuel D. Gosling"
    affiliation   : "5"
    role:
      - "Resources"
  - name          : "Markus Bühner"
    affiliation   : "2"
    role:
      - "Resources"
  - name          : "Clemens Stachl"
    affiliation   : "1"
    role:
      - "Writing - Review & Editing"
      - "Supervision"  

affiliation:
  - id            : "1"
    institution   : "Institute of Behavioral Science and Technology, University of St. Gallen"
  - id            : "2"
    institution   : "Department of Psychology, Ludwig-Maximilians-Universität München" 
  - id            : "3"
    institution   : "Department of Communication, Stanford University"
  - id            : "4"
    institution   : "Charlotte Fresensius Hochschule, University of Psychology"
  - id            : "5"
    institution   : "Department of Psychology, The University of Texas at Austin"
  - id            : "6"
    institution   : "Media Informatics Group, Ludwig-Maximilians-Universität München"
note: "\\clearpage" #added this to move entire author note to second page
authornote: |
  Conflict of interest: none
  Acknowledgements: We thank Peter Ehrich and Dominik Heinrich for their support with the technical implementation of the on-device voice feature extraction, ZPID for the support with data collection for study 1, and audEERING for the assistance with the implementation of the on-device audio feature extractor. Thank you Sumer Vaid for the technical support with the set up of the environment for the analyses of study 2. This project was supported by the Swiss National Science Foundation (SNSF) under project number 215303 and a scholarship of the German Academic Scholarship Foundation awarded to the first author.
  All materials are available in the project’s repository on the Open Science Framework (OSF): https://osf.io/a5db8/

abstract: |
  Advances in the area of artificial intelligence (AI) and the ubiquity of speech data, for example from voice assistants, have created numerous commercial products that claim to be able to automatically recognize emotions from human voice. However, the employed algorithms have often been trained solely on enacted or labelled speech samples from artificial lab settings representing affect *expression* and are used to infer everyday subjective affect *experience*. In the present work, we investigate if machine learning algorithms can truly recognize subjective affect momentary experience from speech samples collected in the wild. In two studies, we extract acoustic voice parameters and state-of-the-art word embeddings from 24766 speech samples with corresponding experience-sampled self-reports on momentary affect experience from 997 participants collected using off-the-shelf smartphones. While voice acoustics provide limited predictive information of subjective affective arousal, speech content is much more predictive. From voice cues, the fundamental frequency, spectral flux, and loudness revealed most affective information. Further, experimental and explorative findings suggest that emotional speech content does not affect predictions from voice acoustics (i.e., what someone talks about does not affect how well emotions can be inferred from voice cues alone). We discuss implications for the algorithmic monitoring of affect experience from speech in everyday life.
  
keywords          : "Artificial Intelligence, Affect, Emotion, Speech, Voice"
wordcount         : "6135"

bibliography      : ["../speech_paper_library.bib"]

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

documentclass     : "apa6"
classoption       : "man"
output: 
  papaja::apa6_pdf:
    figsintext: yes

---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```


# Introduction

<!-- One catchy intro paragraph on voice AI boom and opportunities / promises of affect recognition research, commercial tools and applications - relevance and link to study - especially focus on excitement around voice! -->
In a world where subjective well-being is central to our quality of life, detecting how people genuinely feel, as opposed to merely observing what they outwardly express, is crucial to gaining a deeper understanding of human emotions and guiding the development of technologies that better serve and respond to our emotional needs. Here, recent scientific breakthroughs in the algorithmic recognition of affective states and related affective disorders from speech and particularly voice cues offer promising applications, for instance in (mental) health care, human-machine interaction, education, and business [@millingSpeechNewBlood2022; @hildebrandVoiceAnalyticsBusiness2020; @vlahosTalkMeHow2019]. Fueled by the ubiquity of speech data due to the rise of voice assistants (e.g., Amazon's Alexa and Apple's Siri) there is also an increasing commercial interest in deploying emotion-detecting algorithms in everyday life to quantify at scale how patients, customers, and employees are feeling in a given moment often with the intention to provide personalized information [@matzUsingBigData2017]. We argue that affect-recognition models used in research and in the related commercial tools are trained on enacted or labelled speech samples from artificial lab settings that represent affect *expressions* but are deployed by claiming to detect people’s subjective affect *experience* in everyday life. So far, it has been unclear if the performance of those AI models can generalize from recognizing observable affect expressions to subjective affect experience. The present work investigates the algorithmic recognition of subjective self-reported momentary affect *experience* from speech samples collected with smartphones in everyday life.

## Predicting affect expression and experience from voice cues

<!-- voice cues AND speech content; Studies on affect expression, very good performance reported!, difference versus affect experience  -->
Research has reported on successfully predicting affective states (e.g., discrete emotions) from a range of speech data, such as labelled TV clips [@grimmVeraAmMittag2008], recorded phone calls, and enacted speech samples from the lab [@banzigerIntroducingGenevaMultimodal2012; @burkhardtDatabaseGermanEmotional2005a; @schullerSpeechEmotionRecognition2018; @vogtAutomaticRecognitionEmotions2008]. They report on impressive prediction performances for the automatic recognition of emotions with correlations between true scores and predicted scores of up to .81 for arousal and .68 for valence predictions [@weningerAcousticsEmotionAudio2013]. In those works, the target emotion that had been enacted by professional actors or labels that had been assigned by raters serve as ground truth. Thereby, these findings are concerned with detecting affect *expression*.  This comes with a set of downsides, such as actors potentially overacting and the ambiguity of ground truth due to the subjective nature of labeling [@batlinerAutomaticRecognitionEmotions2011; @schullerSpeechEmotionRecognition2018; @wiltingRealVsActed2006]. Moreover, there is a phenomenological difference between what affect we observably express and what affect we subjectively experience, even though there is an overlap to a varying degree and how those manifest in speech cues (see Figure \@ref(fig:experiencevsexpression)). In this context, affect *expression* represents the emotional expressive behavior based on our internal affect *experience*. At first sight, it might seem trivial to distinguish between affect expression and affect experience, but this distinction is significant because expressions can often be intentionally moderated or socially conditioned, while experiences reflect a person's genuine emotional state. However, "feeling is not always revealing", i.e., one does not necessarily express what one experiences affectively or might even express something completely different [@grossRevealingFeelingsFacets1997; @grossDissociationEmotionExpression2000]. Furthermore, the way one expresses affect might be perceived and interpreted differently due to interindividual, situational, and cultural differences. We argue that the experience of affect is the key criteria of interest to both researchers and commercial actors, as it provides a deeper understanding of how individuals truly feel rather than just how they outwardly express themselves, which can be superficial or misleading. This focus on affective experience over mere expressions allows for a more accurate and meaningful interpretation of emotional states, which is crucial in developing empathetic technologies and meaningful interactions.

```{r experiencevsexpression, echo = FALSE, warning=FALSE, fig.cap="Diagram illistrating the conceptual difference between subjective affect experience and observable affect expression and how they overlap with speech cues.", fig.scap= "Experience versus expression", out.width="75%", fig.align="center", fig.pos. = "H", results="asis"}

knitr::include_graphics(path = "../figures/experience_vs_expression.png")
```

<!-- studies on everyday speech + EAR and its downsides + how smartphones can help -->
Studies investigating predictions of subjective affect experience from speech collected in everyday life are rare and algorithmically recognizing affect expression is considered easier detecting than real-life *experienced* affect [@vogtAutomaticRecognitionEmotions2008]. Some recent works have collected everyday speech samples using the Electronically Activated Recorder (EAR), a small electronic device usually attached to one's clothes that unobtrusively captures snippets of ambient sounds and conversations throughout the day [@mehlElectronicallyActivatedRecorder2017]. Hereby, everyday speech samples can be collected over a period of time which allows researchers to not only investigate between-person differences in affect (i.e., is this person sad?), but also assess within-person fluctuations (i.e., is this person more sad than other days?) [@huangPredictionEmotionChange2018; @weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020]. Using the EAR can raise issues regarding privacy protection since potentially non-consenting persons may be recorded, too. Moreover, handling the EAR recorders and transmitting the collected data can be tedious for participants and researchers. Here, off-the-shelf smartphones represent a useful platform to collect experience samples on momentary affect experience over time and make corresponding speech records using the build-in microphone [@weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020; @carlierSearchStateTrait2022].

## Prosody-content interactions in affect recognition from speech

<!-- considering the interplay of speech from and content: WHAT people talk about and how it impacts acoustics, comparing phonetic and semantic predictions, how semantic content affects acoustics, how this could be investigated using smartphones -->
Prior research has shown that voice cues (prosody) and the lexical content of the produced words (semantics) work together when transmitting affective information through speech [@ben-davidProsodySemanticsAre2016]. Moreover, some studies even suggest that there is a prosodic (i.e., from voice cues) dominance in the perception of affect based on data from lab experiments [@ben-davidProsodySemanticsAre2016; @linyiProsodyDominatesSemantics2020], but not (yet) using speech data from the wild [@schwartzEmotionalSpeechProcessing2012]. Moreover, while this research field had focused on the interplay of prosody and semantics in the recognition of affect by human raters, there are, to our knowledge, no studies on prosody-content interactions in algorithmic affect detection. Hence, it is currently unclear if what users talk about (i.e., the emotional content) has an effect on voice acoustics that impact automated affect recognition. In an applied setting, for example, the question is if an AI model could recognize affective states regardless of what the person talks about, may it be a mundane topic, such as the weather or ordering pizza, or does one need to talk about an emotional topic (e.g., meeting a loved one). 

## The ambiguous sound of expressed emotional prosody
<!-- summarize key findings of affect-prosody links, BUT again only regarding affect expressions! -->
For algorithms to decipher the relationship between prosody and affective states, they need to capture the complex interplay of acoustic features. Here, prior research has shown how pitch, volume, speech rate, and voice quality, or timbre, serve as central cues in conveying emotions. For instance, higher pitch and faster speech rates are generally associated with excitement or stress [@banseAcousticProfilesVocal1996]. However, while there is a degree of consistency in how emotions are expressed and perceived through prosody, significant variability exists in these affect-voice mappings [@larrouy-maestriSoundEmotionalProsody2024; @banseAcousticProfilesVocal1996; @schererVocalCommunicationEmotion2003]. One source of variability is the cultural background of the person rating emotional speech samples [@vanrijnModellingIndividualCrosscultural2023]. This variability underscores the challenges faced by AI in affect recognition, emphasizing that while certain prosodic features might be universally recognized, their interpretation can vary between individuals. However, all those findings are based on data from enacted emotional speech from actors or affect labels from raters representing affect expressions and it remains unclear if the detected affect-prosody links also transfer to subjective affect experience.

<!-- conclusion of intro & theory, wrap up, lead to this study, present tense! -->
The present work leverages methodological advances in the area of smartphone-based data collection methods to investigate the AI-based recognition of momentary affect experience from speech. In two large-scale field studies, we train cross-validated machine learning models on acoustic voice cues and state-of-the-art word embeddings from speech samples collected in the wild. While the first study control for speech's content, the second study allows for free speech. In prediction models, we investigate the effects of the emotionality of speech's content on algorithmic affect recognition from voice acoustics. Moreover, we investigate which voice cues carry the most affective information. Thereby, we aim to inform potential applications and promises in automatic affect recognition from speech signals.

# Method

## Study 1: Data collection

<!-- Study data collection description + ESM -->
Data collection for study 1 was part of a large panel study (from May 15 until November 14, 2020) using the *PhoneStudy* research app [@schoedelBasicProtocolSmartphone2020]. Data collection was approved by the ethics committee of the psychology department at LMU Munich and all procedures adhered to the General Data Protection Regulation (GDPR). We recruited a quota sample of *N* = 850 participants representative for the German population regarding age, gender, education, income, confession, and relationship status. Participants had to be between 18 and 65 years old, fluent in German, and in the possession of an Android smartphone (Android 5 or higher) for private usage as a sole user (for more details on the panel study see @schoedelSnapshotsDailyLife2023 and  @grossedetersKeepScrollingUsing2024). The study also comprised of two two-week experience sampling phases (July 27, 2020, to August 9,2020; September 21, 2020, to October 4, 2020) during which participants received two to four short questionnaires per day on their personal smartphone. Here, self-reported valence and arousal were assessed in two separate items on six-point Likert scales among other psychological properties (i.e., sleep quality, stress, situational characteristics that had been used in other research) as part of an experience sampling procedure.

<!-- voice: on device acoustic feature extraction -->
The last experience sampling questionnaire of each day included an additional instruction, where participants were asked to read out a series of predefined emotional sentences while making an audio recording of their voice. The sentences presented to the participants are based on a set of 54 validated German neutral and affective sentences [@defrenEmotionalSpeechPerception2018] and differ in their emotional content: positive (e.g., "My team won yesterday."), negative (e.g., "Nobody is interested in my life."), and neutral (e.g., "The plate is on the round table."). In each measurement instance, participants were instructed to read out three positive, neutral, and negative sentences. The order of the categories was randomized per experience sampling instance. For each emotional content category, three sentences were randomly drawn (with replacement) from the respective sets of sentences in the database. The experimental manipulation of these emotional semantic categories allowed us to control for the content spoken by our participants and at the same time enabled us to conduct a privacy-friendly study. The audio recording was started by the participants via a button on the screen. Participants could stop the recording manually after a minimum of four seconds. Alternatively, the recording was stopped automatically after twelve seconds. We chose these lower- and upper-time thresholds because this is the minimum and maximum time required to read out the three sentences per condition. Once the audio record had been completed, we used the widely adopted *OpenSMILE* open-source algorithm [@eybenOpensmileMunichVersatile2010] to automatically extract acoustic features directly on the participant’s device. Here, we used the extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPS) that is comprised of 88 acoustic features [@eybenGenevaMinimalisticAcoustic2016]. The eGeMAPS feature set has been used in a range of prior studies on affect recognition from speech [@vanrijnModellingIndividualCrosscultural2023; @weidmanNotHearingHappiness2020]. After feature extraction, the voice records were automatically deleted and only extracted voice features were stored on our servers. As a result, there were three sets of voice features per experience sampling instance (one per sentiment condition).
```{r study 1 sample data, echo = FALSE, warning = FALSE, results = FALSE, message = FALSE}

library("dplyr")

## load data

# merged voice and affect data before quality checks
affect_voice_study1  <- readRDS("../data/study1/affect_voice_changed.rds")

# final merged data set of voice data with esm data after quality checks
affect_voice_study1_cleaned  <- readRDS("../data/study1/affect_voice_study1_cleaned.rds")

# count how many es instances with valence and arousal ratings are available per participant in raw data

count_es_user <- affect_voice_study1 %>%
  dplyr::group_by(user_id) %>%
  dplyr::summarise(n = n(), .groups = 'drop')

es_init_sample <- count_es_user %>%
  summarise(mean_n = mean(n), sd_n = sd(n))

# find participants with less than 10 es instances
length(which(count_es_user$n < 10))

```
<!-- describe the sample, raw sample and final sample  -->
We collected `r nrow(affect_voice_study1)` audio logs with respective affect self-reports from `r length(unique(affect_voice_study1$user_id))` participants. Participants made on average `r round(es_init_sample$mean_n, 2)` (*SD* = `r round(es_init_sample$sd_n, 2)`) voice records. We excluded data from `r length(which(count_es_user$n < 10))` participants with less than ten experience sampling instances and eight participants who had no variance in all their valence and arousal scores across all their experience samples. Finally, we excluded 214 voice logs from 5 participants because the respective voice indicators suggested that no human voice was recorded (mean voicing score < 0.5, voiced segments per second = 0, mean voiced segment length = 0).
This left us with a final data set of `r nrow(affect_voice_study1_cleaned)` voice samples with corresponding acoustic features from `r length(unique(affect_voice_study1_cleaned$e_s_questionnaire_id))` experience sampling instances for valence and arousal from `r length(unique(affect_voice_study1_cleaned$user_id))` participants (`r round((table(affect_voice_study1_cleaned$Demo_GE)[2] / sum(table(affect_voice_study1_cleaned$Demo_GE)))*100,1)`% female, Age~M~ = `r round(mean(affect_voice_study1_cleaned$Demo_A1, na.rm = T),2)` years). In the final sample, there were on average 8.22 (*SD* = 3.53) affect self-reports with 24.1 (*SD* = 10.6) voice logs per participant. Overall self-reported valence was positive (*M* = `r round(mean(affect_voice_study1_cleaned$valence),2)`, *SD* = `r round(sd(affect_voice_study1_cleaned$valence),2)`) and overall arousal was slightly geared towards activation (*M* = `r round(mean(affect_voice_study1_cleaned$arousal),2)`, *SD* = `r round(sd(affect_voice_study1_cleaned$arousal),2)`). The distribution of valence and arousal ratings is provided in the online supplements. 

## Study 2: Data collection

<!-- US Study data collection description + ESM -->
Data collection for study 2 was part of the UT1000 Project at the University of Texas at Austin in the United States in fall 2018 [@wuMultimodalDataCollection2021]. During a three-week self-tracking assignment using their smartphones, students from a Synchronous Massive Online Course (SMOC) in introductory psychology received four short experience sampling questionnaires per day where they could also make records of their speech at the end. Here, self-reported arousal (assessed on a five-point Likert scale), contentedness, and sadness were assessed in separate items on four-point Likert scales among other psychological properties as part of an experience sampling procedure. Thereby, in Study 2, we captured emotional valence on two items (contentedness and sadness) instead of one as done in Study 1. According to the affect grid, contentedness and sadness have a comparable low level of arousal and an opposing emotional valence [@russellCircumplexModelAffect1980a; @posnerCircumplexModelAffect2005].  
For the audio records, participants received the following instruction: "Please record a short audio clip in which you describe the situation you are in, and your current thoughts and feelings. Collect about 10 seconds of environmental sound after the description.” The responses to this prompt are analyzed in the present study. Any parts of the record that did not contain speech were cut out before further analysis since the focus of this work is affect in human speech. The collected speech samples had also been used in another research project that describes the data collection procedure in more detail [@marreroEvaluatingVoiceSamples2022].

```{r study2 sample data, include = FALSE}

library("dplyr")

## load data

# ema and voice data before data cleaning 
affect_voice_study2  <- readRDS("../data/study2/affect_voice_study2.rds")

# final merged data set of voice data with esm data after cleaning 
affect_egemaps_wordembeddings_study2  <- readRDS("../data/study2/affect_voice_wordembeddings.rds")

```

<!--  initial sample, preprocessing -->
With this procedure, we collected `r nrow(affect_voice_study2)` audio logs with corresponding affect self-reports from `r length(unique(affect_voice_study2$user_id))` participants. We followed the same procedure to filter the data as in Study 1: First, we excluded 281 participants with less than ten experience sampling instances and another participant who had no variance in all their self-reports across all their experience samples. Next, we removed 337 voice records from 11 participants where the respective acoustic features indicated that no human voice was recorded and, to ensure comparability of the two studies with regard to the length of speech samples, we retained all speech transcripts that contained at least 15 words and were more than four seconds long which is equivalent to the length of the sentences that had been read out in Study 1 

<!-- final sample description - study 2 -->
This procedure left us with a final data set of `r nrow(affect_egemaps_wordembeddings_study2)` speech samples with corresponding experience-sampled self-reports on momentary affect experience from `r length(unique(affect_egemaps_wordembeddings_study2$user_id))` participants (`r round((table(affect_egemaps_wordembeddings_study2$Gender)[1] / sum(table(affect_egemaps_wordembeddings_study2$Gender)))*100,1)`% female, *M*(Age) = `r round(mean(affect_egemaps_wordembeddings_study2$Age, na.rm = T),2)` years). Overall participants reported balanced experienced contentedness (*M* = `r round(mean(affect_egemaps_wordembeddings_study2$content),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings_study2$content),2)`) and low sadness (*M* = `r round(mean(affect_egemaps_wordembeddings_study2$sad),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings_study2$sad),2)`). Overall arousal was balanced out (*M* = `r round(mean(affect_egemaps_wordembeddings_study2$arousal),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings_study2$arousal),2)`). The distribution of arousal, contentedness, and sadness is provided in the online supplements. 

<!-- acoustic feature extraction -->
Replicating the approach from study 1 we extracted the extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPS) from the collected audio files using the OpenSMILE algorithm [@eybenOpensmileMunichVersatile2010; @eybenGenevaMinimalisticAcoustic2016]. In contrast to the on-device feature-extraction approach in study 1, in study 2 those features were extracted from the raw recorded audio files after data collection.

<!-- word embeddings -->
We transcribed all raw audio records using the Google Speech-to-text API. The API also assigns a sentiment score (*M* =`r round(mean(affect_egemaps_wordembeddings_study2$Sentiment.score, na.rm =T),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings_study2$Sentiment.score, na.rm =T),2)`) within the interval of [-1; 1] to each speech transcript. Then, we extracted state-of-the-art word embeddings from speech transcripts using the *text* R package [@kjellTextRpackageAnalyzing2021]. Word embeddings are vector representations of words in a high-dimensional space, which capture their contextualized meaning and relationships with other words. Specifically, we used the 1024 dimensions form the second to last layer (layer 23) from the language model "RoBERTa large" as features as recommended in prior work [@liuRoBERTaRobustlyOptimized2019; @materoEvaluatingContextualEmbeddings2022].

## Predictive modelling

<!-- general ml -->
In both studies, we trained linear LASSO (Least Absolute Shrinkage and Selection Operator) regularized regression models [@tibshiraniRegressionShrinkageSelection1996] and non-linear tree-based Random Forest models [@breimanRandomForests2001; @wrightRangerFastImplementation2017], and a baseline model on the extracted features for the prediction of self-reported affect experience. The baseline model predicted the respective mean values for valence and arousal of the respective training set for all cases in a test set. We trained additional prediction models with gender as the dependent variable to evaluate whether the collected data at all contained information about individual differences since past work indicates that speaker gender can be reliably predicted from voice cues [@kwasnyGenderAgeEstimation2021]. We evaluated models using a ten-fold five times repeated cross-validation scheme [@bischlResamplingMethodsMetaModel2012] and used blocking of participants in the resampling procedure to ensure that for each train/test set pair the given participant is either in the training set or in the test set. In the results section, we report on the average model performance across those 50 models.

<!-- Model evaluation -->
We evaluated the predictive performance of the models based on the coefficient of determination (*R*^2^) and Spearman's (rank) correlation (*r*) between the predicted scores and participants’ self-reported scores. To determine whether a model was predictive beyond chance (*alpha* = 0.05), we carried out variance-corrected (one-sided) t-tests comparing the *R*^2^ measures of prediction models from voice cues with those of the baseline models [@nadeauInferenceGeneralizationError2003]. We adjusted for multiple comparison  (in study 1: *n* = 4; in study 2: *n* = 6) via Holm correction. 
<!-- Software -->
All data processing and statistical analyses in this work were performed with the statistical software R version 4.1.1 [@rcoreteamLanguageEnvironmentStatistical2021]. For machine learning, we used the *mlr3* framework [@langMlr3ModernObjectoriented2019]. Specifically, we used the *glmnet* [@friedmanRegularizationPathsGeneralized2010] and *ranger* [@wrightRangerFastImplementation2017] packages to fit prediction models. We preregistered study 1 as a transparent account of our work [@kochPredictingAffectiveStates2021] and extended the analytical approach to the data from study 2.

# Results

## Voice contains little predictive information of affect experience, but content does

```{r study1 predictions, echo = FALSE, warning = FALSE}

library(mlr3)

#load benchmark results
bmr_egemaps_gender_study1 <- readRDS("../results/study1/bmr_egemaps_gender_study1.rds") 

bmr_egemaps_study1 <- readRDS("../results/study1/bmr_egemaps_study1.rds") 

mes = mlr3::msrs(c( "regr.rsq", "regr.srho")) # set performance measures

# get aggregated performance measures
mes_egemaps_gender_study1 <- bmr_egemaps_gender_study1$aggregate(msrs(c("classif.acc", "classif.auc")))

mes_egemaps_study1 <- bmr_egemaps_study1$aggregate(mes)

```

```{r study2 predictions, echo = FALSE, warning = FALSE}

library(mlr3)

#load benchmark results 
bmr_gender_study2 <- readRDS("../results/study2/bmr_egemaps_gender_study2.rds")

bmr_egemaps_study2 <- readRDS("../results/study2/bmr_egemaps_study2.rds")

bmr_wordembeddings_study2 <- readRDS("../results/study2/bmr_wordembeddings_study2.rds")

mes = mlr3::msrs(c( "regr.rsq", "regr.srho")) # set performance measures

# get aggregated performance measures
mes_egemaps_gender_study2 <- bmr_gender_study2$aggregate(msrs(c("classif.acc", "classif.auc")))
mes_egemaps_study2 <- bmr_egemaps_study2$aggregate(mes)
mes_wordembeddings_study2 <- bmr_wordembeddings_study2$aggregate(mes)

```

<!-- study 1: prediction from voice cues -->
Overall, the employed machine learning algorithms only showed a low prediction performance in predicting subjective momentary affect experience from voice cues. Figure \@ref(fig:predictionoverview) provides an overview of the performance of all learners across prediction tasks while in this section we report on the best performing algorithm respectively (either Random Forest or LASSO). 
For prompted speech (study 1), the prediction of affective arousal (*r*~m~ = `r round(mes_egemaps_study1[which(mes_egemaps_study1$learner_id == "imputehist.regr.cv_glmnet" & mes_egemaps_study1$task_id  == "egemaps_arousal")]$regr.srho, 2)`) was significantly better than chance. Valence predictions did not yield significant results (*r*~m~ = `r round(mes_egemaps_study1[which(mes_egemaps_study1$learner_id == "imputehist.regr.cv_glmnet" & mes_egemaps_study1$task_id  == "egemaps_valence")]$regr.srho, 2)`).

<!-- study 2: prediction from voice cues -->
Employed machine learning models trained on voice acoustics from participants talking freely about their current situation, thoughts, and feelings freely (study 2) did not yield a high prediction performance either. Still, the prediction of arousal (*r*~m~ = `r round(mes_egemaps_study2[which(mes_egemaps_study2$learner_id == "imputehist.regr.cv_glmnet" & mes_egemaps_study2$task_id  == "egemaps_arousal")]$regr.srho, 2)`) was significantly better than the baseline. Also, predictions of contentedness (*r*~m~ = `r round(mes_egemaps_study2[which(mes_egemaps_study2$learner_id == "imputehist.regr.cv_glmnet" & mes_egemaps_study2$task_id  == "egemaps_content")]$regr.srho, 2)`) were significantly better than chance, but not for sadness (*r*~m~ = `r round(mes_egemaps_study2[which(mes_egemaps_study2$learner_id == "imputehist.regr.cv_glmnet" & mes_egemaps_study2$task_id  == "egemaps_sad")]$regr.srho, 2)`). 

<!-- using larger acoustic feature set did not improve acoustic predictions, ie no issue with voice features -->
Further, as done in prior research on voice-affect predictions [@weidmanNotHearingHappiness2020], we also compared the prediction performance of machine learning models trained on the much larger Compare2016 (6,737 features) acoustic feature set [@schullerINTERSPEECH2016Computational2016] in contrast to the economic eGeMAPS (88 features) feature set we had used.  Just as in prior research, the larger voice feature set did not yield better affect predictions [@weidmanNotHearingHappiness2020]. Detailed predictions results are provided in the online supplements. 

<!-- study 2: word embeddings and combined voice + embeddings -->
On the contrary, prediction models trained on word embeddings that had been extracted from free speech yielded much better performance than voice cues. Here, speech cues were predictive of contentedness (*r*~m~ = `r round(mes_wordembeddings_study2[which(mes_wordembeddings_study2$learner_id == "imputehist.regr.cv_glmnet" & mes_wordembeddings_study2$task_id  == "wordembeddings_content")]$regr.srho, 2)`) , arousal (*r*~m~ = `r round(mes_wordembeddings_study2[which(mes_wordembeddings_study2$learner_id == "imputehist.regr.cv_glmnet" & mes_wordembeddings_study2$task_id  == "wordembeddings_arousal")]$regr.srho, 2)`), and sadness (*r*~m~ = `r round(mes_wordembeddings_study2[which(mes_wordembeddings_study2$learner_id == "imputehist.regr.cv_glmnet" & mes_wordembeddings_study2$task_id  == "wordembeddings_sad")]$regr.srho, 2)`).

Finally, models trained on voice cues and word embedding combined yielded a similar prediction performance compared to those models trained on word embeddings only. This pattern suggest that predictions were mostly driven by the information coming from speech content as represented in the word embeddings. 
\newpage
```{r predictionoverview, echo = FALSE, warning=FALSE, fig.cap="Box and whisker plot of out-of-sample prediction performance measures from five times repeated 10-fold cross-validation for each affect measure. The top part above the horizontal line belongs to study 1 and the part below the horizontal line to study 2.", fig.scap= "Prediction performance", out.width="100%", out.height="100%", fig.align="center",  fig.pos. = "H", results="asis"}

knitr::include_graphics(path = "../figures/bmr_plot.png")
```
\newpage
<!-- give some details on predictive word embeddings in study 2 ?? No! very student specific! -->
## Content sentiment has no effect on affect predictions from voice cues alone 

```{r sentenceanova, echo = FALSE, warning = FALSE, results = FALSE, message = FALSE}

library(car)

predictions_condition <- readRDS(file="../results/study1/predictions_condition.RData")

# run anova comparing means for valence 
aov_valence <- car::Anova(aov(error_valence ~ condition, data = predictions_condition))

# run anova comparing means for arousal
aov_arousal <- car::Anova(aov(error_arousal ~ condition, data = predictions_condition))

```

<!-- study1: is absolute error different across conditions -->
In study 1, we analyzed if the experimentally altered emotional content (positive/ neutral/ negative) of the predefined sentences that had been read out by participants had an effect on affect predictions from voice acoustics. There were no significant differences in prediction errors across the three sentence conditions for valence (*F*~(2,9905)~ = `r round(aov_valence[3][[1]][1],2)`,
*p* = `r round(aov_valence$`Pr(>F)`[[1]],2)`) and arousal (*F*~(2,9905)~ = `r round(aov_arousal[3][[1]][1],2)`,
*p* = `r round(aov_arousal$`Pr(>F)`[[1]],2)`) predictions suggesting that sentences' emotional valence did not influence affect predictions from voice acoustics.

```{r sentimentcorrs, echo = FALSE, warning = FALSE, results = FALSE, message = FALSE}

predictions_sentiment <- readRDS(file="../results/study2/predictions_sentiment.rds")

cor_error_content <- cor.test(predictions_sentiment$sentiment, predictions_sentiment$error_content)
cor_error_arousal <- cor.test(predictions_sentiment$sentiment, predictions_sentiment$error_arousal)
cor_error_sad <- cor.test(predictions_sentiment$sentiment, predictions_sentiment$error_sad)

affect_voice_wordembeddings_study2 <- readRDS("../data/study2/affect_voice_wordembeddings_study2_ml.rds")

```

<!-- study 2: (no) Effects of sentiment on voice predictions, maybe (!) a little bit for contentedness - but keep in mind that predictions were not significant overall! -->
In study 2, in order to exploratively investigate the effect of the emotional valence of the spoken content on affect predictions from voice cues alone, we analyzed the correlation of content sentiment with the the absolute prediction errors in the prediction of self-reported momentary arousal, contentedness, and sadness from voice cues using the LASSO algorithm. Results indicate that content sentiment did not have an effect on affect predictions from voice cues: Correlations of content sentiment with absolute predictions errors was not significant for neither contentedness (*r* = `r round(cor_error_content$estimate[[1]], 2)`, *p* > .05), sadness (*r* = `r round(cor_error_sad$estimate[[1]], 2)`, *p* > .05), nor arousal (*r* = `r round(cor_error_arousal$estimate[[1]], 2)`, *p* > .05). When interpreting those results, one has to keep in mind that overall the predictive performance of the models trained on voice cues was generally low. We provide additional figures depicting the residuals in the online supplements. 
<!-- combined error figure goes to supplements -->

## Affective voice cues
<!-- general description and info-->
For models with better-than-chance predictions performance (i.e., arousal predictions in study 1 and 2 as well as contentedness predictions in study 2), we investigated the most informative features in a LASSO regularized model that had been trained on the full data sets to capture the most information possible. As a consequence, any findings need to be interpreted in the context of this specific work. The LASSO algorithm selects those voice parameters from the initial set of 88 voice features that are most informative and assigns those respective betas coefficients while setting uninformative variables to zero. Figure \@ref(fig:lassobetas) provides an overview of all selected voice cues in the regularized regression across prediction tasks and their standardized beta regression weights. All regression coefficients and descriptive (in-sample) correlations of voice features and self-reported affect experience are supplied in the online supplements. 
<!-- arousal predictions across studies -->
For the prediction of momentary arousal, the non-zero beta coefficients suggest that more features related to voice cues were selected in study 1 (scripted speech; p~retained~ = 25) in comparison to study 2 (free speech; p~retained~ = 8) with higher absolute average coefficients (B~meanS1~ = .11; B~meanS2~ = .03). Specifically, for the prediction of arousal from scripted speech, the variation of the third formant frequency (F3) from the speech signal was most informative. Formants are resonance frequencies in the vocal tract, and F3 is particularly important for speech clarity and vowel quality. Also the Hammarberg Index which measures the relative energy concentration in the higher frequencies compared to the lower frequencies within voiced speech segments was highly informative. 
For arousal predictions from free speech, the variation of the logarithmic amplitude (instead of frequency) of the third formant relative to the fundamental frequency (F0) was most infomative followed by th average (mean and median) loudness.
<!-- contentedness -->
For the recognition of contentedness, the LASSO algorithm selected 18 voice parameters with the variation of the third formant frequency (F3) being most informative - analogous to the arousal model trained on scripted speech. Also the variation of the first Mel-frequency cepstral coefficient (MFCC) for voiced speech segments was found to be informative of experience contentedness. The first MFCC typically captures the overall energy in the signal.
\newpage
```{r lassobetas, echo = FALSE, warning=FALSE, fig.cap="Standardized beta coefficients for eGeMAPs voice features from LASSO (Least Absolute Shrinkage and Selection Operator) regularized regression. All voice features that have been selected by any LASSO model for any target variable are shown. The target variables have been standardized to account for their varying scaling in the data sets.", fig.scap= "LASSO betas", out.width="100%", out.height="100%", fig.align="center",  fig.pos. = "H", results="asis"}

knitr::include_graphics(path = "../figures/betas_plot.png")
```
\newpage

# Discussion

<!-- mini summary of findings, use past tense here -->
In the present work, we extracted acoustic voice parameters and state-of-the-art word embeddings from speech samples collected using smartphones to predict subjective momentary affect experience. In contrast to prior work on the algorithmic recognition of affect expression, our findings suggest that voice cues provide only limited predictive information of subjective affect experience. Here, we identified loudness and fluctuations of the voice spectrum to contain most affective information. Also, experimental (Study 1) and explorative (Study 2) findings suggest that the content's sentiment of speech does not impact affect predictions from voice acoustics alone (i.e., what someone talks about does not influence how well affect can be predicted from their voice cues). On the contrary, we found speech content (word embeddings) to be particularly informative for the prediction of subjective affect experience.

<!-- comparing our results to prior findings: comparable performance to affect experience but lower performance than affect expression predictions; some works on how smartphones can help -->
Our results indicate that speech samples, and particularly their content, allow for the automatic prediction of subjective momentary affect experience. However, our machine learning models achieve a lower prediction performance as reported in prior work on automatic predictions of affect *expression* [@schullerSpeechEmotionRecognition2018]. Still, our reported performance is similar to studies predicting subjective affect *experience* from speech samples collected in the wild [@weidmanNotHearingHappiness2020; @carlierSearchStateTrait2022; @sunLanguageWellbeingTracking2020]. This observation is in line with prior research suggesting that real-life emotions are more difficult to algorithmically recognize than enacted or elicited emotions [@vogtAutomaticRecognitionEmotions2008]. Also, there are only few instances of extreme affect experiences in our data sets compared to the data used in prior studies on acted or labelled emotions. As a result, we rather predicted *mood* in this work, which is, by definition, less intense than emotions [@schererVocalCommunicationEmotion2003] and, consequently, more challenging to algorithmically recognize. 
<!-- arousal predictions worked in both studies, indicating that arousal can be algorithmically detected more easily -->
Across the two studies, arousal predictions from voice cues were more successful in comparison with emotional valence, highlighting prior work showing that the latter is more challenging to automatically infer due to its individual nature [@sridharUnsupervisedPersonalizationEmotion2022]. Moreover, to further relate our work to prior work in the field [@weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020], we also conducted supplementary analyses for the prediction of within-person fluctuations in affect experience. Overall, those yielded lower prediction performance than between-person differences. All analyses can be found in the online supplements.
<!--  content (word embeddings) way superior to voice cues (study 2) + what are implications? -->
State-of-the-art word embeddings showed a superior affect prediction performance compared to voice acoustics suggesting that speech content could be more informative for algorithms than voice cues. This result is in line with prior research that found speech content to be more predictive than voice cues in predicting momentary subjective experience of happiness [@weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020]. These findings contrast prior research suggesting that voice acoustics could be more relevant for human affect inferences than speech content [@ben-davidProsodySemanticsAre2016; @linyiProsodyDominatesSemantics2020]. We argue that potentially algorithms are better at picking up signals from text's structured nature in contrast to prosody's complexity  for example intonation of very specific words, subtle nuances while the ability to read vocal cues is deeply rooted in human communication.
<!-- we question the TRANSFERABILITY of prior findings to subjective affect experience and NOT the findings themselves; implications from predictions for research and practice  -->
Generally, our findings challenge the transferability of the optimistic prediction results that can be found in past work on the recognition of affect *expression* (e.g., enacted speech) to the recognition of subjective affect *experience* in everyday speech, particularly from acoustic voice cues. Thereby, our findings also question the proclaimed performance of commercial affect recognition algorithms deployed in daily life that have been trained on enacted or labelled affect *expression*. Consequently, current expectations regarding the performance of emotion-detecting AI services, especially the ones that are focused on voice cues, applied to everyday speech might be overly optimistic. Hence, more research is needed to determine how well algorithms can pick up on subjective affect experience from day-to-day speech.
<!-- the role of smartphones in future research -->
In future research, smartphones could play a prominent role in collecting and analyzing speech data and corresponding in-situ self-reports on subjective affect experience for affect inferences. Hereby, starting from our work, smartphones could be used as a mobile experimental lab to study different aspects of affect recognition from speech, for example by experimentally varying the content as done in Study 1 [@millerSmartphonePsychologyManifesto2012]. 

<!-- Sentence sentiment condition (study 1) and sentiment score (study 2) -->
By experimentally varying the emotional valence of the spoken content in Study 1 and exploratively investigating the effect of content sentiment on voice predictions in Study 2, our findings suggest that the sentiment of the content what participants talked about did not have a substantial impact on the prediction of affect experience from voice cues alone. This insight could imply that it does not matter what people talk about when algorithmically inferring affect experience from voice cues. However, this finding has to be considered in the light of overall limited predictive performances. Hence, more research is needed to disentangle speech content and form in automatic affect recognition.
<!-- some words on scripted sentences vs prompted speech , valence was better in study 2, but might be due to specific target -->
While the predefined sentences in Study 1 allowed us to control for the sentiment of the content of participants' voice records, they were unable to express themselves freely, which could have impaired predictions from voice acoustics compared to Study 2 where participants could talk freely. As a result, researchers and practitioners should consider the context of speech production and keep in mind that findings and trained models might be specific to the given production context and do not necessarily generalize well to other production contexts. 

<!-- summarize findings on affect-prosody links and embed our findings into existing lit  -->
Across the detected prosody-affect links in this work, a number of spectral properties (MFCCs, spectral flux, formant amplitudes, and bandwidths) and voice quality indices (Hammarberg Index) showed as important for the prediction of affect experience. This mix underscores that emotional expression and perception through voice are multifaceted, with both the static qualities of sound and their temporal variations being crucial. The presence of features unique to either arousal (e.g., F3 amplitude) or contentedness (e.g., mean loudness) predictions further illustrates that while some voice features are broadly applicable to emotional expression, others more specifically align with the nuances of emotional intensity (arousal) or the type of emotion conveyed (contentedness). These voice features have also been associated with affect expressions in prior work suggesting that those prosody-voice links generalize to affect experience [@larrouy-maestriSoundEmotionalProsody2024; @banseAcousticProfilesVocal1996; @vanrijnModellingIndividualCrosscultural2023]. Still, there is more research needed investigating those specific associations.

## Limitations and future directions

<!-- limitation 1: operationalizations / sample composition / slightly different target variable and participants in the two samples  -->
The findings of this work are limited in multiple ways. First, we used slightly different operationalizations of affect experience and sample compositions in the two studies that might affect their comparability: In Study 1, we assessed valence and arousal on a six-point Likert scale. In Study 2, we used two items to assess affective valence (contentedness and sadness) and arousal on five-/four-point Likert scales. As a consequence, findings might be transferable but not directly comparable across studies. We addressed this limitation by ensuring consistent methodologies and statistical approaches in both studies, allowing us to draw meaningful conclusions from our analyses. Further, while Study 1 drew on a representative German sample, Study 2 was based on a student convenience sample from the United States with the respective limitations, such as potential constraints in generalizability [@mullerDepressionPredictionsGPSbased2021]. Also, the German data set that was analyzed in study 1 only contained Android users because of the technical requirements of the logging software. However, past research suggests that the selection bias regarding participant demographics and personality for the German population is negligible [@schoedelSnapshotsDailyLife2022; @gotzUsersMainSmartphone2017; @keuschCoverageErrorData2020]. Moreover, both data sets have been collected in western countries, specifically Germany and the United States. Prior research suggests that cultural differences in emotion experience exist [@limCulturalDifferencesEmotion2016] and that mood inference models from, for example, smartphone sensing data do not necessarily generalize to other countries and cultures [@meegahapolaGeneralizationPersonalizationMobile2023]. Therefore, future studies should draw on diverse international samples from different cultural contexts in non-western countries.

<!-- limitation 2: how we collected the speech data (still artificial records) and ESM data + we do not know for sure that proper records have been made in study1 -->
Moreover, in contrast to prior work using passive speech sensing (e.g., via the EAR; @weidmanNotHearingHappiness2020), our participants had to actively log their speech in the present work. This artificial setting might have had an effect on results. Hence, the findings of this study might be subject to the specific instructions that had been given for the audio records. In this manner, future work should employ multiple different speech tasks for affect predictions and investigate how well predictions generalize from one task to another.
Also, similar to other work using the experience sampling method, our data might be impacted by peoples ability to complete experience samplings and make speech recordsin a given moment (e.g., phone nearby, driving). Moreover, study participants knew that their affect self-reports and corresponding speech samples would be recorded and later analyzed. As a consequence, with regard to assessed subjective affect experience, participants might have only completed the experience sampling questionnaire in selected affective situations, for example not when they were experiencing extreme affective states, or they had not reported on extreme affect at all [@schoedelSnapshotsDailyLife2022]. Moreover, participants might have not spoken as naturally as they would if they had not known that their data would be scientifically analyzed. Here, participants might have made the audio records only in selected suitable situations, for example when they were alone in a quiet place.

Additionally, specifically to Study 1, one limitation lies in our privacy-preserving on-device data pre-processing approach. By applying on-device feature extraction, we had no opportunity to check in detail if participants truly complied with study instructions and had recorded their voice while reading out the predefined sentences accordingly (beyond the data-driven quality checks we had applied). Further, our approach did not allow to control for records' background noise (e.g., when participants were outside next to a road) or how they held their smartphone during the voice record. However, we excluded samples without human voice based on voice indicators and speaker gender predictions that served as a sanity check yielded very good prediction results (prediction accuracy = `r round(mes_egemaps_gender_study1[which(mes_egemaps_gender_study1$learner_id == "imputeoor.classif.ranger" & mes_egemaps_gender_study1$task_id  == "egemaps_gender_study1")]$classif.acc[1]*100, 2)`%) indicating good data quality. Future research could investigate additional data-driven approaches to check speech data quality directly on the device. Finally, in future work, smartphones could be used to log and pre-process participants' everyday speech by using pre-trained language models to extract content features (e.g., specific topics or word embeddings) directly on the device, too. Thereby, no raw speech data would have to be transferred to a server and valuable information of language's content could be also used for privacy-respectful affect recognition.

<!-- limitation 3: everyday affect in this sample vs. extreme cases in prior research, future studies could use this approach for clinical studies -->
Furthermore, the data in this work is comprised of in-situ self-reports of affect experience from participants' everyday life in a non-clinical population. As a result, the data represents the "normal" everyday *moods* of regular people with only few cases of extreme affect experience. As a result, the trained affect recognition models should be considered in this context. If the conducted analyses were to be replicated in a clinical sample that contains more cases of extreme affect experience, prediction performances might be even higher. This does not necessarily mean, however, that findings and prediction models from the present work generalize well to a clinical sample. Additionally, future studies could aim to collect language samples when participants are known to experience strong emotions, for example based on their physiological signals [@hoemannContextawareExperienceSampling2020a]. 

<!-- measurement error in self reports, particularly for single-item measures! + future outlook -->
Finally, the ground truth, i.e., the information that is assumed to be fundamentally true, used for model training and evaluation are self-reports on participants' subjective affect experience that are prone to response biases [@gaoInvestigatingReliabilitySelfreport2021]. The introduced measurement error can have an impact on predictive modeling [@jacobucciMachineLearningPsychological2020]. Moreover, we employed single items to assess momentary affect experience on different dimensions to as we wanted to assess multiple other psychological constructs in the studies. This is an established approach to reduce participant burden, but can introduce additional measurement error [@dejonckheereAssessingReliabilitySingleitem2022]. Therefore, future studies that are particularly interested in subjective affect experience should use multiple items assessing a broad range of affect experience in an intensive longitudinal design. 

Moreover, there is a debate on how much of an underlying psychological construct, i.e., of subjective affect experience, scientists can assess using questionnaires: In order to assess one's affect experience most accurately through a survey item, one needs to have introspection to recognize their current affective state and have an adequate understanding of how to translate it to a numerical value on a scale [@montagWeStillNeed2022; @boydPersonalityPanoramaConceptualizing2020]. However, people can vary greatly in that regard [@critchleyInteroceptionEmotion2017]. Possibly, in the future, algorithms can replace self-report questionnaires altogether by analyzing natural language directly since transformers (that use word embeddings as employed in study 2) have been reported to converge with rating scales with an accuracy that approach the theoretical upper limits already [@kjellNaturalLanguageAnalyzed2022].

# Conclusion

<!-- summary of discussion: predictions for experience are worse than for expression, content superior to form, semi-free prompt better than fixed content, smartphones are awesome to collect data! -->
In this work, we investigated if machine learning algorithms can recognize subjective affect experience from speech samples collected in the wild using smartphones. Extracted acoustic voice parameters provided limited predictive information of affective arousal across both studies, while speech content as reflected in state-of-the-art word embeddings had been shown to be predictive of arousal as well emotional valence. Further, experimental and explorative findings suggest that emotional speech content did not affect predictions from voice cues alone (i.e., what someone talked about did not affect how well emotions could be predicted from voice cues). Our findings challenge the transferability of the overly optimistic prediction results from prior research work and commercial emotion-detection AI algorithms on the recognition of affect *expression* (e.g., enacted and labelled speech) to the recognition of subjective affect *experience* in everyday speech. The ability to discern how people truly feel, rather than just observing what they outwardly express, is vital for a deeper understanding of human emotions and for developing technologies that better respond to our emotional needs.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
