---
title             : "AI-based recognition of momentary subjective affect experience from natural speech"
shorttitle        : "Affect Experience in Speech"

author: 
  - name          : "Timo K. Koch"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "timo.koch@unisg.ch"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Formal Analysis"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Gabriella M. Harari"
    affiliation   : "3"
    role:
      - "Writing - Review & Editing"
      - "Supervision"
  - name          : "Ramona Schoedel"
    affiliation   : "2"
    role:
      - "Conceptualization"
      - "Writing - Review & Editing"
  - name          : "Zachariah Marrero"
    affiliation   : "4"
    role:
      - "Data curation"
  - name          : "Florian Bemmann"
    affiliation   : "5"
    role:
      - "Software"
  - name          : "Samuel D. Gosling"
    affiliation   : "4"
    role:
      - "Resources"
  - name          : "Markus Buehner"
    affiliation   : "2"
    role:
      - "Resources"
  - name          : "Clemens Stachl"
    affiliation   : "1"
    role:
      - "Writing - Review & Editing"
      - "Supervision"  

affiliation:
  - id            : "1"
    institution   : "Institute of Behavioral Science and Technology, University of St. Gallen"
  - id            : "2"
    institution   : "Department of Psychology, Ludwig-Maximilians-Universität München" 
  - id            : "3"
    institution   : "Stanford University"
  - id            : "4"
    institution   : "University of Texas"
  - id            : "5"
    institution   : "Media Informatics Group, Ludwig-Maximilians-Universität München"

authornote: |
  Conflict of interest: none
  Acknowledgements: We thank Peter Ehrich and Dominik Heinrich for their support with the technical implementation of the on-device voice feature extraction and the ZPID for the support with data collection for study 1. Thank you Sumer Vaid for the technical support with the set up of the environment for the analyses of study 2. This project was supported by the Swiss National Science Foundation (SNSF) under project number 215303 and a scholarship of the German Academic Scholarship Foundation awarded to the first author.
  All materials are available in the project’s repository on the Open Science Framework (OSF): https://osf.io/a5db8/

abstract: |
  Advances in the area of artificial intelligence (AI) and the ubiquity of speech data, for example coming from voice assistants, have created numerous commercial products that claim to be able to automatically recognize emotions from human speech. However, the employed algorithms have often been trained solely on enacted or labelled speech samples from artificial lab settings representing affect *expression* and are used to infer everyday subjective affect *experience*. In the present work, we investigate if machine learning algorithms can truly recognize subjective affect experience from speech samples collected in the wild. In two studies, we extract acoustic voice parameters and state-of-the-art word embeddings from 23632 speech samples with corresponding experience-sampled self-reports on momentary affect experience from 1066 participants collected using off-the-shelf smartphones. While voice acoustics provide limited predictive information of affective arousal, speech content is predictive of arousal as well as valence (sadness and contentedness). Further, experimental and explorative findings suggest that emotional speech content does not affect predictions from voice acoustics (i.e., what someone talks about does not affect how well emotions can be predicted from voice cues alone). We discuss implications for the algorithmic monitoring of affect experience from speech in everyday life.
  
keywords          : "Affect, Speech, Voice, Machine Learning, Mobile Sensing"
wordcount         : "5464"

bibliography      : ["../speech_paper_library.bib"]

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

documentclass     : "apa6"
classoption       : "man"
output: 
  papaja::apa6_pdf:
    figsintext: yes

---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```


## Introduction

<!-- One catchy intro paragraph (Not history!) boom and opportunities / promises of affect recognition research, commercial tools and applications - relevance and link to study - especially focus on excitement around voice! -->
Research findings on the algorithmic recognition of affective states (e.g., emotions) and related affective disorders from speech offer promising applications, for instance in health care, human-machine interaction, education, and marketing [@millingSpeechNewBlood2022; @hildebrandVoiceAnalyticsBusiness2020]. The advances in algorithmic affect recognition from speech leveraging AI and the ubiquity of available speech data due to the rise of voice assistants, for example Amazon's Alexa and Apple's Siri, have created an increasing commercial interest in the field. Here, tech companies aim to leverage speech data to, for instance, recognize what momentary affect their customers experience in order to develop personalized user interfaces or make product recommendations [@mandellSpotifyPatentsVoice2020; @knightAmazonWorkingMaking2016; @vlahosTalkMeHow2019]. Most of the prediction models used in research and in the corresponding commercial tools are trained on enacted or labelled speech samples from artificial lab settings that represent emotion *expressions*. However, those algorithms are often deployed to detect people’s subjective affect *experience* in everyday life. Further, many of the commercial algorithms are not transparent with regard to how well their predictions work and how (biased) predictions are being made. These issues raise questions regarding the promises of emotion-detecting speech technology and the protection of user privacy in setting where speech data can be analyzed, for example, when using voice assistants. The present work investigates the algorithmic recognition of subjective self-reported momentary affect *experience* from speech samples collected with smartphones.

### Predicting affect expressions from speech

<!-- voice cues AND speech content; Studies on affect expression, very good performance reported! -->
Researchers have successfully predicted affective states from a range of speech data, such as labelled TV clips [@grimmVeraAmMittag2008], phone calls, and enacted speech samples from the lab [@banzigerIntroducingGenevaMultimodal2012; @burkhardtDatabaseGermanEmotional2005a; @schullerSpeechEmotionRecognition2018; @vogtAutomaticRecognitionEmotions2008]. They report on impressive prediction performances for the automatic recognition of emotions (i.e., correlations between true scores and predicted scores of up to .81 for arousal and .68 for valence predictions) [@weningerAcousticsEmotionAudio2013]. However, one has to keep in mind that in those works the enacted target emotion or the rater labels serve as ground truth. Thereby, these works predict affect *expression*, which is considered easier to algorithmically recognize than real-life *experienced* affect [@vogtAutomaticRecognitionEmotions2008]. There is a fundamental difference between what affect we express and what affect we actually experience, even though there is an overlap to a varying degree. Affect *expression* represents the emotional expressive behavior based on our internal affect *experience*. However, "feeling is not always revealing", i.e., one does not necessarily express what one experiences affectively or might even express something completely different [@grossRevealingFeelingsFacets1997; @grossDissociationEmotionExpression2000]. Furthermore, the way one expresses affect might be perceived and interpreted differently. Figure \@ref(fig:experiencevsexpression) depicts this distinction. Moreover, the prediction performance varies greatly across studies due to a varying choice of emotion targets (i.e., discrete emotion versus core affect), conceptualizations of affect (e.g., short-termed elicited emotions versus moods), and employed algorithms (e.g., supervised versus unsupervised machine learning). 

```{r experiencevsexpression, echo = FALSE, warning=FALSE, fig.cap="The difference of subjective affect experience and observable affect expression.", fig.scap= "Feature importance", out.width="75%", fig.align="center", results="asis"}

knitr::include_graphics(path = "../figures/experience_vs_expression.png")
```

<!-- Back box models, Affect-Linked Speech Cues (voice and spoken content) - affect association -->
Also, these prior studies on algorithmic affect recognition often offer no insights into how predictions in their "black box" models were being made. For instance, it frequently remains unclear which specific speech characteristics are particularly predictive of a given affective state. Prior descriptive research reported on associations of specific acoustic features and affective states. For example, voice pitch and intensity were found to be associated with affective arousal [@vogtAutomaticRecognitionEmotions2008; @weningerAcousticsEmotionAudio2013]. Two recent studies provide a remarkable non-technical summary of voice features [@hildebrandVoiceAnalyticsBusiness2020] and a comprehensive overview of associations of word use with affect in spoken language [@sunLanguageWellbeingTracking2020]. Recent developments in the area of interpretable machine learning can help gain insights into the inner working of machine learning algorithms and, consequently, aid with detecting speech features that are especially predictive of affective states [@molnarInterpretableMachineLearning2019].

<!-- Ambiguity in the prediction target for labelled data (ground truth!) + getting (sufficient) corresponding speech data (predictor side) + how smartphones can help -->
Due to the challenge of obtaining speech data with corresponding affect labels in-vivo, most prior research on affect recognition from speech has used actors or labelled samples. This comes with a set of downsides, such as actors potentially overacting and the ambiguity of ground truth due to the subjective nature of labeling [@batlinerAutomaticRecognitionEmotions2011; @schullerSpeechEmotionRecognition2018; @wiltingRealVsActed2006]. As a consequence, studies investigating predictions of subjective affect experience from speech are rare. Recent works have collected everyday speech samples using the Electronically Activated Recorder (EAR) [@mehlElectronicallyActivatedRecorder2017]. Hereby, speech data can be collected over a period of time which allows researchers to not only investigate between-person differences in affect (i.e., is this person sad?), but also assess within-person fluctuations (i.e., is this person sadder than other days?) [@huangPredictionEmotionChange2018; @weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020]. Using the EAR, however, can be privacy invasive since potentially non-consenting persons may be recorded, too. Moreover, handling the EAR recorders and transmitting the collected data can be tedious for participants and researchers. 
Here, off-the-shelf smartphones represent a useful platform to collect experience samples on momentary affect experience over time and make corresponding speech records using the build-in microphone [@weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020; @carlierSearchStateTrait2022].

### Content-form interactions in affect recognition from speech

<!-- considering the interplay of speech from and content: WHAT people talk about and how it impacts acoustics, comparing phonetic and semantic predictions, how semantic content affects acoustics, how this could be investigated using smartphones -->
Prior research has shown that voice form (prosody) and the lexical content of the produced words (semantics) work together when transmitting affective information through speech [@ben-davidProsodySemanticsAre2016]. Moreover, studies suggest that there is a prosodic (i.e., from voice cues) dominance in the perception of affect based on lab experiments [@ben-davidProsodySemanticsAre2016; @linProsodyDominatesSemantics2020], but not (yet) using speech data from the wild [@schwartzEmotionalSpeechProcessing2012]. Moreover, while this research field had focused on the interplay of prosody and semantics in the recognition of affect by human raters, there are, to our knowledge, no studies on content-form interactions in algorithmic affect detection. Hence, it is currently unclear if what users talk about (i.e., the emotional content) has an effect on voice acoustics that impact automated affect recognition. In an applied setting, for example, the question is if an algorithm could recognize affective states regardless of what the person talks about, may it be a mundane topic, such as the weather or ordering pizza, or does one need to talk about an emotional topic (e.g., meeting a loved one). 

<!-- conclusion of intro & theory, wrap up, lead to this study, present tense! -->
The present work leverages methodological advances in the area of smartphone-based data collection methods to investigate the prediction of subjective momentary affect experience from speech. In two large-scale studies, we train cross-validated machine learning models on acoustic voice cues and state-of-the-art word embeddings from speech samples collected in the wild. Moreover, for predictive models, we investigate which voice cues are most predictive. Further, we experimentally and exploratively investigate the effects of the emotionality of speech's content on algorithmic affect recognition from voice acoustics. Thereby, we aim to inform potential applications and promises in automatic affect recognition from speech signals and advise the discussion on the protection of user privacy rights.

## Method

### Study 1: Data collection

<!-- Study data collection description + ESM -->
Data collection for study 1 was part of a large six-month panel study (from May until November 2020) using the *PhoneStudy* research app at Ludwig-Maximilian-Universität München [@schoedelBasicProtocolSmartphone2020]. Data collection was approved by the responsible IRB board. The study comprised two two-week experience sampling phases (July 27, 2020, to August 9,2020; September 21, 2020, to October 4, 2020) during which participants received two to four short questionnaires per day. Here, self-reported valence and arousal were assessed in two separate items on six-point Likert scales among other psychological properties as part of an experience sampling procedure.

<!-- voice: on device acoustic feature extraction -->
The last experience sampling questionnaire of each day included an additional instruction, where participants were asked to read out a series of predefined emotional sentences while making an audio recording of their voice. The sentences presented to the participants are based on a set of validated German neutral and affective sentences [@defrenEmotionalSpeechPerception2018] and differ in their emotional content: positive (e.g., "My team won yesterday."), negative (e.g., "Nobody is interested in my life."), and neutral (e.g., "The plate is on the round table."). These three emotional categories are presented consecutively in each audio logging task. The order of the categories was randomized per experience sampling questionnaire. For each emotional content category, three sentences were randomly drawn from respective sets of sentences in the database. The use and experimental manipulation of these emotional semantic categories allowed us to control for the content spoken by our participants and at the same time enabled us to conduct a privacy-friendly study. The audio recording was started by the participants via a button on the screen. Participants could stop the recording manually after a minimum of four seconds. Alternatively, the recording was stopped automatically after twelve seconds. We chose these lower- and upper-time thresholds because this is the minimum and maximum time required to read out the three sentences per condition.
Once the audio record had been completed, we used the widely adopted *OpenSMILE* open-source algorithm [@eybenOpensmileMunichVersatile2010] to automatically extract acoustic features directly on the participant’s device. Here, we used the extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPS) that is comprised of 88 acoustic features [@eybenGenevaMinimalisticAcoustic2016]. Theses voice feature have been used in a range of prior studies on affect recognition from speech. After feature extraction, the voice records were automatically deleted and only extracted voice features were stored on our servers.

```{r ger sample data, echo = FALSE, warning = FALSE, results = FALSE, message = FALSE}

library("dplyr")

## load data

# all voice samples
# egemaps_features_all  <- readRDS("../study1_ger/data/egemaps_features_all.RData") 

# all voice samples after voice quality check
egemaps_features  <- readRDS("../data/study1/egemaps_features.RData") 

# final merged data set of voice data with esm data
affect_egemaps  <- readRDS("../data/study1/affect_egemaps.RData") #load data

# show distribution of sentence conditions
# table(affect_egemaps$condition)

# count number of audio samples per experience sampling instance
audio_per_es <- affect_egemaps %>%
  group_by(e_s_questionnaire_id) %>%
  count() 

# table(audio_per_es$n) # there are three audio records for most es instances

# show valence and arousal statistics per condition - no differences!
affect_egemaps %>%
  group_by(condition) %>%
  dplyr::summarize(mean_valence = mean(valence, na.rm=TRUE), mean_arousal = mean(arousal, na.rm=TRUE))

```

<!-- describe the sample, raw sample and final sample  -->
With this procedure, we collected 11,199 audio logs from 586 participants. We excluded 214 voice logs because the respective acoustic features (mean voicing score, voiced segments per second, mean voiced segment length) indicated that no human voice was recorded. Moreover, we excluded a total of 997 samples without corresponding self-reports on valence and arousal, from participants with less than ten experience sampling instances, and those participants who had no variance in all their valence and arousal scores across all their experience samples.
This left us with a final data set of `r nrow(affect_egemaps)` voice samples with corresponding acoustic features from `r length(unique(affect_egemaps$e_s_questionnaire_id))` experience sampling instances for valence and arousal from `r length(unique(affect_egemaps$user_id))` participants (`r round((table(affect_egemaps$Demo_GE)[2] / sum(table(affect_egemaps$Demo_GE)))*100,1)`% female, *M*(Age) = `r round(mean(affect_egemaps$Demo_A1, na.rm = T),2)` years). Overall self-reported valence was positive (*M* = `r round(mean(affect_egemaps$valence),2)`, *SD* = `r round(sd(affect_egemaps$valence),2)`) and overall arousal was slightly geared towards activity (*M* = `r round(mean(affect_egemaps$arousal),2)`, *SD* = `r round(sd(affect_egemaps$arousal),2)`). The distribution of valence and arousal is provided in the online supplements. 

In the final sample, voice samples were not equally distributed across emotional sentence conditions (*chi*^2^(2) = `r round(chisq.test(table(affect_egemaps$condition))$statistic,2)`, *p* < .001): `r table(affect_egemaps$condition)[[3]]` came from the positive condition, `r table(affect_egemaps$condition)[[2]]` from the neutral condition, and `r table(affect_egemaps$condition)[[1]]` from the negative condition.

### Study 2: Data collection

<!-- US Study data collection description + ESM -->
Data collection for study 2 was part of the UT1000 Project at the University of Texas at Austin in the United States in fall 2018 [@wuMultimodalDataCollection2021]. During a three-week self-tracking assignment using their smartphones, students from a Synchronous Massive Online Course (SMOC) in introductory psychology received four short experience sampling questionnaires per day where they could also make records of their speech at the end. Here, self-reported arousal (assessed on a five-point Likert scale), contentedness, and sadness were assessed in separate items on four-point Likert scales among other psychological properties as part of an experience sampling procedure. Thereby, in Study 2, we captured emotional valence on two items (contentedness and sadness) instead of one as done in Study 1. According to the affect grid, contentedness and sadness have a comparable low level of arousal and an opposing emotional valence.  
For the audio records, participants received the following instruction: "Please record a short audio clip in which you describe the situation you are in, and your current thoughts and feelings. Collect about 10 seconds of environmental sound after the description.” The responses to this prompt are analyzed in the present study. Any parts of the record that did not contain speech were cut out before further analysis since the focus of this work is affect in human speech. The collected speech samples had also been used in another research project that describes the data collection procedure in more detail [@marreroEvaluatingVoiceSamples2022].

```{r us sample data, include = FALSE}

library("dplyr")

## load data

# # all voice samples
# egemaps_features_all  <- readRDS("../study2_us/data/egemaps_features_all.RData")
# 
# # all voice samples after voice quality check
# egemaps_features  <- readRDS("../study2_us/data/egemaps_features.RData")
# 
# final merged data set of voice data with esm data
affect_egemaps_wordembeddings  <- readRDS("../data/study2/affect_acoustics_wordembeddings.RData") #load data

```

<!--  initial sample, preprocessing -->
With this procedure, we collected 23,482 audio logs from 980 participants. We followed the same procedure to select speech records as in Study 1 and, to ensure comparability of the two studies with regard to the length of speech samples, we retained all speech transcripts that contained at least 15 words and were more than four seconds long which is equivalent to the length of the sentences that had been read out in Study 1: We removed records where the respective acoustic features indicated that no human voice was recorded. Moreover, we excluded audio samples without corresponding affect self-reports, participants with less than ten experience sampling instances, and those participants who had no variance in all their self-reports across all their experience samples.

<!-- final sample description - study 2 -->
This procedure left us with a final data set of `r nrow(affect_egemaps_wordembeddings)` speech samples with corresponding experience-sampled self-reports on momentary affect experience from `r length(unique(affect_egemaps_wordembeddings$user_id))` participants (`r round((table(affect_egemaps_wordembeddings$Gender)[1] / sum(table(affect_egemaps_wordembeddings$Gender)))*100,1)`% female, *M*(Age) = `r round(mean(affect_egemaps_wordembeddings$Age, na.rm = T),2)` years). Overall participants reported balanced experienced contentedness (*M* = `r round(mean(affect_egemaps_wordembeddings$content),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings$content),2)`) and low sadness (*M* = `r round(mean(affect_egemaps_wordembeddings$sad),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings$sad),2)`). Overall arousal was balanced out (*M* = `r round(mean(affect_egemaps_wordembeddings$arousal),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings$arousal),2)`). The distribution of arousal, contentedness, and sadness is provided in the online supplements. 

<!-- acoustic feature extraction -->
In the same manner as in Study 1, we extracted the extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPS) from the collected audio files using the OpenSMILE algorithm [@eybenOpensmileMunichVersatile2010; @eybenGenevaMinimalisticAcoustic2016]. In Study 2, those features were extracted from the raw recorded audio files after data collection and not directly on participants' smartphones as in Study 1.

<!-- word embeddings -->
We transcribed all raw audio records using the Google Speech-to-text API. Then, we extracted state-of-the-art word embeddings from speech transcripts using the *text* R package [@kjellTextRpackageAnalyzing2021]. Word embeddings are vector representations of words in a high-dimensional space, which capture their contextualized meaning and relationships with other words. Specifically, for predictive modeling, we used the second to last layer (layer 23) from the language model "RoBERTa large" as recommended in prior work [@liuRoBERTaRobustlyOptimized2019; @materoEvaluatingContextualEmbeddings2022].

#### Predictive modelling

<!-- general ml -->
In both studies, we trained two supervised machine learning models on the extracted features for the prediction of self-reported valence and arousal. Here, we compared the predictive performance of linear regularized regression models (LASSO)  [@zouRegularizationVariableSelection2005] with those of a non-linear tree-based Random Forest model [@breimanRandomForests2001; @wrightRangerFastImplementation2017], and a baseline model. The baseline model would predict the respective mean values for valence and arousal of the respective training set for all cases in a test set. Additionally, we included the prediction of participants' gender as a benchmark. Models were evaluated using a ten-fold cross-validation scheme [@bischlResamplingMethodsMetaModel2012]. We blocked participants in the resampling procedure ensuring that for each train/test set pair the given participant is either in the training set or in the test set.  

<!-- Model evaluation -->
We evaluated the predictive performance of the models based on the coefficient of determination (*R*^2^) and Spearman's (rank) correlation (*r*) between the predicted scores and participants’ self-reported scores. To determine whether a model was predictive beyond chance (*alpha* = 0.05), we carried out variance-corrected (one-sided) t-tests comparing the *R*^2^ measures of all prediction models with those of the baseline models [@nadeauInferenceGeneralizationError2003]. We adjusted for multiple comparison via Holm correction. 

<!-- Software -->
All data processing and statistical analyses in this work were performed with the statistical software R version 4.1.1 [@rcoreteamLanguageEnvironmentStatistical2021]. For machine learning, we used the *mlr3* framework [@langMlr3ModernObjectoriented2019]. Specifically, we used the *glmnet* [@friedmanRegularizationPathsGeneralized2010] and *ranger* [@wrightRangerFastImplementation2017] packages to fit prediction models. To quantify the impact of single predictors in Random Forest prediction models, we computed (out-of-bag) permutation feature importance using the *DALEX* package [@wrightLittleInteractionsGet2016; @biecekDALEXExplainersComplex2018]. We preregistered study 1 as a transparent account of our work [@kochPredictingAffectiveStates2021].

### Results

#### Voice contains only little predictive informaion of affect experience, but speech content does

```{r ger predictions, echo = FALSE, warning = FALSE}

library(mlr3)

#load benchmark results
bmr_egemaps_gender_study1 <- readRDS("../results/study1/bmr_egemaps_gender_study1.RData") 

bmr_egemaps_study1 <- readRDS("../results/study1/bmr_egemaps_study1.rds") 

mes = mlr3::msrs(c( "regr.rsq", "regr.srho")) # set performance measures

# get aggregated performance measures
mes_egemaps_gender_study1 <- bmr_egemaps_gender_study1$aggregate(msrs(c("classif.acc", "classif.auc")))

mes_egemaps_study1 <- bmr_egemaps_study1$aggregate(mes)

```

```{r us predictions, echo = FALSE, warning = FALSE}

library(mlr3)

#load benchmark results 
bmr_gender_study2 <- readRDS("../results/study2/bmr_gender_study2.RData")

bmr_egemaps_study2 <- readRDS("../results/study2/bmr_egemaps_study2.rds")

bmr_wordembeddings_study2 <- readRDS("../results/study2/bmr_wordembeddings_study2.rds")

mes = mlr3::msrs(c( "regr.rsq", "regr.srho")) # set performance measures

# get aggregated performance measures
mes_gender_study2 <- bmr_gender_study2$aggregate(msrs(c("classif.acc", "classif.auc")))
mes_egemaps_study2 <- bmr_egemaps_study2$aggregate(mes)
mes_wordembeddings_study2 <- bmr_wordembeddings_study2$aggregate(mes)

```

<!-- study 1: prediction from voice cues -->
Figure \@ref(fig:predictionoverview) provides an overview of the performance of all learners across prediction tasks while in this section we report on the best performing algorithm respectively (either Random Forest or LASSO). Overall, none of the employed machine learning algorithms predicted affect experience significantly better than chance from voice cues. On average, predictions of arousal (*R*^2^ = `r round(mes_egemaps[which(mes_egemaps$learner_id == "regr.ranger" & mes_egemaps$task_id  == "egemaps_arousal")]$regr.rsq,2)`, *r* = `r round(mes_egemaps[which(mes_egemaps$learner_id == "regr.ranger" & mes_egemaps$task_id  == "egemaps_arousal")]$regr.srho, 2)`) from Random Forest models were on average slightly better than the baseline models' predictions. 

<!-- study 2: prediction from voice cues -->
Also, for free speech, the employed machine learning models trained on voice acoustics did not predict the subjective experience of momentary affect experience significantly better than chance. 
Prediction models trained on voice acoustics alone were not significantly better than chance. However, on average, predictions of contentedness (*R*^2^ = `r round(mes[which(mes$learner_id == "regr.ranger" & mes$task_id  == "egemaps_content")]$regr.rsq, 2)`, *r* = `r round(mes[which(mes$learner_id == "regr.ranger" & mes$task_id  == "egemaps_content")]$regr.srho, 2)`) and arousal (*R*^2^ = `r round(mes[which(mes$learner_id == "regr.ranger" & mes$task_id  == "egemaps_arousal")]$regr.rsq, 2)`, *r* = `r round(mes[which(mes$learner_id == "regr.ranger" & mes$task_id  == "egemaps_arousal")]$regr.srho, 2)`). Our models yielded the best prediction performance for contentedness (*R*^2^ = `r round(mes[which(mes$learner_id == "regr.cv_glmnet" & mes$task_id  == "egemaps_wordembeddings_content")]$regr.rsq, 2)`, *r* = `r round(mes[which(mes$learner_id == "regr.cv_glmnet" & mes$task_id  == "egemaps_wordembeddings_content")]$regr.srho, 2)`), arousal (*R*^2^ = `r round(mes[which(mes$learner_id == "regr.cv_glmnet" & mes$task_id  == "egemaps_wordembeddings_arousal")]$regr.rsq, 2)`, *r* = `r round(mes[which(mes$learner_id == "regr.cv_glmnet" & mes$task_id  == "egemaps_wordembeddings_arousal")]$regr.srho, 2)`), and sadness (*R*^2^ = `r round(mes[which(mes$learner_id == "regr.cv_glmnet" & mes$task_id  == "egemaps_wordembeddings_sad")]$regr.rsq, 2)`, *r* = `r round(mes[which(mes$learner_id == "regr.cv_glmnet" & mes$task_id  == "egemaps_wordembeddings_sad")]$regr.srho, 2)`).

<!-- study 2: word embeddings and combined voice + embeddings -->
On the contrary, prediction models trained on word embeddings were significantly predictive of arousal (*R*^2^ = `r round(mes[which(mes$learner_id == "regr.cv_glmnet" & mes$task_id  == "wordembeddings_arousal")]$regr.rsq, 2)`, *r* = `r round(mes[which(mes$learner_id == "regr.cv_glmnet" & mes$task_id  == "wordembeddings_arousal")]$regr.srho, 2)`), contentedness (*R*^2^ = `r round(mes[which(mes$learner_id == "regr.cv_glmnet" & mes$task_id  == "wordembeddings_content")]$regr.rsq, 2)`, *r* = `r round(mes[which(mes$learner_id == "regr.cv_glmnet" & mes$task_id  == "wordembeddings_content")]$regr.srho, 2)`), and sadness (*R*^2^ = `r round(mes[which(mes$learner_id == "regr.cv_glmnet" & mes$task_id  == "wordembeddings_sad")]$regr.rsq, 2)`, *r* = `r round(mes[which(mes$learner_id == "regr.cv_glmnet" & mes$task_id  == "wordembeddings_sad")]$regr.srho, 2)`). Finally, models trained only on voice acoustics or word embedding respectively revealed that predictions were mostly driven by the information coming from speech content as represented in the word embeddings. 

<!-- some words about algo performance - LASSO vs RF -->
For voice acoustics, the Random Forest algorithm performed slightly better than the LASSO algorithm, suggesting non-linear relationships between voice cues and affect experience. On the contrary, for word embeddings, LASSO models performed better than Random Forest models, indicating linear predictor-outcome relationships between speech content as captured with word embeddings and momentary affect experience.

```{r predictionoverview, echo = FALSE, warning=FALSE, fig.cap="Box and whisker plot of prediction performance measures from 10-fold cross-validation for prediction models.", fig.scap= "Prediction performance", out.width="100%", out.height="75%", fig.align="center", results="asis"}

knitr::include_graphics(path = "../figures/bmr_plot.png")
```

#### Loudness and spectral flux are the most informative affective voice cues

<!-- add one combined figure of predictive arousal features for both studies here -->
<!-- add grouped feature importance plot here ?? does this tell us much? -->

Overall, features related to spectral flux (i.e., how quickly the power spectrum of the voice signal is changing) and loudness were most important. This observation is in line with descriptive correlations of voice features and self-reported affect experience (see figures \@ref(fig:gercorrelations1) and \@ref(fig:gercorrelations2) in the online appendix, where spectral flux and loudness features also had the highest (Spearman) correlation coefficients. Together, these findings indicate that a louder voice that has a quickly changing spectrum is indicative of the experience of heightened arousal.

Figure \@ref(fig:featureimportance) shows the five most important features in the Random Forest model (based on permutation feature importance) for the prediction of arousal from acoustic voice features. We refrained from reporting feature importance scores for word embeddings as they are not as clearly interpretable as voice features. Overall, features related to loudness of the voice were most important from arousal predictions. Even though features describing spectral flux (i.e., how quickly the power spectrum of the voice signal changes) showed high correlations with self-reported affect experience (see online appendix), other voice features related to the voice spectrum (mean of the mel-frequency cepstral coefficient and bandwidth of the first formant) made it into the top five most important features in the Random Forest model. Again, in line with results from study 1, these findings indicate that a louder voice that has a quickly changing broad spectrum is indicative of heightened experienced arousal.

<!-- give some details on predictive word embeddings in study 2 ?? very student specific! -->

<!-- show table instead of figure for most important voice features for arousal (study 1 & 2) and contentedness (study2) -->

```{r, echo=FALSE, message=FALSE}
library(knitr)
library(kableExtra)

# Your table data
table_data <- data.frame(
  Target = c("Target 1", "Target 2", "Target 3"),
  Study = c("Study A", "Study B", "Study C"),
  `Top Five Features` = c("Feature1, Feature2, Feature3, Feature4, Feature5",
                          "Feature1, Feature2, Feature3, Feature4, Feature5",
                          "Feature1, Feature2, Feature3, Feature4, Feature5")
)

# Creating the table
kable(table_data, "latex", booktabs = TRUE, caption = "Your Table Title") %>%
  kable_styling(latex_options = c("striped", "scale_down"))


```

#### Speech content has no effect arousal predictions from voice acoustics

```{r sentenceanova, echo = FALSE, warning = FALSE, results = FALSE, message = FALSE}

library(car)

predictions_condition <- readRDS(file="../results/study1/predictions_condition.rds")

# run anova comparing means for valence 
aov_valence <- car::Anova(aov(error_valence ~ condition, data = predictions_condition))

# run anova comparing means for arousal
aov_arousal <- car::Anova(aov(error_arousal ~ condition, data = predictions_condition))

```


```{r sentimentcorrs, echo = FALSE, warning = FALSE, results = FALSE, message = FALSE}

predictions_condition <- readRDS(file="../results/study2/predictions_condition.rds")

```

<!-- study1: is absolute error different across conditions -->
In study 1, we analyzed if the experimentally altered emotional content (positive/ neutral/ negative) of the predefined sentences that had been read out by participants had an effect on affect predictions from voice acoustics. There were no significant differences in prediction errors across the three sentence conditions for valence (*F*(2,11873) = `r round(aov_valence[3][[1]][1],2)`,
*p* > .05) and arousal (*F*(2,11873) = `r round(aov_arousal[3][[1]][1],2)`,
*p* > .05) predictions suggesting that sentences' emotional valence did not influence affect predictions from voice acoustics.

<!-- study 2: (no) Effects of sentiment on voice predictions, maybe (!) a little bit for contentedness - but keep in mind that predictions were not significant overall! -->
In study 2, in order to exploratively investigate the effect of the emotional valence of the spoken content on affect predictions from voice cues, we used the sentiment score (*M* =`r round(mean(affect_egemaps_wordembeddings$Sentiment.score, na.rm =T),2)`, *SD* = `r round(sd(affect_egemaps_wordembeddings$Sentiment.score, na.rm =T),2)`) within the interval of [-1; 1] that had been assigned to each speech transcript by the Google text-to-speech API. Here, we analyzed the correlation of content sentiment with the the absolute prediction errors in the prediction of self-reported momentary arousal, contentedness, and sadness from voice cues using the LASSO algorithm. Result indicate that content sentiment did not have an effect on affect predictions from voice cues: Correlations of content sentiment with absolute predictions errors was not significant for neither contentedness (*r* = XX, *p* > .05), sadness (*r* = XX, *p* > .05), nor arousal (*r* = XX, *p* > .05). When interpreting those results, one has to keep in mind that overall the predictive performance of the models trained on voice cues was generally limited. 

\newpage

## Discussion

<!-- mini summary of findings, use past tense here -->
In the present work, we extracted acoustic voice parameters and state-of-the-art word embeddings from speech samples collected using smartphones to predict subjective momentary affect experience. In contrast to prior work on the algorithmic recognition of affect expression, our findings suggest that voice cues provide only limited predictive information of subjective affect experience. Here, we identified loudness and features related to fluctuations of the voice spectrum to contain most affective information. Also, experimental (Study 1) and explorative (Study 2) findings suggest that emotional speech content did not affect predictions from voice acoustics (i.e., what someone talks about does not influence how well affect can be predicted from voice cues). Speech content, on the contrary, had been shown to be much more revealing of subjective affect experience.  

### Recognizing subjective momentary affect experience from speech is challenging 

<!-- comparing our results to prior findings: comparable performance to affect experience but lower performance than affect expression predictions; some works on how smartphones can help -->
Our results indicate that speech samples, and particularly their content, allow for the automatic prediction of subjective momentary affect experience. However, our machine learning models achieve a lower prediction performance as reported in prior work on automatic predictions of affect *expression* [@schullerSpeechEmotionRecognition2018]. Still, our reported performance is similar to studies predicting subjective affect *experience* from speech samples collected in the wild [@weidmanNotHearingHappiness2020; @carlierSearchStateTrait2022; @sunLanguageWellbeingTracking2020]. This observation is in line with prior research suggesting that real-life emotions are more difficult to algorithmically recognize than enacted or elicited emotions [@vogtAutomaticRecognitionEmotions2008]. Also, there are only few instances of extreme affect experiences in our data sets compared to the data used in prior studies on acted or labelled emotions. As a result, we rather predicted *mood* in this work, which is, by definition, less intense than emotions [@schererVocalCommunicationEmotion2003] and, consequently, more challenging to recognize. 

<!-- arousal predictions worked in both studies, indicating that arousal can be algorithmically detected more easily -->
Furthermore, across the two studies, arousal predictions from voice acoustics were better than those of emotional valence, highlighting prior work showing that the latter is more challenging to automatically infer due to its individual nature [@sridharUnsupervisedPersonalizationEmotion2022]. Moreover, in line with prior work [@weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020], we also ran supplementary analyses for the prediction of within-person fluctuations in affect experience. Overall, those yielded weaker predictions than the between-person differences (see online supplements). 

<!-- using larger acoustic feature set did not improve acoustic predictions, ie no issue with voice features -->
Further, as done in prior research on voice-affect predictions [@weidmanNotHearingHappiness2020], we also compared the prediction performance of machine learning models trained on the much larger Compare2016 (6,737 features) acoustic feature set [@schullerINTERSPEECH2016Computational2016] in contrast to the economic eGeMAPS (88 features) feature set we had used.  Just as in prior research, the larger voice feature set did not yield better affect predictions [@weidmanNotHearingHappiness2020]. Results are provided in the online supplements. This finding suggests that an economic acoustic feature set is sufficient for affect detection from voice. Moreover, the small features set is less computationally expensive and would allow for online or on-device pre-processing in a scientific or applied setting.

<!-- we question the TRANSFERABILITY of prior findings to subjective affect experience and NOT the findings themselves; implications from predictions for research and practice  -->
Generally, our findings challenge the transferability of the optimistic prediction results from prior research work on the recognition of affect *expression* (e.g., enacted speech) to the recognition of subjective affect *experience* in everyday speech, particularly from acoustic voice cues. Thereby, our findings also question the proclaimed performance of commercial affect recognition algorithms deployed in daily life that have been mostly trained on enacted or labelled affect *expression*. Consequently, current expectations regarding the performance of emotion-detecting AI services, especially the ones that are focused on voice cues, applied to everyday speech might be overoptimistic. More research is needed to determine how well algorithms can pick up on subjective affect experience from day-to-day speech.

<!-- the role of smartphones in future research -->
In future research, smartphone could play a prominent role in collecting and analyzing speech data and corresponding in-situ self-reports on subjective affect experience for affect inferences. Hereby, starting from our work, smartphones could be used as a mobile experimental lab to study different aspects of affect recognition from speech, for example by experimentally varying the content as done in Study 1 [@millerSmartphonePsychologyManifesto2012]. 

### The context of speech production matters

<!-- scripted sentences vs prompted speech had no effect on arousal predictions, valence was better in study 2, but might be due to specific target -->
Our results indicate that the context of speech production (i.e., reading out predefined emotional sentences versus prompted free speech) had an impact on algorithmic affect predictions. While the predefined sentences in Study 1 allowed us to control for the emotionality of the content of participants' voice records, they were unable to express themselves freely, which could have impaired predictions from voice acoustics compared to Study 2 where participants could talk freely. As a result, researchers and practitioners should consider the context in which speech had been produced in and keep in mind that findings and trained models might be specific to the given production context and do not necessarily generalize well to other production contexts. 

### The role of speech content 

<!--  content (word embeddings) way superior to voice cues (study 2) + what are implications? -->
In Study 2, state-of-the-art word embeddings showed a superior affect prediction performance compared to voice acoustics suggesting that speech content could contain more affective information than speech form. This finding is in line with prior research that found speech content to be more predictive than voice acoustics when predicting momentary subjective experience of happiness [@weidmanNotHearingHappiness2020; @sunLanguageWellbeingTracking2020]. As a consequence, even though prior research has suggested that voice acoustics could be more relevant for human affect inferences than speech content [@ben-davidProsodySemanticsAre2016; @linProsodyDominatesSemantics2020], future research and AI applications should consider both channels - content and form - simultaneously. 

<!-- Sentence condition (study 1) and sentiment score (study 2) -->
By experimentally varying the emotional valence of the spoken content in Study 1 and exploratively investigating the effect of word sentiment on voice predictions in Study 2, our findings suggest that the content about what participants talked about did not have a substantial impact on affect predictions from voice cues. This insight could imply that it does not matter what people talk about when algorithmically inferring affect experience from voice cues. However, one has to keep than in mind that affect predictions from voice cues were overall not very strong. More research is needed to disentangle speech content and form in automatic affect recognition.

## Limitations and future directions

<!-- limitation 1: operationalizations / sample composition / slightly different target variable and participants in the two samples  -->
The findings of this work are limited in four ways. First, we used slightly different operationalizations of affect experience and sample compositions in the two studies that might affect their comparability: In Study 1, we assessed valence and arousal on a six-point Likert scale. In Study 2, we used two items to assess affective valence (contentedness and sadness) and arousal on five-/four-point Likert scales. As a consequence, findings might not be directly comparable. Further, while Study 1 drew on a representative German sample, Study 2 was based on a student convenience sample from the United States with the respective limitations, such as potential constraints in generalizability of findings [@mullerDepressionPredictionsGPSbased2021]. Also, the German data set that is analyzed in study 1 only contained Android users, excluding those using iOS devices, because of the technical requirements of the logging software. However, past research suggests that the selection bias regarding participant demographics and personality for the German population is negligible [@schoedelSnapshotsDailyLife2022; @gotzUsersMainSmartphone2017; @keuschCoverageErrorData2020]. Moreover, both data sets have been collected in Western countries, specifically Germany and the United States. Prior research suggests that there are cultural differences in emotion experience [@limCulturalDifferencesEmotion2016] and that mood inference models from sensing data do not necessarily generalize to other countries and cultures [@meegahapolaGeneralizationPersonalizationMobile2023]. Future studies should investigate multiple target emotions in diverse international samples from different cultural contexts in non-western countries.

<!-- limitation 2: how we collected the speech data (still artificial records) and ESM data + we do not know for sure that proper records have been made in study1 -->
Second, in contrast to prior work using passive speech sensing (e.g., via the EAR), our participants had to actively log their speech in the present work. This artificial setting might have had an effect on results. Moreover, the findings of this study might be subject to the specific instructions that had been given for the audio records: In Study 1, participants were instructed to read out predefined sentences and, in Study 2, participants were prompted to talk about the situation they were in as well as their current thoughts and feelings in a semi-structured fashion. While affect-linked acoustic voice cues in the two studies are similar and are possibly transferable to new voice data, word embeddings are specific to the given task in Study 2. In this manner, future work should employ multiple different speech tasks for affect predictions and investigate how well predictions generalize from one to another.

Also, it should be noted that affective self-reports only from those situations when participants had their smartphone on them and felt comfortable to complete the experience sampling had been analyzed in the present dissertation. Moreover, study participants knew that their affect self-reports and corresponding language samples would be recorded and later analyzed. As a consequence, with regard to assessed subjective affect experience, participants might have only completed the experience sampling questionnaire in selected affective situations, for example not when they were experiencing extreme affective states, or they had not reported on extreme affect at all [@schoedelSnapshotsDailyLife2022]. Moreover, participants might have not spoken as naturally as they would if they had not known that their data would be scientifically analyzed. Participants might have made the audio records only in selected suitable situations, for example when they were alone in a quiet place. As a result, the resulting data set would also only contain participants' affect experience from being alone in quiet places.

Moreover, specifically to Study 1, another related limitation lies in our privacy-preserving on-device data pre-processing approach. By applying on-device feature extraction, we had no opportunity to check in detail if participants truly complied with study instructions and had recorded their voice while reading out the predefined sentences accordingly (beyond the data-driven quality checks we had applied). Further, our approach did not allow to control for records' background noise (e.g., when participants were outside next to a road) or how they held their smartphone during the voice record. Since checking single raw audio files manually would be out of scope, future research could investigate additional data-driven approaches to check speech data quality directly on the device. However, gender predictions yielded very good prediction results (prediction accuracy = `r round(mes_gender[which(mes_gender$learner_id == "classif.ranger" & mes_gender$task_id  == "egemaps_wordembeddings_gender")]$classif.acc[1]*100, 2)`%). These findings suggest that voice acoustics from the collected prompted voice samples in Study 1 contain valuable information about speaker demographics.Finally, in future work, smartphones could be used to log and immediately pre-process participants' everyday speech by using pre-trained language models to extract content features (e.g., specific topics or word embeddings) directly on the device, too. Thereby, no raw speech data would have to be transferred to a server and valuable information of language's content could be also used for privacy-respectful affect recognition.

<!-- limitation 3: everyday affect in this sample vs. extreme cases in prior research, future studies could use this approach for clinical studies -->
Third, the data is comprised of in-situ self-reports of affect experience from participants' everyday life in a non-clinical population. As a result, the data represents the "normal" everyday *moods* of regular people with only few cases of extreme positive or negative or very high or low aroused affect experience. As a result, the trained affect recognition models should be considered in this context. If the conducted analyses were to be replicated in a clinical sample that contains more cases of extreme affect experience, findings might be even more distinct. This does not necessarily mean, however, that findings and prediction models from the present dissertation generalize well to a clinical sample. Alternatively, future studies could aim to collect language samples when participants are known to experience strong emotions, for example based on their physiological signals [@hoemannContextawareExperienceSampling2020a]. 

<!-- measurement error in self reports, particularly for single-item measures! + future outlook -->
Fourth and finally, the ground truth, i.e., the information that is assumed to be fundamentally true, used for model training and evaluation are self-reports on participants' subjective affect experience. However, these are prone to response biases [@gaoInvestigatingReliabilitySelfreport2021]. These can introduce measurement error that can have a profound impact on the consecutive predictive modeling [@jacobucciMachineLearningPsychological2020]. Moreover, single items were employed to assess state affect experience on different dimensions (i.e., valence and arousal) in the empirical studies. This is an established approach to reduce participant burden by not having them fill out many lengthy experience sampling questionnaires that would also need to be compensated for. However, this single-item approach can introduce additional measurement error for affective responses [@dejonckheereAssessingReliabilitySingleitem2022]. Future studies that are particularly interested in subjective affect experience should use multiple items assessing a broad range of affect experience in an intensive longitudinal design. Beyond the psychometric challenges associated with using (single item) self-reports to assess momentary affect experience, there is a conceptual debate on how much of an underlying psychological construct, i.e., of affect experience, one can assess using questionnaires. In order to report one's affect experience most accurately through a survey item, one needs to have introspection to recognize it and have an adequate understanding to communicate it accordingly [@montagWeStillNeed2022; @boydPersonalityPanoramaConceptualizing2020]. However, people can vary greatly in that regard [@critchleyInteroceptionEmotion2017]. Possibly, in the future, algorithms can replace self-report questionnaires altogether by analyzing natural language directly since transformers (that use word embeddings as employed in study 2) have been reported to approach the upper limits in accuracy already [@kjellNaturalLanguageAnalyzed2022].

## Conclusion

<!-- summary of discussion: predictions for experience are worse than for expression, content superior to form, semi-free prompt better than fixed content, smartphones are awesome to collect data! -->
In this work, we investigated if machine learning algorithms can recognize subjective affect experience from speech samples collected in the wild using smartphones. Extracted acoustic voice parameters provided limited predictive information of affective arousal across both studies, while speech content as reflected in state-of-the-art word embeddings had been shown to be predictive of arousal as well valence (sadness and contentedness). Also, speech content showed superior prediction performance compared to voice acoustics. Further, experimental and explorative findings suggest that emotional speech content did not affect predictions from voice acoustics (i.e., what someone talked about did not affect how well emotions could be predicted from voice cues). Our findings challenge the transferability of the optimistic prediction results from prior research work and commercial emotion-detection AI algorithms on the recognition of affect *expression* (e.g., enacted and labelled speech) to the recognition of subjective affect *experience* in everyday speech. Finally, we discussed resulting implications for the algorithmic monitoring of affect experience.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
