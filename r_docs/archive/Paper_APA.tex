\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[english,man]{apa6}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Predicting Affective States from Acoustic Cues in Speech in the Wild},
            pdfauthor={Timo K. Koch1 \& Ramona Schoedel1},
            pdfkeywords={Affect, Emotion, Speech, Acoustics, Machine Learning},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\begin{center}\begin{threeparttable}}
%   {\end{threeparttable}\end{center}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\begin{center}\begin{ThreePartTable}}{\end{ThreePartTable}\end{center}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother
\shorttitle{Predicting Affect from Speech}
\keywords{Affect, Emotion, Speech, Acoustics, Machine Learning\newline\indent Word count: 5000}
\DeclareDelayedFloatFlavor{ThreePartTable}{table}
\DeclareDelayedFloatFlavor{lltable}{table}
\DeclareDelayedFloatFlavor*{longtable}{table}
\makeatletter
\renewcommand{\efloat@iwrite}[1]{\immediate\expandafter\protected@write\csname efloat@post#1\endcsname{}}
\makeatother
\usepackage{csquotes}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[shorthands=off,main=english]{babel}
\else
  % load polyglossia as late as possible as it *could* call bidi if RTL lang (e.g. Hebrew or Arabic)
  \usepackage{polyglossia}
  \setmainlanguage[]{english}
\fi

\title{Predicting Affective States from Acoustic Cues in Speech in the Wild}
\author{Timo K. Koch\textsuperscript{1} \& Ramona Schoedel\textsuperscript{1}}
\date{}


\authornote{

Timo K. Koch, Department of Psychology, Psychological Methodss and Assessment, Ludwig-Maximilians-Universität München, Leopoldstr. 13, 80802 Munich

Ramona Schoedel, Department of Psychology, Psychological Methodss and Assessment, Ludwig-Maximilians-Universität München, Leopoldstr. 13, 80802 Munich

The authors made the following contributions. Timo K. Koch: Conceptualization, Writing - Original Draft Preparation, Writing - Review \& Editing; Ramona Schoedel: Conceptualization, Writing - Review \& Editing.

Correspondence concerning this article should be addressed to Timo K. Koch, Ludwig-Maximilians-Universität München, Department of Psycholgy, Leopoldstr. 13, 80802 Munich. E-mail: \href{mailto:timo.koch@psy.lmu.de}{\nolinkurl{timo.koch@psy.lmu.de}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Ludwig-Maximilan-Universiät München}

\abstract{
One or two sentences providing a \textbf{basic introduction} to the field, comprehensible to a scientist in any discipline.

Two to three sentences of \textbf{more detailed background}, comprehensible to scientists in related disciplines.

One sentence clearly stating the \textbf{general problem} being addressed by this particular study.

One sentence summarizing the main result (with the words ``\textbf{here we show}'' or their equivalent).

Two or three sentences explaining what the \textbf{main result} reveals in direct comparison to what was thought to be the case previously, or how the main result adds to previous knowledge.

One or two sentences to put the results into a more \textbf{general context}.

Two or three sentences to provide a \textbf{broader perspective}, readily comprehensible to a scientist in any discipline.
}



\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Affect recognition is human skill. Growing effort to teach machines to do it across disciplines. Application in psychotherapy and huge privacy issues with all the passively collected speech data (Alexa, Siri etc.)

\hypertarget{affect-prediction-from-acoustic-properties-of-speech}{%
\subsection{Affect prediction from acoustic properties of speech}\label{affect-prediction-from-acoustic-properties-of-speech}}

Prior work has generated insights on affect prediction

Common approach in the prior literature is to map affective states based on the Circumplex Model of Affect, which suggests that emotions can be mapped onto a space with the two dimensions of valence (i.e., pleasure) and arousal (i.e., physical and psychological activation) (Russell, Weiss, \& Mendelsohn, 1989).

Most results are based on either enacted or rated speech and only little on speech in the wild with self-reported affect.

In order to collect the required affective language, researchers usually rely either on enacted speech or use raters to assign emotion labels to utterances due to the aforementioned issues with collecting longitudinal self-report data on fluctuating states (Burkhardt, Paeschke, Rolfes, Sendlmeier, \& Weiss, 2005; Gong et al., 2015; Schröder et al., 2008; Schuller, 2011). Both approaches come with multiple downsides.

Second, since language in most data sets has been manually labelled by raters instead of using self-reported scores, there is an ambiguity of ground truth due to the subjective nature of labelling (Schuller, 2011). In this manner, previous studies mostly assessed perceived personality or affect from external raters instead of self-reported personality or affective states (Mohammadi et al., 2010; Polzehl, 2015). Thereby, expressed affect or personality rather than the experienced affect or inherent personality were being assessed. However, there is high subjectivity and uncertainty in the target labels because raters tend to disagree to some extent as to what the state or trait should be expressed in the language of others (Schuller, 2018).

Previous studies mostly analyzed language data, which was created in experimental settings rather than naturally occurring language, due to the challenges associated with collecting natural language in-the-wild (Schuller, 2011). Therefore, most datasets cited in the literature contain language data created in lab situations, for example from actors, who enact different affective states (Burkhardt et al., 2005; Polzehl, 2015). However, this comes at a disadvantage because the desired state may not be authentically acted out and it may be driven by how the actor believes the respective personality or emotion must be expressed (Schuller, 2018).

Further, the few available data sets containing naturally occurring speech data have less than 100 participants (Ivanov et al., 2011; Mairesse et al., 2007; Polzehl, 2015). Yet larger sample sizes are needed to discover robust effects.

However, new research tools help to collect speech data in the wild and in-situ self-reports of affect.
In the form of the Experience Sampling Method (ESM) allow to assess self-reported affect in an ecologically valid way over a period of time (Servia-Rodríguez et al., 2017). ESM is a method growing in popularity among researchers to collect participants' self-reports on their activities, emotions and other situational variables (van Berkel, Ferreira, \& Kostakos, 2017). Recent research by Sun and colleagues (2019) has applied ESM to collect longitudinal data on participants' affective states (i.e., happy mood) as well as their language use. They extracted acoustic features from audio snippets and linguistic features from real-life speech samples collected by a sound recorder in order to predict fluctuations in participants' happiness.
Experience Sampling is particulary helpul when run on a smartphone. Again, new data collections methods, such as research apps for smartphones, allow to collect language in large quantities in-the-wild rather than from the lab (Servia-Rodríguez et al., 2017).

In conclusion, previous findings on the prediction of affect from speech and recent methodological advances in the area of smartphone-based experience sampling motivate us to address the gap in the affect prediction literature based on speech data collected in the wild with self-reported affect annotations. Therefore, we collect speech data and self-reports on affect from participants using an experience-sampling module of the PhoneStudy App. We train cross-validated machine learning models on the extracted audio features to predict participants' self-reported affect, and investigate, which variables were most predictive in the models using interpretable machine learning methods.Thereby, we want to advance theories on affect in speech, elevate applications in automatic affect-detection from speech signals, and inform the discussion on the protection of privacy rights.

\hypertarget{method}{%
\section{Method}\label{method}}

\hypertarget{data-collection}{%
\subsection{Data collection}\label{data-collection}}

Data collection for this work is part of the PhoneStudy project at Ludwig-Maximilian-Universität München. Representative sample through panel.

The study comprises two two-week experience sampling phases (27.07.2020 to 09.08.2020; 21.09.2020 to 04.10.2020) during which participants receive two to four short questionnaires per day. The
questionnaire is available for 15 minutes and participants are given another 15 minutes to
complete it once they have started answering it.

The last experience sampling questionnaire of the day includes an audio logging task at the
end. Here, participants are instructed to read out a series of given sentences while making an
audio recording of their voice. We use the open source software Open Smile by Audeering
(\url{https://audeering.com/technology/opensmile/\#features}) to extract two feature sets
(ComParE2016 and eGeMAPS) of voice parameters directly on the participant's device. We do
not store the raw audio logging files. The sentences presented to the participants are based on
a set of validated German neutral and emotionally affective sentences (Defren et al. (2018)) and
differ in their emotional content: positive, negative, and neutral. These three emotional
categories are presented consecutively in each audio logging task. The order of the categories
is randomized per experience sampling questionnaire. For each emotional content category
three sentences are randomly drawn from respective sets of sentences in the database created
by Defren and colleagues. The audio recording is started by the participants via a button on the
screen. Participants can stop the recording manually after a minimum of four seconds.
Alternatively, the recording is stopped automatically after twelve seconds.

Further, we extract features directly on the participant's device. No audio files have to be transferred, only information on extracted features.

\hypertarget{predictive-modelling}{%
\subsection{Predictive Modelling}\label{predictive-modelling}}

We trained two separate models for valence and arousal.
We used interpretability measures.

\hypertarget{model-analysis}{%
\subsection{Model analysis}\label{model-analysis}}

We also address two issues with relevance of the method in practice.

\hypertarget{number-of-features}{%
\subsubsection{Number of features}\label{number-of-features}}

How many features do we need? Does the larger feature set improve predictions?
In Sun et al.~the larger set did not improve predictions.

\hypertarget{content-effects-on-acoustics}{%
\subsubsection{Content effects on acoustics}\label{content-effects-on-acoustics}}

What effect does the content participants talk about have on the prediction performance?

\hypertarget{software-open-science}{%
\subsection{Software \& Open Science}\label{software-open-science}}

We used R (Version 4.0.2; {\textbf{???}}) and the R-package \emph{papaja} (Version 0.1.0.9997; {\textbf{???}}) for all our analyses.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{prediction-of-valence-and-arousal}{%
\subsection{Prediction of valence and arousal}\label{prediction-of-valence-and-arousal}}

\hypertarget{interpretation}{%
\subsection{Interpretation}\label{interpretation}}

\hypertarget{model-anlysis}{%
\subsection{Model anlysis}\label{model-anlysis}}

Figure 3 shows the residuals for different values for valence and arousal.

\hypertarget{number-of-features-1}{%
\subsubsection{Number of features}\label{number-of-features-1}}

\hypertarget{content-effects-on-acoustics-1}{%
\subsubsection{Content effects on acoustics}\label{content-effects-on-acoustics-1}}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

Our results crated new insights.

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-defrenEmotionalSpeechPerception2018}{}%
Defren, S., de Brito Castilho Wesseling, P., Allen, S., Shakuf, V., Ben-David, B., \& Lachmann, T. (2018). Emotional Speech Perception: A set of semantically validated German neutral and emotionally affective sentences. In \emph{9th International Conference on Speech Prosody 2018} (pp. 714--718). ISCA. \url{https://doi.org/10.21437/SpeechProsody.2018-145}

\endgroup


\end{document}
