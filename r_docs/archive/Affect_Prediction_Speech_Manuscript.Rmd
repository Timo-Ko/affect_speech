---
title             : "Predicting Between-Person Differences and Within-Person Fluctuations in Affect Experience from Speech Collected with Smartphones"
shorttitle        : "Predicting Affect from Voice"

author: 
  - name          : "Timo K. Koch"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "University of St. Gallen, Institute of Behavioal Science and Technology, Torstrasse 25, 9000 St. Gallen"
    email         : "timo.koch@unisg.ch"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Investigation
      - Methodology
      - Formal Analysis
      - Visualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "Zachariah Marrero"
    affiliation   : "3"
    role:
      - Data curation
  - name          : "Florian Bemmann"
    affiliation   : "2"
    role:
      - Software
  - name          : "Gabriella Harari"
    affiliation   : "4"
    role:
      - Resources
  - name          : "Sam Gosling"
    affiliation   : "2"
    role:
      - Resources
  - name          : "Markus Bühner"
    affiliation   : "1"
    role:
      - Resources
      - Writing - Review & Editing
  - name          : "Ramona Schoedel"
    affiliation   : "1"
    role:
      - Conceptualization
      - Writing - Review & Editing
      - Supervision
  - name          : "Clemens Stachl"
    affiliation   : "5"
    role:
      - Conceptualization
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Institute of Behavioral Science and Technology, University of St. Gallen"
  - id            : "2"
    institution   : "Media Informatics Group, Ludwig-Maximilians-Universität München"
  - id            : "3"
    institution   : "University of Texas"
  - id            : "4"
    institution   : "Stanford University"
  - id            : "5"
    institution   : "Institute of Behavioral Science and Technology, University of St. Gallen"



authornote: |
  Conflict of interest: none
  Acknowledgements: We thank Peter Ehrich and Dominik Heinrich for their support with the technical implementation of the on-device acoustic feature extraction for study 1. We thank ZPID for the support with data collection for study. Thanks Sumer Vaid for technical support in the analyses of study 2. This project was supported by a scholarship of the German Academic Scholarship Foundation awarded to the first author.

abstract: |
  Recognizing affective states from acoustic voice cues is a unique feature of human communication. In recent years, researchers attempted to decipher this complex process using novel machine learning algorithms to predict affective states from voice cues. However, the majority of research is based on enacted speech from the lab or affect-annotated speech samples assessing affect *expression* rather than subjective affect *experience*. In this paper, given the growing scientific and commercial interest in the recognition of subjective affect experience, we investigate if machine learning algorithms are able to recognize subjective affect experience from voice samples collected in the wild. Therefore, in two studies we collected 6,101 voice samples with corresponding experience-sampled self-reports on affect experience from 1,203 (NStudy1 = 500, NStudy2 = 700) participants using smartphones. We show that affect experience on the dimension of arousal, but not valence, is significantly predictable from acoutic properties of speech, but with much lower prediction accuracy than comparable models on affect expression. In line with prior literature, we find that loudness and pitch are most predictive of arousal in our models. Further, our results suggest that semantics (i.e., the emotional content) do not affect predictions. We discuss implications for the monitoring of affect experience and raise issues regarding the protection of user privacy rights, for example in the context of voice assistants.
  
keywords          : "Affect, Speech, Voice, Machine Learning, Mobile Sensing"
wordcount         : "5000"

bibliography      : ["../../timo_zotero_library.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)

```

# Introduction

<!-- Catchy intro paragraph: History of affect recognition from voice -->
When communicating affectively through speech, the way *how* we say things using our voice, for example by changing loudness and intonation, is often more relevant than *what* words we use. Therefore, the question of how affective states specifically manifest in voice has fascinated thinkers ever since the Romans. While the early works in rhetoric (e.g., by Cicero) and evolution biology (e.g., by Darwin) were descriptive in nature, psychologists initiated the empirical investigation of affect in voice in the first half of the 20th century using electroacoustic analysis [@fairbanksExperimentalStudyPitch1939, @skinnerCalibratedRecordingAnalysis1935]. With new technological means emerging over the course of the following decades, voice could be transmitted, stored, and reproduced. This fueled the scientific investigation of affective states in voice and evolved into a multidisciplinary research field spanning across psychology, cognitive science, linguistics, phonetics, and acoustics.

## Algorithmic affect recognition from voice
<!-- New AI-based affect recognition  methods -->
After the turn of the millennium, the development of more sophisticated methods to quantify the voice signal and the advent of advanced statistical methods, such as machine learning, to make use of these high-dimensional data streams, enabled scientists to work on algorithms that can automatically recognize affective states from voice cues [@schullerSpeechEmotionRecognition2018, @vogtAutomaticRecognitionEmotions2008]. With the recent advent of artificial intelligence and the "rise of affectivism" [@dukesRiseAffectivism2021] the interest in  algorithmic affect recognition is ever increasing. The latest state-of-the art studies report correlations for valence of up to .15 and arousal of up to .40. The automated recognition of affect offers multiple advantages over the traditional psychological assessment via self-report questionnaires, for example when monitoring mental health conditions through acoustic voice analysis [@muaremiAssessingBipolarEpisodes2014]. First, it eliminates the general downsides of questionnaire assessment, such as social desirability and response biases [@demetriouSelfReportQuestionnaires2015]. Second, it offers an unobtrusive mean to monitor fluctuating affective states over time, which would otherwise require intrusive repeated assessment. Here, self-report methods are particularly impractical because reporting one’s affect can alter those them and the amount of self-reports that people can complete in a given time period of interest is limited [@kassamEffectsMeasuringEmotion2013, @kuppensFeelingsChangeAccounting2010]. The timely recognition of affect from voice also holds many promises in human-machine interaction, where the artificial agent uses the recognized affective information to craft an appropriate emotional response.

<!-- Commercial AI-based affect recognition  -->
The promises of algorithmic affect recognition from voice also creates increasing commercial interest in this field. Particularly, due to the rise of voice assistants, for example Amazon's Alexa and Apple's Siri, voice data has become ubiquitous. Now, tech companies want to make commercial use of these voice data by, for instance, recognizing what affect their customers experience [@mandellSpotifyPatentsVoice2020, @knightAmazonWorkingMaking2016a] to develop personalized user interfaces, for example by mimicing the user's characteristics [@vlahosTalkMeHow2019]. However, many of these nontransparent commercial services for affect recognition from voice do not disclose how predictions are being made. This raises many issues regarding the protection of user privacy in setting where voice data is analyzed, for example, when using voice assistants. Here, researchers play an important role to investigate these algorithms and inform society and policy makers.

## Conceptual and methodological challenges

<!-- Definition  affective states -->
When discussing research on affect recognition from, for example, voice one has have an agreement on how different affective states, such as emotions, are defined and how to model them. In this paper, we will use the design feature delimitation of different affective states by Scherer [@schererVocalCommunicationEmotion2003] as guideline. He proposes to differentiate between emotion, mood, interpersonal stances, attitudes, and personality traits that decrease in intensity and increase in duration in the aforementioned order. This differentiation is relevant when comparing findings across studies. Further, there is much scientific debate on which model of affect to adopt: Either categorical or dimensional models. Categorical approaches propose the existence of 6 to 14 fundamental emotions [@ekmanArgumentBasicEmotions1992]. Dimensional approaches suggest that affective states can be mapped in a two-dimensional space with the dimensions of valence (i.e., pleasure) and arousal (i.e., physical and psychological activation) [@posnerCircumplexModelAffect2005, @russellCircumplexModelAffect1980]. The discrete emotions can be mapped on the dimensional space [@]Prior studies on affect recognition from voice differ in the choice of emotion model and, as a consequence, make a comparison across studies sometimes challenging.
<!-- Diesen Abschnitt zur Unterscheidung von den Emotionsmodellen, Circumplex würde ich bissl ausführlicher machen und richtig etablierte psychologische Literatur dazu zitieren; z.B. zum Circumplex-Modell; das zeigt dem Leser, dass du dich nicht nur mit den Voice Sachen super auskennst, sondern auch die Psychologie-Basics dazu super drauf hast -->

<!-- affect experience vs affect expression -->
However, research on how much information on one's subjective affect experience can actually be inferred from voice cues is limited. Prior research is mostly based on small data sets of enacted speech collected in lab settings or labeled voice samples, for example, coming from TV shows. <!-- Refs are missing --> Thereby, the focus of these works has been on the recognition of affect *expression* instead of affect *experience*. Affect *expression* represents the emotional expressive behavior, including  take based on affect *experience*. "Feeling is not always revealing" [@grossRevealingFeelingsFacets1997, @grossDissociationEmotionExpression2000]. But it is the subjective affect experience that is of high relevance for research and applied science. Are models trained on affect expression of others at all generalizable to recognition of affect experience? Questionable, because affect expression are extreme cases; but for clinical context, like depression detection, it would be immensely important to have well-functioning models for affect experience.

<!-- Ground truth data dilemma in automatic affect recognition-->
So far, data on affect expression had been collected. Prior research on automatic affect recognition has differed the way the data had been collected. In order to collect speech data with emotional labels, researchers usually rely either on enacted speech or use raters to assign emotion labels to utterances because it is challenging to collect in-situ self-reports of emotion experience as ground truth data. As a result, many corpora are available. However, both approaches, enacted and labeled speech, come with multiple downsides since these approaches assess expressed emotion through voice rather than emotion experience. For enacted speech, the desired state may not be authentically acted out and it may be driven by how the actor believes the respective emotion should be expressed [@schullerSpeechEmotionRecognition2018]. Further, actors do not feel the induced emotion and might overact [@wiltingRealVsActed2006]. Further, these data is created with few actors in experimental settings rather than naturally occurring language with many participants. Yet larger sample sizes are needed to discover generalizable effects. For labeled speech, there is an ambiguity of ground truth due to the subjective nature of labeling [@schullerVoiceSpeechAnalysis2011]. However, there is high subjectivity and uncertainty in the target labels because raters tend to disagree to some extent as to what the state should be expressed in the language of others [@schullerEmotionRecognition2018]. Therefore, these labels represent perception processes (perceived affect) rather than production processes (felt affect) [@schullerRecognisingRealisticEmotions2011]. To our knowledge, Only few studies used self-reports on affective states as ground truth due do the aforementioned challenges. Weidman and colleagues also used the EAR and had raters label the audio snippets, which represents another way to go about this. 

<!-- How "well" prior studies had predicted affect -->
Studies on affect expression report performance between x and y. Studies on affect experience report performance between x and y. 

<!-- Was mir in diesem Abschnitt noch fehlt: harte Fakten, d.h. wie gut sind den die bisher genannten Ansätze darin affective states zu predicten? Kurzen Überblick über STudien geben, die das schon gemacht haben und was deren results waren; ggf. kann man dann hier auch überleiten und sagen, dass die "auffällig" gut klappen (z.B, auch im vergleich zu Ansätzen, die andere Datentypen, wie semantics o.ä. nehmen, um affective states zu predicten), was an der Ground truth, nämlich affective expression liegen könnte. -->


<!--  was mir schwer fällt beim lesen: im Abschnitt drüber sagst du, dass self-report nicht so dolle ist und im Folgenden sagst du, dass es dolle ist. Vielleicht muss man hier noch klarer rausstellen, dass es nicht dolle ist für die Erkennung von affective states an sich, da ist voice besser; aber als Ground Truth ist es die beste Option. -->

<!-- Gibt es von Mehl und Kollegen zufällig Papers, wo sie manuell über EAR affectives states fremd-gelabelt haben? Das könnte man noch als alternativen Ansatz hier nennen, Bzw. als Zwischenschritt in der Evultion von self-report zu automatic recognition -> Ja, gibt es! -->

<!-- Specific voice-affect associations and outlook on interpretable ML -->
Besides the accurate recognition of affect, prior studies also reported on associations of specific acoustic features and affective states. For example, pitch and intensity were found to be associated with arousal [@vogtAutomaticRecognitionEmotions2008]. However, by reporting on correlation of voice features with affect, most prior studies were descriptive in nature [@weningerAcousticsEmotionAudio2013]). Recent studies on algorithmic offer no insights into their "black-box" models. Here, new developments in the area of interpretable machine learning can offer new insights into which acoustic features are particularly predictive of affective states [@molnarInterpretableMachineLearning2019].

## The role of semantics in affect recognition from acoustics

<!-- comparing phonetic and semantic predictions, how semantic content affects acoustics -->
Research has shown that prosody (tone of speech) and semantics (lexical content of the produced words) work together when transmitting affective information through speech [@ben-davidboazm.ProsodySemanticsAre2016]. Prior studies suggest that there is a prosodic dominance in the perception of affect [@ben-davidboazm.ProsodySemanticsAre2016, @linyiProsodyDominatesSemantics2020]. Prior studies investigated this by systematically controlling for the prosody and semantics of distinct enacted emotions (e.g., 5 to 7) in lab contexts using experimental approaches, such as reaction time or Stroop experiments, but no real world data [@schwartzEmotionalSpeechProcessing2012]. While most research focused on the interplay of prosody and semantics in the recognition of affect by humans, there are to our knowledge no studies on algorithmic affect detection. Here, researchers still lack a proper understanding of what users talk about (i.e., the emotional content) has an effect on voice acoustics that impacts automated affect recognition. Therefore, it remains unclear, what the effect of the content participants talk about have on the prediction performance. Weidman and colleagues using data  from the wild did not control for the content of the utterances [@weidmanNotHearingHappiness2020]. Here, the question is how algorithms affect recognition from prosody is affected by semantics. In application, for example, could an algorithm in a voice assistant recognize affective state regardless of what the person talks about. Can one talk about a mundane topic, such as the weather or does one need to talk about an emotional topic for the algorithm to pick up the cues? Or, could an affect recognition algorithm detect one's positive mood better if one talks about a positive experience (e.g., meeting a loved one) instead of a neutral experience (e.g., ordering pizza)?
Compare predictions from two sun papers (semantic content worked better)

## Collecting self-reports on affect experience and voice samples in the wild

<!-- esm intro -->
Recent technological progress, particularly in smartphones, has equipped researchers with new research tools to collect both, in-situ self-reports on affect experience and corresponding voice samples, in the wild. With regard to self-reports on affect experience, the Experience Sampling Method (ESM) allow to assess self-reported affect in an ecologically valid way over a period of time [@bolgerIntensiveLongitudinalMethods2013]. ESM is a method growing in popularity among researchers to collect participants’ self-reports on their activities, emotions and other situational variables [@vanberkelExperienceSamplingMethod2017]. It is seen as the gold standard for collecting in vivo experience (Conner at al., 2009). Ideally participants can fill out the short questionnaires directly on their smartphones. 

<!-- mobile voice data collection -->
With regard to collecting in-situ voice samples, researchers have been using the Electronically Activated Recorder (EAR) [@mehlElectronicallyActivatedRecorder2017]. The EAR is a small device that takes audio records of participants' everyday lives in predefined intervals. Sun and colleagues [@sunLanguageWellbeingTracking2020] used the EAR to collect audio samples and analyze language. Another way is to use the smartphone for voice data collection, too. For example, collect voice data from phone calls [@muaremiAssessingBipolarEpisodes2014, @wangStudentLifeAssessingMental2014]. Recent research by Weidman and colleagues (2020) [@weidmanNotHearingHappiness2020] has combined ESM a audio recordings using their smartphone and emailing the file to the researchers. They applied ESM to collect longitudinal data on participants’ affective states (i.e., happy mood) as well as voice samples. They extracted acoustic features from audio snippets and linguistic features from real-life speech samples collected with smartphones in order to predict fluctuations in participants’ happiness. They were unable to predict above baseline levels.

<!-- downsides of other voice data collection methods -->
However, collecting audio samples using the EAR or eavesdropping on phone calls can be privacy invasive since they record raw data. They could also record other people without their consent. Weidman was more privacy respectful, but still participants sent raw files. This is more tedious for participants and researchers. Further, having participants sent raw audio records to researchers poses data privacy threats. Plus, this increases the data transfer load.

<!-- conclusion -->
In conclusion, previous findings on the recognition of affect expression from voice and recent methodological advances in the area of smartphone-based experience sampling and voice data collection methods motivate us to address the gap in the literature on the automatic recognition of affect experience from acoustic voice cues. Using this approach we collect in-situ voice data and self-reports on affect experience from participants in two large-scale studies. We train cross-validated machine learning models on the extracted acoustic features to predict participants’ self-reported affect experience, and investigate, which variables were most predictive in the predictive models. Further, we explore the effect of semantics on prosody. Thereby, we want to advance theories on affect in speech, elevate applications in automatic affect-detection from speech signals, and inform the discussion on the protection of privacy rights.

# Study 1

## Method

## Smartphone-based data collection and privacy-preserving on-device acoustic feature extraction

<!-- Study description -->
Data collection for this work was part of a large-scale quota sample-based panel study using the *PhoneStudy* research app developed at Ludwig-Maximilian-Universität München [@schoedelBasicProtocolSmartphone2020]. Data collection was approved by the responsible IRB board and data privacy office. We preregistered the present study as a transparent account of our work [@kochPredictingAffectiveStates2021]. The study inter-alia comprised two two-week experience sampling phases (27.07.2020 to 09.08.2020; 21.09.2020 to 04.10.2020) during which participants received two to four short questionnaires per day. Here, self-reported valence and arousal were assessed in two separate items on six-point Likert scales among other psychological properties. Hence, the affective state we were assessing was closest to the "mood" definition of Scherer [@schererVocalCommunicationEmotion2003]. The last experience sampling questionnaire of each day included an additional instruction, where participants were asked to read out a series of predefined emotional sentences while making an audio recording of their voice. The sentences presented to the participants are based on a set of validated German neutral and affective sentences [@defrenEmotionalSpeechPerception2018] and differ in their emotional content: positive (e.g., XX), negative (e.g., XX), and neutral (e.g., XX). These three emotional categories are presented consecutively in each audio logging task. The order of the categories was randomized per experience sampling questionnaire. For each emotional content category three sentences were randomly drawn from respective sets of sentences in the database created by Defren and colleagues. The use and experimental manipulation of these semantic categories allowed us to control the content spoken by our participants, but at the same time allowed us to conduct a privacy-friendly study.The audio recording was started by the participants via a button on the screen. Participants could stop the recording manually after a minimum of four seconds. Alternatively, the recording was stopped automatically after twelve seconds. We chose these lower and upper time thresholds because this is the minimum and maximum time required to read out the three sentences.

<!-- on device acoustic feature extraction -->
Once the audio record was complete, we automatically extracted voice parameters using the widely adopted open source OpenSMILE algorithm [@eybenOpensmileMunichVersatile2010] to extract two sets of acoustic features directly on the participant’s device: First, the extended Geneva Minimalistic Acoustic Parameters Set (eGeMAPS) comprised of 88 features  [@eybenGenevaMinimalisticAcoustic2016]. The feature sets are clustered into feature groups termed low level descriptors (LLD) (e.g., Loudness, Pitch, Frequency). Second, the 2016 Interspeech Computational Paralinguistic Challenge (ComParE2016) feature set comprised of 6,737 features [@schullerINTERSPEECH2016Computational2016]. Theses feature sets have been used in prior studies on affect recognition from acted or labelled voice samples. For a non-technical summary of voice features see [@hildebrandVoiceAnalyticsBusiness2020]. After feature extraction the voice records were automatically deleted and only extracted voice features were stored on our servers.

```{r, include = FALSE}

library("dplyr")

affect_egemaps  <- readRDS("../data/affect_egemaps.RData") #load data

# total number of audio samples with corresponding 
nrow(affect_egemaps)

# number of participants in the final sample
length(unique(affect_egemaps$user_id))

# show distribution of sentence conditions

table(affect_egemaps$condition)

# count number of audio samples per experience sampling instance
audio_per_es <- affect_egemaps %>%
  group_by(e_s_questionnaire_id) %>%
  count() 

table(audio_per_es$n) # there are three audio records for most es instances

# show valence and arousal statistics per condition - no differences!
affect_egemaps %>%
  group_by(condition) %>%
  dplyr::summarize(mean_valence = mean(valence, na.rm=TRUE), mean_arousal = mean(arousal, na.rm=TRUE))

# get sample demographics
demographics <- read.csv2(file = "../../Questionnaires/waves/wave1_2021_02_19.csv", header = T)
demographics <- demographics[, c('Demo_A1', 'Demo_GE1', 'p_0001')] # keep relevant columns only

affect_egemaps_demo <- base::merge(affect_egemaps, demographics , by.x = "user_id", by.y = "p_0001" )

length(unique(affect_egemaps_demo$user_id))

# create new user id df
affect_egemaps_demo_user <- affect_egemaps_demo[!duplicated(affect_egemaps_demo$user_id),]

mean(affect_egemaps_demo_user$Demo_A1)

# the user id matching is missing!!

```

With this procedure, we collected 12,000 audio logs from 3,678 experience samples for valence and arousal with corresponding acoustic features from 651 participants. We excluded three participants because there was no variance in all of their valence and arousal scores across all their experience samples. Further, we excluded 6,001 acoustic feature sets because the respective features (Voiced segments per second) indicated that no human voice was recorded. This left us with a final data set of `r nrow(affect_egemaps)` voice samples from `r length(unique(affect_egemaps$e_s_questionnaire_id))` experience sampling instances for valence and arousal with corresponding acoustic features from `r length(unique(affect_egemaps$user_id))` participants (`r (table(affect_egemaps_demo_user$Demo_GE)[2] / sum(table(affect_egemaps_demo_user$Demo_GE)))*100`
% female, $M_{Age}$ = `r mean(affect_egemaps_demo_user$Demo_A1)` years). The mean arousal was `r mean(affect_egemaps$arousal)` (SD = `r sd(affect_egemaps$arousal)`) and mean valence was 4.21 (SD). In the final sample, `r table(affect_egemaps$condition)[[3]]` voice samples were from the positive condition, `r table(affect_egemaps$condition)[[2]]` from the neutral condition, and `r table(affect_egemaps$condition)[[1]]` from the negative condition with no substantial differences across conditions.
<!-- R: haben wir dazu nen Test gerechnet? 
T: mhhh ne bisher kein Test - sollen wir noch einen machen? -->


```{r}
hist(affect_egemaps$valence)

hist(affect_egemaps$arousal)
```

## Predictive Modelling

<!-- general ml -->
We trained multiple supervised machine learning regression models on the extracted acoustic features for the prediction of self-reported valence and arousal. Machine learning algorithms have multiple advantages over classical regression and have been used in prior research [@weidmanNotHearingHappiness2020]. Here, we compared the predictive performance of Elastic Net regularized regression models [@zouRegularizationVariableSelection2005] with those of a non-linear tree-based random forest model [@breimanRandomForests2001; @wrightRangerFastImplementation2017], and a baseline model. The baseline model would predict the respective mean values for valence and arousal of the respective training set for all cases in a test set. We evaluated the predictive performance of our models and tuned model hyperparameter in a nested cross-validation scheme [@bischlResamplingMethodsMetaModel2012]. We blocked participants in the resampling procedure ensuring that for one train/test set pair the given participant is either in the training set or in the test set. We also ran the described analyses on the larger ComParE2016 feature set in order to investigate if a larger feature set improves predictions. This is relevant for potential application of affect recognition on device.

<!-- Model evaluation -->
We evaluated the predictive performance of the models based on the coefficient of determination, Spearman (rank) correlation (r) and mean absolute error (MAE) and between the predicted valence and arousal scores and participants’ self-reported scores. To determine whether a model was predictive (alpha = 0.05) at all, we carried out t tests (one-sided) by comparing the R2 measures in all prediction models with those of the baseline models. We used variance corrected t tests based on 10-fold cross- validation to account for the dependence structure of cross-validation experiments [@nadeauInferenceGeneralizationError2003]. All comparisons were adjusted for multiple comparisons (n = 2) via Holm correction. 

<!-- (grouped) feature importance -do we want to include grouped performance or imp of single features? -->
Further, for predictive models for arousal, we investigated which aspects of the voice contribute to the predictions. Therefore, we computed the grouped feature importance of the 25 Low Level Descriptors of the six parameters groups of the eGeMaps feature set [@auGroupedFeatureImportance2021]. 

<!-- Software -->
All data processing and statistical analyses in this work were performed with the statistical software R version 4.0.2 (R Core Team, 2020). For machine learning, we used the mlr3 framework [@langMlr3ModernObjectoriented2019] and the DALEX package [@biecekDALEXExplainersComplex2018]. We provide the R code and our main figures in the project’s repository of the Open Science Framework (OSF).

# Results

```{r, include = FALSE}

library(mlr3)

bmr_egemaps_valence <- readRDS("../results/bmr_egemaps_valence.RData") #load data
bmr_egemaps_arousal <- readRDS("../results/bmr_egemaps_arousal.RData") #load data

bmr_compare_valence <- readRDS("../results/bmr_compare_valence.RData") #load data
bmr_compare_arousal <- readRDS("../results/bmr_compare_arousal.RData") #load data

mes = mlr3::msrs(c("regr.mae","regr.srho", "regr.rsq")) # set performance measures

# get aggregated performance measures
mes_egemaps_arousal <- bmr_egemaps_arousal$aggregate(mes)
mes_egemaps_valence <- bmr_egemaps_valence$aggregate(mes)
mes_compare_arousal <- bmr_compare_arousal$aggregate(mes)
mes_compare_valence <- bmr_compare_valence$aggregate(mes)

```

## Recognizing valence and arousal from acoustic voice cues

Our employed algorithms significantly predicted arousal, but not valence, above chance. Overall, Random Forest models performed better than Elastic Net models, indicating non-linear associations of voice features and affect experience.

For valence, neither the Random Forest models (r = `r mes_compare_valence[which(mes_compare_valence$learner_id == "regr.ranger.tuned")]$regr.srho`, MAE = `r mes_compare_valence[which(mes_compare_valence$learner_id == "regr.ranger.tuned")]$regr.mae`) nor the Elastic Net models (r = `r mes_compare_valence[which(mes_compare_valence$learner_id == "regr.cv_glmnet.tuned")]$regr.srho`, MAE = `r mes_compare_valence[which(mes_compare_valence$learner_id == "regr.cv_glmnet.tuned")]$regr.mae`) models did not predict significantly above baseline levels (r = `r mes_compare_valence[which(mes_compare_valence$learner_id == "regr.featureless")]$regr.srho`, MAE = `r mes_compare_valence[which(mes_compare_valence$learner_id == "regr.featureless")]$regr.mae`).

For arousal, neither the Random Forest models (r = `r mes_compare_arousal[which(mes_compare_arousal$learner_id == "regr.ranger.tuned")]$regr.srho`, MAE = `r mes_compare_arousal[which(mes_compare_arousal$learner_id == "regr.ranger.tuned")]$regr.mae`) nor the Elastic Net models (r = `r mes_compare_arousal[which(mes_compare_arousal$learner_id == "regr.cv_glmnet.tuned")]$regr.srho`, MAE = `r mes_compare_arousal[which(mes_compare_arousal$learner_id == "regr.cv_glmnet.tuned")]$regr.mae`) models did not predict significantly above baseline levels (r = `r mes_compare_arousal[which(mes_compare_arousal$learner_id == "regr.featureless")]$regr.srho`, MAE = `r mes_compare_arousal[which(mes_compare_arousal$learner_id == "regr.featureless")]$regr.mae`).

We compared prediction performance of algorithms trained on the smaller eGeMaPs (88 features) and the larger Compare2016 (6,000 features) feature sets.

## Predictiveness of acoustic feature groups

We identified predictive feature groups in arousal predictions. For arousal, features related to loudness were most predictive. Show (grouped) feature importance for Low Level Descriptors (25). Show PDP plots for important feature groups. [@auGroupedFeatureImportance2021].

<!---
{r groupedimportance_affect, fig.cap = "Grouped permutation feature importance for low level descriptors arousal predictions in Random Forest algorithm."}
# library(ggplot2)
# library(gridExtra)

# groupedimportance_affect = grid.arrange (performance_folds_valence, performance_folds_arousal, ncol = 2)
# 
# performance_affect


-->

## Model diagnostics and semantics' effects on acoustics 
<!-- add some statistical tests here? is absolute error different across conditions -->
Our models were much more accurate in the middle and less accurate for extreme cases. This curve reflects the distribution of our data, i.e. the predictions could reflect that most participants were in the middle area most of the time. 
Our results indicate that the emotional content of the sentences had not impact on predictions.
<!-- put figure in the appendix -->

![Prediction performance for valence and arousal across sentence conditions. Predictions from Random Forest algorithm based on eGeMaPs features.](../figures/performance_condtion.png)

# Study 2

Here we used distinct variables instead of dimensional approach. 

## Method

### Smartphone-based data collection of raw audio files
<!-- Study description -->
We collected data from the US at University of Texas at Austin in fall 2018. Predicted contentedness and sadness and arousal. Same level of arousal, opposing valence. Contentedness and Sadness on 4-point Likert scale from zero to four. 

<!-- acoustic feature extraction -->
Raw transcripts were transcribed using the Google Speech-to-text API. Acoustic features were extracted with OpenSmile. 

<!-- word embeddings -->
We extract word embeddings, using the text R package.
Used last two layers from Roberta large models as suggested by Schwartz (REF).

### Predictive modelling 
We used the same prediction methods as in study 1. Used voice and embeddings as features. Outcome variables sadness, contentedness and arousal of regression models are different. 

## Results

### Predicting discrete affective states 
We predicted contentedness and sadness above baseline levels. 

### Content effects on predictions and model diagnostics
<!-- Effects of sentiment on predictions -->
Run sentiment lexica and correlate sentiment with absolute prediction error / create plot for this. 
Correlate word count with absolute error here! Does is matter how much people talk? in study 1 this was pretty much standardized. 

<!-- Predictions based on word embeddings and we + acoustics -->
Include embeddings here. Combine both!

# Discussion

We significantly predicted experienced self-reported arousal, but not valence, from voice cues collected in the wild using a  smartphone-based data approach. We identified loudness and frequency to be particularly predictive of arousal. Our results suggest that semantics did not affect the recognition of affect experience from voice cues.

## Recognizing affect experience from speech

Our results indicate that arousal experience can be automatically recognized from voice, where as valence experience cannot. Thereby, we report lower prediction performance than prior work on automatic prediction of affect expression (REF) and similar on affect experience (REF). Our findings that arousal can be recognized easier than valence is in line with prior research and theoretical considerations[@]. Weidman and colleagues were unable to predict happiness, which could be interpreted as analogue to valence [@weidmanNotHearingHappiness2020]. Valence is about the quality of affect, which is very individual. 

With Weidman et all Random Forests also performed best (better than SVM and NN) 
Prior studies report that humans can detect emotions with arousal of .70 and valence of .75 and state of the art algorithms are reported with .4 and .15.

As mentioned in the introduction, we used dimensional approach, which allows for a comparison with similar studies only. real experienced emotions, 

<!-- Comparing model performance across the two studies -->
Predictions are better in study 2 because participants could express freely. 

The main limitations of our work lie in our privacy-preserving on-device voice recording approach. In contrast to other studies, we extracted acoustic features directly on participants' smartphones instead of transferring  raw audio records to our servers for further analyses. As a consequence, we have no opportunity to check if participants truly complied with study instructions and had recorded their voice while reading out the predefined sentences accordingly. Further, our approach did not allow to control for records' background noise (e.g., when participants were outside next to a road) or how they held their smartphone during the voice record. Here, lab records using professional microphones surely offer superior sound quality. However, checking single audio files manually would be out of scope and was also not performed by Weidman et al.!

While the predefined sentences allowed us to control for the semantics of what participants talked about in their voice records we let them record, they we unable to express themselves freely. Possibly, if participants were allowed to talk about a topic of their choice, their affect experience would have manifested more strongly in their voice learning to more accurate predictions. In future studies, participants could be free with regard to the content they can talk about and pre-trained language models extract features of what they talk about (e.g., specific topics) directly on the device, too. Thereby, there would be no raw data transferred, but potentially valuable information of language content could be also used for affect recognition.

<!-- Why our predictions are "weaker" than in the literature -->
We would expect to find an effect on at least valence prediction! Our overall effect are rather small, we cannot predict valence overall, maybe in some pockets of the spectrum? Maybe it only has an effect on valence recognition? 
Other reasons why we did not find any effect of semantics? Overall low signal in the data? Too much noise?
Weidman and Sun used the same EAR data for predictions from semantics and acoustics and only found something in semantics! They had a student sample.
Weidman's models were also more accurate in the middle area and less accurate in the extreme areas.


We propose three reasons, why our predictive performance is lower than in prior work. First, our and prior research indicates that recognizing affect experience is more challenging than recognizing affect expression. Second, real-life affect is more difficult to recognized that enacted one [@vogtAutomaticRecognitionEmotions2008]. Third, there are less instances of extreme affect experiences in our data set compared to the data used in prior studies on acted emotions. We rather predicted mood, which is, by definition, less intense than emotion, which are short-lived and directed [@schererVocalCommunicationEmotion2003].

## What we talk about matters for affect recognition from acoustics

<!-- Sentence condition (study 1) and sentiment score (study 2) -->
Our results based on a novel experimental design suggest that the semantics, meaning the content what participants talked about, did not have an impact on affect recognition on the dimensions of valence and arousal. This implies that it does not matter what people talk about when inferring affect experience from voice.  

<!-- Advantages of using on device feature extraction like in GER study -->
Study 2 predictions might be better because participants could talk freely. 

<!-- Using word embeddings to predict affect (study 2) -->
Word embeddings in study 2 improved predictions. WHAT people talk about is very relevant.

## Using smartphones to collect voice data in the wild
<!-- Advantages of smartphones to collect acoustic data -->
Previously, researchers were limited in the available methods to collect in-situ self-report on affect experience and corresponding voice samples. The only two viable solution, were using the EAR and having participants make audio records using their smartphones and e-mailing files to researchers. We applied a novel, privacy-pervasive framework combining ESM and on-device acoustic feature extraction with many advantages. Further, in line with prior research [@weidmanNotHearingHappiness2020], our results suggest that a sparse feature set is suitable. Weidman et al. sent raw audio files and were unable to recognize affect. This could indicate the an on-device feature extraction must not be worse. 

<!-- Some words on on-device feature extraction like in GER study -->
We show that sparse feature extraction on the device is fruitful and privacy-preserving. This could be used as diagnostics instrument.  Our findings indicate that more features do not lead to more accurate predictions. Small features set is less computationally expensive and would allow for online or on-device pre-processing in a scientific or applied setting. 

## Implications 

Our results suggest that subjective affect experience, particularly valence, is much more difficult to recognize than affect expression. By making our predictions interpretable we added to affect and voice theory. 

Further, our new research approach to collect voice data offers a fruitful avenue for future research based on voice data. It also offers in the sense of goal, experimental variation of sentences : use smartphone as experimental lab [@millerSmartphonePsychologyManifesto2012]. We also show that an economic acoustic feature set sufficient, this is relevant for application.

<!-- Implications for applied use of voice analytics to infer affect -->
Our findings have broad implications for the monitoring of affect experience in a practical setting. They question the proclaimed performance of commercial affect recognition algorithms and highlight the challenges ahead in affect monitoring, for example in mental health care. Current expectations, particularly of commercial solutions for affect predictions from voice, might be overoptimistic. Finally, implications for the privacy of users of, for example, voice assistants. No impact of semantics on affect recognition

## Limitations and future directions

<!-- limitation: how we collected the voice data -->
Participants had to actively log their voice data. Might yield different results to passive voice sensiong (e.g., via EAR), but this has huge privacy issues! 
Limitation: 1) artificial setting for our voice records, no eavesdropping on naturally occurring speech data
2) our dv is a single  item self-report as ground truth, does this reflect true affect? 


<!-- limitation: One-item scales -->
Further, we used a one-item scale. This approach is established, but can come with downsides regarding measurement error [@jacobucciMachineLearningPsychological2020]. We did not use the same scales in study 1 and 2 which makes results not directly comparable with each other.

<!-- limitation: Everyday moods instead of extreme emotions -->
Because we collected in-situ self reports of affect experience from participants' everyday life, most of our data represents average affect experience and much fewer extreme cases. This, however, represents the normal everyday states of regular people, for whom many of the affect recognition algorithms were designed. Possibly, if future studies are able to collect more data from extreme cases, such as from clinical samples, results could be more distinct.

<!-- Future direction: Multi-modal & idiosyncratic models -->
Moreover, future work should further account for the subjective nature of affect experience, particularly valence. Potentially, researchers could create idiosyncratic (person-specific) models that are trained on data of a single participant. Finally, extracted information from voice could be merged with other sensing data (e.g., movement patterns) into one model to improve predictions.

# Conclusion

In this work, we made predictions based on voice parameters collected in the wild using smartphones. We used a privacy protective on-device feature extraction. Our models predicted arousal significantly above baseline levels, but not arousal. Further, we investigated the effects of semantics on prosody in affect recognition. We discussed implications for theory and practice and further research opportunities.

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
